{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- Model Averaging\n",
    "    - Bagging\n",
    "    - Boosting\n",
    "- Stagewise Additive Modeling\n",
    "- Boosting and Logistic Regression\n",
    "- MART\n",
    "- Example: Predicting e-mail spam\n",
    "\n",
    "Methods for improving the performance of weak learners. Classification\n",
    "trees are adaptive and robust, but do not generalize well. The techniques\n",
    "discussed here enhance their performance considerably."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Problem\n",
    "\n",
    "Data $(X,Y) \\in R^p \\times \\{0,1\\}$\n",
    "\n",
    "X is predictor, feature; $Y$ is class label, response.\n",
    "\n",
    "$(X,Y)$ have joint probability distribution $D$.\n",
    "\n",
    "Goal: Based on $N$ training pairs $(X_i,Y_i)$ drawn from $D$ produce a \n",
    "classifier $\\hat{C(X)} \\in \\{0,1\\}$\n",
    "\n",
    "Goal: Choose $\\hat{C}$ to have low generalization error\n",
    "$$\n",
    "R(\\hat{C}) = P_D(\\hat{C}(X)\\neq Y) = E_D[\\textbf{1}_{\\hat{C}(X)\\neq Y}]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Concepts\n",
    "\n",
    "\n",
    "$X \\in R^p$ has distribution $D$.\n",
    "\n",
    "C(X) is deterministic function $\\in$ concept class.\n",
    "\n",
    "Goal: Based on $N$ training pairs $(X_i,Y_i=C(X_i))$ drawn from $D$ produce a \n",
    "classifier $\\hat{C(X)} \\in \\{0,1\\}$\n",
    "\n",
    "Goal: Choose $\\hat{C}$ to have low generalization error\n",
    "$$\n",
    "R(\\hat{C}) = P_D(\\hat{C}(X)\\neq Y) = E_D[\\textbf{1}_{\\hat{C}(X)\\neq Y}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging\n",
    "\n",
    "Classification trees can be simple, but often produce noisy (bushy) or weak (stunted)\n",
    "classifiers.\n",
    "- Bagging: Fit many large trees to bootstrap-resampled versions of the training data, and classify by majority vote.\n",
    "- Boosting: Fit many large or small trees or reweighted versions of the training data. Classify by weighted majority vote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging or bootstrap aggragation averages a given procedure over many samples, to reduce\n",
    "its variance -- a poor man's Bayes.\n",
    "\n",
    "Suppose $G(\\textbf{X},x)$ is a classifier, such as a tree, producing a predicted class label\n",
    "at input point x. To bag $G$, we draw bootstrap samples $\\textbf{X}^{*1},...,\\textbf{X}^{*B}$\n",
    "each of size $N$ with replacement from the training data. Then\n",
    "$$\n",
    "\\hat{G}_{\\text{bag}}(x) = \\text{Majority Vote} \\{G(\\textbf{X}^{*b}, x)\\}_{b=1}^B\n",
    "$$\n",
    "\n",
    "Bagging can dramatically reduce the variance of unstable procedures (like trees), leading to\n",
    "improved prediction. However any simple structure in $G$ (e.g. a tree) is lost. Bagging averages many trees, and produces smoother decision boundaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "![title](img/boosting.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "1. Initialize the observation weights\n",
    "$$\n",
    "w_i = \\frac{1}{N} \\text{, } i=1,2,..., N\n",
    "$$\n",
    "2. For $m=1$ to $M$ repeat steps (a)-(d):\n",
    "    - a) Fit a classifier $G_m(x)$ to the training data using weights $w_i$\n",
    "    - b) Compute\n",
    "        $$\n",
    "        err_m = \\frac{\\sum_{i=1}^N w_i I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N w_i}\n",
    "        $$\n",
    "\n",
    "    - c) Compute $\\alpha = \\log\\left(\\frac{1-err_m}{err_m}\\right)$\n",
    "    - d) Update weights for $i=1,..,N$:\n",
    "        $$\n",
    "        w_i \\leftarrow w_i\\cdot e^{\\alpha_m \\cdot I(y_i \\neq G_m(x_i))}\n",
    "        $$\n",
    "        and renormalize to $w_i$ to sum to 1.\n",
    "3. Output $G(x)= sign\\left[\\sum_{m=1}^M \\alpha_m G_m(x)\\right]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAC Learning Model\n",
    "\n",
    "$X \\sim D$: Instance Space\n",
    "\n",
    "$C: X \\rightarrow \\{0,1\\}$ Concept\n",
    "\n",
    "$h: X \\rightarrow \\{0,1\\}$ Hypothesis\n",
    "\n",
    "$error(h) = P_D[C(X) \\neq h(X)]$\n",
    "\n",
    "**Definition**: Consider a concept class defined over a set $X$ of length\n",
    "$N$. $L$ is a learner (algorithm) using hypothesis space $H$. A concept class is PAC learn-able\n",
    "by $Q$ using hypothesis space if for all $C$ in concept space, all distributions $D$ over $X$\n",
    "and all $\\epsilon,\\delta\\in (0,\\frac{1}{2})$, learner $L$ will, with $Pr \\geq (1-\\delta)$, output\n",
    "an $h$ in hypothesis space s.t. $error_D(h) \\leq \\epsilon$ in time polynomial in \n",
    "$\\frac{1}{\\epsilon},\\frac{1}{\\delta}, N$ and length of concept size.\n",
    "\n",
    "Such an $L$ is called a strong Learner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting a Weak Learner\n",
    "\n",
    "Weak learner $L$ produces an $h$ with error rate \n",
    "$\\beta = (\\frac{1}{2}-\\epsilon) < \\frac{1}{2}$ with $Pr \\geq (1-\\delta)$ for any\n",
    "distribution $D$.\n",
    "\n",
    "$L$ has access to continuous stream of training data and a class oracle.\n",
    "1. $L$ learns $h_1$ on first $N$ training points.\n",
    "2. $L$ randomly filters the next batch of training points, extracting $\\frac{N}{2}$ points correctly classified by $h_1$, $\\frac{N}{2}$ incorrectly classified, and produces $h_2$.\n",
    "3. $L$ builds a third training set of $N$ points for which $h_1$ and $h_2$ disagree, and produces $h_3$.\n",
    "4. $L$ outputs $h=\\text{Majority Vote}(h_1,h_2,h_3)$\n",
    "\n",
    "$\\textbf{Theorem}$: The Strength of Weak Learnability\n",
    "$$\n",
    "error_D(h) \\leq 3\\beta^2 - 2\\beta^3 < \\beta\n",
    "$$\n",
    "\n",
    "A stump is a two-node tree, after a single split. Boosting stumps works remarkably well on the nested-spheres problem.\n",
    "\n",
    "Boosting drives the training error to zero. Further iterations continue to improve test error in many examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stagewise Additive Modeling\n",
    "\n",
    "Boosting builds an additive model\n",
    "$$\n",
    "f(x) = \\sum_{m=1}^M \\beta_m b(x;\\gamma_m)\n",
    "$$\n",
    "Here $b(x, \\gamma_m)$ is a tree, and $\\gamma_m$ parametrizes the splits.\n",
    "We do things like that in statistics all the time!\n",
    "\n",
    "With boosting, the parameters $(\\beta_m, \\gamma_m)$ are fit in a stagewise\n",
    "fashion. This slows the process down, and tends to overfit less quickly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stagewise Least Squares\n",
    "\n",
    "Suppose we have available a basis family $b(x; \\gamma)$ parametrized by $\\gamma$.\n",
    "For example, a simple family is $b(x; \\gamma_j) = x_j$.\n",
    "- After $m-1$ steps, suppose we have the model\n",
    "$$\n",
    "f_{m-1}(x) = \\sum_{j=1}^{m-1} \\beta_j b(x; \\gamma_j)\n",
    "$$\n",
    "- At the $m$ step we solve\n",
    "$$\n",
    "\\min_{\\beta, \\gamma} \\sum_{i=1}^N (y_i - f_{m-1}(x_i)-\\beta b(x_i;\\gamma))^2\n",
    "$$\n",
    "- Denoting the residuals at the $m$th stage by $r_{im} = y_i - f_{m-1}(x_i)$, the previous step amount to \n",
    "$$\n",
    "\\min_{\\beta, \\gamma}(r_{im}-\\beta b(x_i;\\gamma))^2\n",
    "$$\n",
    "- Thus the term $\\beta_m b(x; \\gamma_m)$ the best fits the current residuals is added to the expansion at each step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost: Stagewise Modeling\n",
    "\n",
    "- AdaBoost builds an additive logistic regression model\n",
    "$$\n",
    "f(x) = \\log \\frac{Pr(Y=1|x)}{Pr(Y=-1|x)} = \\sum_{m=1}^M \\alpha_m G_m(x)\n",
    "$$ \n",
    "by stagewise fitting using the loss function\n",
    "$$\n",
    "L(y, f(x)) = e^{-yf(x)}\n",
    "$$\n",
    "- Given the current $f_{M-1}(x)$, our solution for $(\\beta_m, G_m)$ is \n",
    "$$\n",
    "arg\\min_{\\beta, G} \\sum_{i=1}^N e^{-y_i(f_{m-1}(x_i)+\\beta G(x))}\n",
    "$$\n",
    "where $G_m(x)\\in\\{-1, 1\\}$ is a tree classifier and $\\beta_m$ is a coefficient.\n",
    "- With $w_i^{(m)} = e^{-y_i f_{m-1}(x_i)}$, this can be re-expressed as\n",
    "$$\n",
    "arg\\min_{\\beta, G} \\sum_{i=1}^N w_i^{(m)} e^{-\\beta y_iG(x_i)}\n",
    "$$\n",
    "- We can show that this leads to the Adaboost algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Exponential Loss?\n",
    "\n",
    "- $e^{-yF(x)}$ is a monotone, smooth upper bound on misclassification loss at x.\n",
    "- Leads to simple reweighting scheme.\n",
    "- Has logit transform as population minimizer\n",
    "$$\n",
    "f^*(x) = \\frac{1}{2} \\log \\frac{Pr(Y=1|x)}{Pr(Y=-1|x)}\n",
    "$$\n",
    "- Other more robust loss functions, like binomial deviance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Stagewise Algorithm\n",
    "\n",
    "We can do the same for more general loss functions, not only least squares.\n",
    "1. Initialize $f_0(x) = 0$\n",
    "2. For $m=1$ to $M$:\n",
    "    - Compute\n",
    "    $$\n",
    "    (\\beta_m, \\gamma_m) = arg\\min_{\\beta, \\gamma} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+\\beta b(x_i; \\gamma))\n",
    "    $$\n",
    "    - Set $f_m(x) = f_{m-1}(x) + \\beta_m b(x;\\gamma_m)$ Sometimes we replace the previous step by \n",
    "    - Set $f_m(x) = f_{m-1}(x) + \\nu \\beta_m b(x;\\gamma_m)$\n",
    "    Here $\\nu$ is a shrinkage factor, and often $\\nu < 0.1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MART\n",
    "\n",
    "- General boosting algorithm that works with a variety of different loss functions. Models include regression, outlier-resistant regression, K-class classification and risk modeling.\n",
    "- MART uses gradient boosting to build additive tree models, for example, for representing the logits in logistic regression.\n",
    "- Tree size is a parameter that determines the order of interaction.\n",
    "- MART inherits all the good features of trees (variable selection, missing data, mixed predictors), and improves on the weak feature, such as prediction performance.\n",
    "\n",
    "## MART in detail\n",
    "\n",
    "Model\n",
    "$$\n",
    "f_M(x) = \\sum_{i=1}^M T(x; \\Theta_m)\n",
    "$$\n",
    "Where $T(x;\\Theta)$ is a tree:\n",
    "$$\n",
    "T(x; \\Theta) = \\sum_{j=1}^J \\gamma_j I(x\\in R_j)\n",
    "$$\n",
    "with parameters $\\Theta = \\{ R)j, \\gamma_j\\}_1^J$\n",
    "\n",
    "#### Fitting Criterion\n",
    "Given $\\hat{\\Theta}_j$, $j=1,...,m-1$, we obtain $\\hat{\\Theta}_m$ via stagewise optimization:\n",
    "$$\n",
    "arg\\min_{\\Theta_m}\\sum_{i=1}^N L(y_i, f_{m-1}(x_i)+T(x_i;\\Theta_m))\n",
    "$$\n",
    "For general loss functions this is a very difficult optimization problem. Gradient boosting is an approximate \n",
    "gradient descent method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "1. Initialize $f_0(x) = arg\\min_{\\gamma} \\sum_{i=1}^N L(y_i,\\gamma)$.\n",
    "2. For $m=1$ to $M$:\n",
    "    - For $i=1,2,..., N$ compute\n",
    "    $$\n",
    "    r_{im} = - \\left[ \\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)} \\right]_{f=f_{m-1}}\n",
    "    $$\n",
    "    - Fit a regression tree to the targets $r_{im}$ giving terminal regions\n",
    "    $$\n",
    "    R_{jm}, j=1,2,...,J_m\n",
    "    $$\n",
    "    - For $j=1,2,...,J_m$ compute\n",
    "    $$\n",
    "    \\lambda_{jm} = arg\\min_{\\gamma} \\sum_{x_i\\in R_{jm}} L(y_i, f_{m-1}(x_i)+\\gamma)\n",
    "    $$\n",
    "    - Update\n",
    "    $$\n",
    "    f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m}\\gamma_{jm} I(x\\in R_{jm})\n",
    "    $$\n",
    "3. Output $\\hat{f}(x) = f_M(x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Size\n",
    "\n",
    "The tree size $J$ determines the interaction order of the model:\n",
    "$$\n",
    "\\eta(X) = \\sum_j \\eta_j(X_j) + \\sum_{jk} \\eta_{jk}(X_j, X_k) + \\sum_{jkl} \\eta_{jkl}(X_j, X_k, X_l) + \\cdots \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting e-mail spam\n",
    "\n",
    "- data from 4601 email messages\n",
    "- goal: predict whether an email message is spam (junk email) or good.\n",
    "- input features: relative frequencies in a message of 57 of the most commonly occuring words and punctuation marks in all the training the email messages.\n",
    "- for this problem not all errors are equal; we want to avoid filtering out good email, while letting spam get through is not desirable but less serious in its consequences.\n",
    "- we coded spam as 1 and email as 0\n",
    "- A system like this would be trained for each user separately (e.g. their word lists would be different)\n",
    "\n",
    "## Predictors\n",
    "\n",
    "- 48 quantitative predictors-the percentage of words in the email that match a given word. Examples include `business`, `address`, `internet`, `free`, and `george`. The idea was that these could be customized for individual users.\n",
    "- 6 quantitative predictors-the percentage of characters in the email that match a given character. The characters are `ch`; `ch(`, `ch[`, `ch!`, `ch$`, and `ch#`.\n",
    "- The average length of uninterrupted sequences of capital letters: CAPAVE.\n",
    "- The length of the longest uninterrupted sequence of capital letters: CAPMAX.\n",
    "- The sum of the length of uninterrupted sequences of capital letters: CAPTOT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important features\n",
    "\n",
    "39% of the training data were spam. \n",
    "\n",
    "Average percentage of words or characters in an email message equal to the indicated word or character.\n",
    "We have chosen the words and characters showing the largest difference between spam and email.\n",
    "\n",
    "## Spam Example Results\n",
    "\n",
    "with 3000 training and 1500 test observations, MART fits an additive logistic model\n",
    "$$\n",
    "f(x) = \\log \\frac{Pr(spam|x)}{Pr(email|x)}\n",
    "$$\n",
    "using trees with $J=6$ terminal-node trees. MART achieves a test error of 4%, compared to 5.3% for an additive GAM, 5.5% for MARS,\n",
    "and 8.7% for CART."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
