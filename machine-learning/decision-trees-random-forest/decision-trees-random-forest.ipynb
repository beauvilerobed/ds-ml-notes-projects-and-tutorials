{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypotheses: decision trees $f: X \\rightarrow  Y$\n",
    "- Each internal node tests and attribute $x_i$\n",
    "- One branch for each possible attribute value $x_i = \\nu$\n",
    "- Each leaf assigns a class $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What functions can be represented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees can represent any function of the input attributes\n",
    "- For Boolean functions, path to leaf gives truth table row\n",
    "- Could require exponentially many nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis space\n",
    "\n",
    "![title](img/hypospace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning *simplest* decision tree is NP-hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning the simplist(smallest) decision tree is an NP-complete problem\n",
    "- Resort to a greedy heuristic:\n",
    "    - Start from empty decision tree\n",
    "    - Split on **next best attribute**\n",
    "    - Recurse\n",
    "\n",
    "![title](img/dt-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/recursive-step.png)\n",
    "![title](img/second-level.png)\n",
    "![title](img/full-tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting: choosing a good attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would we prefer to split on $X_1$ or $X_2$?\n",
    "\n",
    "\n",
    "\n",
    "**Idea:** Use counts at leaves to define probability distributions, so we can measure uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring uncertainty\n",
    "\n",
    "- Good split if we are more certain about classification after split\n",
    "    - Deterministic good (all true or all false)\n",
    "    - Uniform distribution bad\n",
    "    - What about distributions in between?\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy $H(Y)$ for a random variable $Y$\n",
    "$$\n",
    "H(Y) = -\\sum_{i=1}^k P(Y=y_i)\\log_{2}P(Y=y_i)\n",
    "$$\n",
    "\n",
    "#### More uncertainty, more entropy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High, Low Entropy\n",
    "\n",
    "- \"High Entropy\"\n",
    "    - Y is from a uniform like distribution\n",
    "    - Flat histogram\n",
    "    - Values sampled from it are less predictable\n",
    "\n",
    "- \"Low Entropy\"\n",
    "    - Y is from a varied (peak and valleys) distribution\n",
    "    - Histogram has many lows and highs\n",
    "    - Values sampled from it are more predictable\n",
    "\n",
    "#### Example:\n",
    "$$\n",
    "P(Y=T) = \\frac{5}{6}\n",
    "$$\n",
    "$$\n",
    "P(Y=F) = \\frac{1}{6}\n",
    "$$\n",
    "$$\n",
    "H(Y) = - \\frac{5}{6} \\log_{2}\\frac{5}{6} - \\frac{1}{6}\\log_{2}\\frac{1}{6} = .65\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropy\n",
    "\n",
    "Conditional Entropy $H(Y|X)$ of a random variable $Y$ conditioned on a random variable $X$\n",
    "$$\n",
    "H(Y|X) = -\\sum_{j=1}^v P(X=x_j)\\sum_{i=1}^k P(Y=y_i|X=x_i)\\log_{2}P(Y=y_i|X=x_j)\n",
    "$$\n",
    "\n",
    "#### Example:\n",
    "$$P(Y=T|X_1 = T) = 4\n",
    "$$\n",
    "$$\n",
    "P(Y=T|X_1 = F) = 1\n",
    "$$\n",
    "$$\n",
    "P(Y=F|X_1 = T) = 0\n",
    "$$\n",
    "$$\n",
    "P(Y=F|X_1 = F) = 1\n",
    "$$\n",
    "$$\n",
    "P(X_1=T) = \\frac{4}{6}\n",
    "$$\n",
    "$$\n",
    "P(X_1=F) = \\frac{2}{6}\n",
    "$$\n",
    "$$\n",
    "H(Y) = - \\frac{4}{6} \\Bigg(1\\log_{2}1 + 0\\log_{2}0 \\Bigg) - \n",
    "\\frac{2}{6} \\Bigg(\\frac{1}{2}\\log_{2}\\frac{1}{2} +\\frac{1}{2}\\log_{2}\\frac{1}{2} \\Bigg) = \\frac{2}{6} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain\n",
    "- Decrease in entropy (uncertainty) after splitting\n",
    "$$\n",
    "IG(X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "\n",
    "In our running example:\n",
    "$$\n",
    "IG(X_1) = H(Y) - H(Y|X) = .65 - .33\n",
    "$$\n",
    "$$\n",
    "IG(X_1) > 0 \\rightarrow \\text{ we prefer the split! }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning decision trees\n",
    "- Start from empty decision tree\n",
    "- Split on **next best attribute (feature)**\n",
    "    - Use, for example, information gain to select attribute:\n",
    "$$\n",
    "    arg\\max_i IG(X_i) = arg\\max_i (H(Y) - H(Y|X_i))\n",
    "$$\n",
    "- Recurse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to stop?\n",
    "\n",
    "#### Base Case 1\n",
    "- Don't split a node if all matching records have the same output values \n",
    "    - e.g. when cylinders = 6 there are 8 bad  and 0 good\n",
    "\n",
    "#### Base Case 2\n",
    "- Don't split a node if data points are identical on remaining attributes\n",
    "    - e.g. when acceleration = medium there is 1 good and 1 bad\n",
    "\n",
    "#### Proposed Base Case 3\n",
    "- If all attributes have small information gain then **don't recurse**\n",
    "    - **This is not a good idea**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem with proposed case 3\n",
    "y = a XOR b\n",
    "\n",
    "|a|b|y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|0|1|1|\n",
    "|1|0|1|\n",
    "|1|1|0|\n",
    "\n",
    "![title](img/info-gain.png)\n",
    "\n",
    "#### If we omit proposed case 3:\n",
    "\n",
    "![title](img/result-tree.png)\n",
    "\n",
    "Instead, perform **pruning** after building a tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees will overfit\n",
    "\n",
    "- Standard decision trees have no learning bias\n",
    "    - Training set error is always zero!\n",
    "        - (If there is not label noise)\n",
    "    - Lots of variance\n",
    "    - Must introduce some bias towards simpler trees\n",
    "\n",
    "- Many strategies for picking simpler trees\n",
    "    - Fixed depth\n",
    "    - Minimum number of samples per leaf\n",
    "\n",
    "- Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Valued inputs\n",
    "What should we do if some of the inputs are real-valued?\n",
    "\n",
    "#### Threshold splits\n",
    "- Binary tree: split on attribute X at value $t$\n",
    "    - One branch: $X < t$\n",
    "    - Other branch: $X \\geq t$\n",
    "\n",
    "- Requires small change\n",
    "    - Allow repeated splits on same variable along a path\n",
    "\n",
    "![title](img/threshold.png)\n",
    "\n",
    "#### The set of possible thresholds\n",
    "- Only a finite number of t's are important:\n",
    "    - Sort data according to X into $\\{ x_1, x_2,..., x_m \\}$\n",
    "    - Consider split points of the form $x_i + \\frac{1}{2}(x_{i+1} - x_i)$\n",
    "    - Moreover, only splits between examples of different classes matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best threshold\n",
    "\n",
    "- Suppose X is real valued with threshold $t$\n",
    "- Want $IG(Y|X:t)$, the information gain for Y when testing if X is greater than or less than $t$\n",
    "- Define:\n",
    "    - $H(Y|X:t) = p(X < t)H(Y|X<t) + p(X \\geq t)H(Y|X \\geq t)$\n",
    "    - $IG(Y|X:t) = H(Y)-H(Y|X:t)$\n",
    "    - $IG^*(Y|X) = \\max_t IG(Y|X:t)$   \n",
    "- Use: $IG^*(Y|X)$ for continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to know about decision trees\n",
    "\n",
    "- Decision trees are one of the most popular ML tools\n",
    "    - Easy to understand, implement, and use\n",
    "    - Computationally cheap (to solve heuristically)\n",
    "\n",
    "- Information gain to select attributes\n",
    "- Presented for classification, can be used for regression(https://www.saedsayad.com/decision_tree_reg.htm) and density estimation too\n",
    "- Decision trees will overfit!!!\n",
    "    - Must use tricks to find \"simple trees\" e.g.\n",
    "        - Fixed depth/Early stopping\n",
    "        - Pruning\n",
    "    - Or, use ensembles of different trees (random forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Variance Without Increasing Bias\n",
    "\n",
    "- Averaging reduces variance:\n",
    "$$\n",
    "Var(\\bar{X}) = \\frac{Var(X)}{N} \\text{ (when predictions are indepenent) }\n",
    "$$\n",
    "\n",
    "Average models to reduce model variance\n",
    "One problem:\n",
    "    only one training set\n",
    "    where do multiple models come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Bootstrap Aggregation\n",
    "\n",
    "- Take repeated **bootstrap samples** from training set D\n",
    "- *Bootstrap sampling*: Given set D containing N training examples, create D' by drawing N examples\n",
    "at random **with replacement** from D.\n",
    "\n",
    "- Bagging:\n",
    "    - Create k boostrap samples $D_1, ..., D_k$\n",
    "    - Train distinct classifiers on each $D_r$\n",
    "    - Classify new instance by majority vote/average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "- Ensemble method specifically designed for decision tree classifiers\n",
    "- Introduce two sources of randomness: \"Bagging\" and \"Random input vectors\"\n",
    "    - Bagging method: each tree is grown using a bootstrap sample of training data\n",
    "    - Random vector method: **At each node**, best split is chosen from a random sample of\n",
    "    m attributes instead of all attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests Algorithm\n",
    "\n",
    "1. For $b=1, ...,B:$\n",
    "    - Draw a bootstrap sample **$Z^*$** of size N from the training data.\n",
    "    - Grow a random-forst tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minumum node size $n_{min}$ is reached.\n",
    "        - Select m variables at random from the p variables\n",
    "        - Pick the best variable/split-point among the m\n",
    "        - split the node into two daughter nodes\n",
    "2. Output the ensemble of trees $\\{ T_b \\}_1^B$\n",
    "\n",
    "To make a prediction at a new point x:\n",
    "\n",
    "#### Regression:\n",
    "$$\n",
    "\\hat{f}_{rf}^B (x) = \\frac{1}{B}\\sum_{b=1}^B T_b(x)\n",
    "$$\n",
    "#### Classification:\n",
    "\n",
    "Let $\\hat{C}_b (x)$ be the class prediction of the b-th random-forest tree. Then \n",
    "$$\n",
    "\\hat{C}_{rf}^B (x) = \\text{majority vote } {\\hat{C}_b (x)}_1^B\n",
    "$$\n",
    "\n",
    "![title](img/rf-flow.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
