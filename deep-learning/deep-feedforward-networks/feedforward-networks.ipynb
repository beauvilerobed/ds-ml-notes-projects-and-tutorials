{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "**Deep feedforward networks** (or **feedforward neural networks**, or **multilayer perceptrons**, MLPs) are fundamental models in deep learning. Their goal is to approximate a target function $f^*$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "For a classifier, $y = f^*(x)$ maps input $x$ to a category $y$. A feedforward network defines $y = f(x;\\theta)$ and learns parameters $\\theta$ to best approximate $f^*$.\n",
    "\n",
    "These models are called **feedforward** because information flows in one direction: there are no **feedback** connections. Networks with feedback are **recurrent neural networks**. Convolutional networks for image recognition are a specialized type of feedforward network. Feedforward networks are represented as a composition of functions over a **directed acyclic graph (DAG)**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Given three layers $f^{(1)}, f^{(2)}, f^{(3)}$, the network computes:\n",
    "\n",
    "$$\n",
    "f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))))\n",
    "$$\n",
    "\n",
    "This is a network of **depth 3**. The last layer is called the **output layer**, while intermediate layers are **hidden layers**. Each hidden layer is typically vector-valued, with dimensionality defining the **width** of the network.\n",
    "\n",
    "Training a feedforward network involves:\n",
    "\n",
    "1. Choosing the **optimizer**, **loss function**, and output unit type.\n",
    "2. Selecting **activation functions** for hidden layers.\n",
    "3. Designing the **architecture**: number of layers, connectivity, and number of units per layer.\n",
    "\n",
    "Feedforward networks are used primarily for **supervised learning** with non-sequential data. Recurrent networks handle sequential data, mapping variable-length sequences $X_k = {x_1, \\dots, x_k}$ to outputs $y_k$.\n",
    "\n",
    "## Single-layer Perceptron\n",
    "\n",
    "A **perceptron** is the simplest feedforward network with **no hidden layers**, only input and output layers. Its output is:\n",
    "\n",
    "$$\n",
    "o = g(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "where $g$ is the activation function (identity, sigmoid $\\sigma(x) = (1 + e^{-x})^{-1}$, or hyperbolic tangent $tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$).\n",
    "\n",
    "**Learning:** The perceptron adjusts weights $$w$$ and bias $$b$$ to classify examples $(x, y)$.\n",
    "\n",
    "![title](img/picture4.png)\n",
    "\n",
    "### Error Function\n",
    "\n",
    "A **loss function** quantifies how well the perceptron matches the target:\n",
    "\n",
    "$$\n",
    "E(X) = \\frac{1}{2N} \\sum_{i=1}^N (o_i - y_i)^2 = \\frac{1}{2N} \\sum_{i=1}^N (g(w \\cdot x_i + b) - y_i)^2\n",
    "$$\n",
    "\n",
    "Minimizing $E(X)$ with respect to $w$ and $b$ improves classification accuracy.\n",
    "\n",
    "### Delta Rule\n",
    "\n",
    "Weights and bias are updated using **gradient descent**:\n",
    "\n",
    "$$\n",
    "w_{i+1} = w_i - \\alpha \\frac{\\partial E(X)}{\\partial w_i}, \\quad\n",
    "b_{i+1} = b_i - \\alpha \\frac{\\partial E(X)}{\\partial b_i}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate**. The **delta rule**, a special case of backpropagation, computes:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\frac{\\alpha}{N} \\sum_{i=1}^N (y_i - o_i) g'(h_i) x_i, \\quad\n",
    "\\Delta b = \\frac{\\alpha}{N} \\sum_{i=1}^N (y_i - o_i) g'(h_i)\n",
    "$$\n",
    "\n",
    "with $h_i = w \\cdot x_i + b$, $o_i = g(h_i)$.\n",
    "\n",
    "### Training Procedure\n",
    "\n",
    "1. **Forward pass:** Compute $h_i$ and $o_i$ for all inputs.\n",
    "2. **Backward pass:** Update $w$ and $b$ using the delta rule.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "Single-layer perceptrons are **linear classifiers**, meaning they can only separate linearly separable data. For example, the XOR function is not linearly separable:\n",
    "\n",
    "![title](img/picture5.png)\n",
    "\n",
    "Nonlinear functions require **multiple layers**, leading to **multi-layer perceptrons (MLPs)**.\n",
    "\n",
    "## Multi-layer Perceptron (MLP)\n",
    "\n",
    "An MLP consists of multiple layers of perceptrons, capable of learning **nonlinear functions**, making them suitable for regression and classification.\n",
    "\n",
    "### Layers\n",
    "\n",
    "MLPs consist of:\n",
    "\n",
    "* Input layer\n",
    "* One or more hidden layers\n",
    "* Output layer\n",
    "\n",
    "![title](img/picture6.png)\n",
    "\n",
    "Feedforward networks form a DAG; outputs of a layer depend only on the previous layer.\n",
    "\n",
    "## Formal Definition\n",
    "\n",
    "1. Hidden layers use activation $g$; output layer uses $g_0$.\n",
    "2. Each perceptron in layer $l_k$ is **fully connected** to all perceptrons in $l_{k-1}$.\n",
    "3. No connections exist between perceptrons in the same layer.\n",
    "\n",
    "![title](img/picture7.png)\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "* Scalars: $w_{ij}^k$, $b_i^k$, $h_i^k$, $o_i^k$, $r_k$\n",
    "* Vectors: $w_i^k$, $o^k$$\n",
    "\n",
    "**Forward computation:**\n",
    "\n",
    "1. Input layer: $o_i^0 = x_i$\n",
    "2. Hidden layers $l_1 \\dots l_{m-1}$:\n",
    "\n",
    "$$\n",
    "h_i^k = w_i^k \\cdot o^{k-1} + b_i^k, \\quad o_i^k = g(h_i^k)\n",
    "$$\n",
    "\n",
    "3. Output layer:\n",
    "\n",
    "$$\n",
    "h_1^m = w_1^m \\cdot o^{m-1} + b_1^m, \\quad o = g_0(h_1^m)\n",
    "$$\n",
    "\n",
    "## Training MLPs\n",
    "\n",
    "Minimize the mean squared error:\n",
    "\n",
    "$$\n",
    "E(X) = \\frac{1}{2N} \\sum_{i=1}^N (o_i - y_i)^2\n",
    "$$\n",
    "\n",
    "Gradient descent updates:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij}^k = -\\alpha \\frac{\\partial E(X)}{\\partial w_{ij}^k}, \\quad\n",
    "\\Delta b_i^k = -\\alpha \\frac{\\partial E(X)}{\\partial b_i^k}\n",
    "$$\n",
    "\n",
    "Backpropagation efficiently computes these derivatives by propagating gradients **backwards** from the output layer. Training proceeds in two phases:\n",
    "\n",
    "1. **Forward pass:** Compute all $h_i^k$ and $o_i^k$.\n",
    "2. **Backward pass:** Compute gradients and update weights and biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
