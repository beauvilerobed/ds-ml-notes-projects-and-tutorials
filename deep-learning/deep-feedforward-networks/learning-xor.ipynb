{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning XOR\n",
    "\n",
    "The XOR (exclusive or) function provides the target function\n",
    "$$y = f^*(x)$$\n",
    "that we want to learn. Our model provides a function\n",
    "$$y = f(x;\\theta)$$\n",
    "and our learning algorithm adapts the parameters $\\theta$ so that $f$ approximates $f^*$.\n",
    "We want our network to perform correctly on the four points\n",
    "$$X = \\{ [0,0], [0,1], [1,0], [1,1] \\}$$\n",
    "\n",
    "We can treat this problem as a regression problem and use mean-squared error (MSE).\n",
    "(*Note: in practice, MSE is not ideal for binary outputs.*)\n",
    "\n",
    "Evaluated over the whole training set, the MSE loss is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4} \\sum_{x \\in X} (f^*(x) - f(x;\\theta))^2\n",
    "$$\n",
    "\n",
    "Now we must choose the form of our model $f(x;\\theta)$. If we choose a **linear model**, with parameters consisting of a weight vector $w$ and bias $b$, then:\n",
    "\n",
    "$$\n",
    "f(x;w,b) = x^T w + b = WX^T\n",
    "$$\n",
    "with $X = [x, 1]$ and $W = [w, b]$. We can minimize $J(\\theta)$ in closed form using the **normal equations**.\n",
    "\n",
    "Solving the normal equations yields\n",
    "\n",
    "$$\n",
    "w = [0, 0], \\qquad b = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0.0, 0.0], b = 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "     [0, 0, 1],\n",
    "     [0, 1 ,1],\n",
    "     [1, 0, 1],\n",
    "     [1, 1 ,1],])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "w1, w2, b = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "w = [w1, w2]\n",
    "print(f'w = {w}, b = {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the linear model outputs **0.5 everywhere**, which obviously cannot represent XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([w1, w2, b])\n",
    "W @ X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Space Transformation - Change in Perspective\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "Solving the XOR problem by learning a representation. The bold numbers on the plot show the required outputs of the XOR function.\n",
    "\n",
    "**(Left)** A linear model applied directly to the inputs cannot solve XOR:\n",
    "\n",
    "* When $x_1 = 0$, the output must **increase** as $x_2$ increases.\n",
    "* When $x_1 = 1$, the output must **decrease** as $x_2$ increases.\n",
    "\n",
    "A linear model uses a fixed coefficient $w_2$ for $x_2$ and therefore cannot change its behavior depending on the value of $x_1$.\n",
    "\n",
    "**(Right)** After transforming the input through a nonlinear feature space, a linear model *can* solve XOR.\n",
    "\n",
    "Because a linear model cannot represent XOR, we require a model that can **learn a new feature space** where XOR becomes linearly separable.\n",
    "\n",
    "To do this, we introduce a **feedforward network with one hidden layer** containing two hidden units.\n",
    "This network computes a hidden vector\n",
    "\n",
    "$$\n",
    "h = f^{(1)}(x; W, c)\n",
    "$$\n",
    "\n",
    "and the output layer applies a linear model to the hidden units:\n",
    "\n",
    "$$\n",
    "y = f^{(2)}(h; w, b)\n",
    "$$\n",
    "\n",
    "Thus the full model is\n",
    "\n",
    "$$\n",
    "f(x; W, c, w, b) = f^{(2)}(f^{(1)}(x))\n",
    "$$\n",
    "\n",
    "![title](img/picture2.png)\n",
    "\n",
    "An example feedforward network for solving XOR, drawn in two styles.\n",
    "- **(Left)** Every unit is shown explicitly.\n",
    "- **(Right)** Each layer is represented as a vector node.\n",
    "This network has one hidden layer with two units.\n",
    "\n",
    "Most neural networks compute hidden units using an **affine transformation** followed by a nonlinear **activation function**:\n",
    "\n",
    "$$\n",
    "h = g(W^T x + c)\n",
    "$$\n",
    "\n",
    "Here, $W$ contains the weights, $c$ contains the biases, and the activation function $g$ is applied **element-wise**:\n",
    "\n",
    "$$\n",
    "h_i = g(x^T W_{:,i} + c_i)\n",
    "$$\n",
    "\n",
    "Modern networks typically use the **Rectified Linear Unit (ReLU)**:\n",
    "\n",
    "$$\n",
    "g(z) = \\max\\{0, z\\}\n",
    "$$\n",
    "\n",
    "![title](img/picture3.png)\n",
    "\n",
    "The rectified linear activation function. ReLU is the default recommended activation for most feedforward networks.\n",
    "\n",
    "Now we can specify the complete network:\n",
    "\n",
    "$$\n",
    "f(x; W, c, w, b) = w^T \\max\\{0, W^T x + c\\} + b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to the XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now specify a solution to the XOR problem. Let the first-layer weight matrix and bias vector be:\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output-layer parameters are\n",
    "\n",
    "$$\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "-1\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "b = 0\n",
    "$$\n",
    "\n",
    "Now define the input matrix:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 1: Compute the affine transformation ($XW$)\n",
    "\n",
    "$$\n",
    "XW =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "2 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 2: Add the bias vector\n",
    "\n",
    "We add c to each row:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 3: Apply the ReLU activation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These hidden-layer features lie in a space where a linear model can solve XOR.\n",
    "\n",
    "### Step 4: Compute the output layer\n",
    "\n",
    "Multiply each hidden vector by ($w = [1, -1]^T$):\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{bmatrix}\n",
    "0 \\cdot 1 + 0 \\cdot (-1) \\\\\n",
    "1 \\cdot 1 + 0 \\cdot (-1) \\\\\n",
    "1 \\cdot 1 + 0 \\cdot (-1) \\\\\n",
    "2 \\cdot 1 + 1 \\cdot (-1)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1 \\\\ 1 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Finally, apply the XOR target pattern:\n",
    "\n",
    "$$\n",
    "\\text{XOR}(x_1, x_2) =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1 \\\\ 1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To produce the final XOR output, we simply adjust the second-layer parameters slightly (or shift the bias), but the structure itself illustrates how the hidden layer makes XOR linearly separable. The key point is that the hidden features:\n",
    "\n",
    "$$\n",
    "h = \\max\\{0, XW + c\\}\n",
    "$$\n",
    "\n",
    "move the points into a space where a linear classifier **can** correctly separate XOR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
