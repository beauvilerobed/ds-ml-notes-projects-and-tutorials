{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks (CNNs)\n",
    "\n",
    "**Convolutional Networks** are specialized neural networks for data with a **grid-like structure**, such as:\n",
    "\n",
    "* **1-D grids:** time series, audio signals, sensor readings\n",
    "* **2-D grids:** images, video frames, spectrograms\n",
    "\n",
    "CNNs replace some or all **matrix multiplications** in standard fully connected networks with **convolutions**, a linear operation that:\n",
    "\n",
    "* Exploits **local correlations** in the data\n",
    "* **Reduces the number of parameters** via weight sharing\n",
    "* Preserves spatial or temporal structure\n",
    "* Improves **generalization** for structured data\n",
    "\n",
    "\n",
    "\n",
    "## The Convolution Operation\n",
    "\n",
    "### Continuous 1-D Convolution\n",
    "\n",
    "For **continuous functions** $x(t)$ and $w(t)$:\n",
    "\n",
    "$$\n",
    "s(t) = (x * w)(t) = \\int_{-\\infty}^{\\infty} x(a) w(t-a) da\n",
    "$$\n",
    "\n",
    "This slides the function $w$ over $x$, weighting and summing local contributions.\n",
    "\n",
    "### Discrete 1-D Convolution\n",
    "\n",
    "For **discrete signals**, the integral becomes a sum:\n",
    "\n",
    "$$\n",
    "s[t] = (x * w)[t] = \\sum_{a=-\\infty}^{\\infty} x[a] w[t-a]\n",
    "$$\n",
    "\n",
    "* $x[a]$ = input value at position $a$\n",
    "* $w[t-a]$ = kernel weight for relative position $t-a$\n",
    "* $s[t]$ = output (feature map) at position $t$\n",
    "\n",
    "In practice, sequences are finite and **“valid” convolution** sums only where the kernel fully overlaps the input.\n",
    "\n",
    "\n",
    "\n",
    "### 2-D Convolution (Images)\n",
    "\n",
    "For an image $I$ and kernel $K$:\n",
    "\n",
    "$$\n",
    "S[i,j] = (I * K)[i,j] = \\sum_m \\sum_n I[m,n] K[i-m, j-n]\n",
    "$$\n",
    "\n",
    "* $S[i,j]$ = feature map at position $(i,j)$\n",
    "* $K[m,n]$ = kernel weights, usually small (e.g., $3 \\times 3$)\n",
    "* Each output depends only on a **local patch** of the input\n",
    "\n",
    "**Cross-correlation**, used in most CNN implementations, does not flip the kernel:\n",
    "\n",
    "$$\n",
    "S[i,j] = \\sum_m \\sum_n I[i+m, j+n] K[m,n]\n",
    "$$\n",
    "\n",
    "This is equivalent to convolution in terms of learning.\n",
    "\n",
    "\n",
    "\n",
    "### Numerical Example: 1-D Convolution\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "x = [1,2,3,4], \\quad w = [1,0,-1]\n",
    "$$\n",
    "\n",
    "Compute **valid convolution** $s = x * w$:\n",
    "\n",
    "* Position 0:\n",
    "\n",
    "$$\n",
    "s[0] = 1\\cdot 1 + 2\\cdot 0 + 3\\cdot(-1) = -2\n",
    "$$\n",
    "\n",
    "* Position 1:\n",
    "\n",
    "$$\n",
    "s[1] = 2\\cdot 1 + 3\\cdot 0 + 4\\cdot(-1) = -2\n",
    "$$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$\n",
    "s = [-2, -2]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Sparse Interactions & Parameter Sharing\n",
    "\n",
    "* **Sparse interactions:** Each output unit depends on a **small local patch** of size equal to the kernel.\n",
    "* **Parameter sharing:** The **same kernel** is applied across all positions, drastically reducing the number of parameters.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Input length $m = 10$, output length $n = 8$, kernel size $k=3$\n",
    "* Fully connected: $m \\cdot n = 80$ parameters\n",
    "* Convolution: $k = 3$ parameters (shared) → huge reduction\n",
    "\n",
    "\n",
    "\n",
    "## Equivariance to Translation\n",
    "\n",
    "A function $f$ is **equivariant** to transformation $g$ if:\n",
    "\n",
    "$$\n",
    "f(g(x)) = g(f(x))\n",
    "$$\n",
    "\n",
    "* For convolution, shifting the input by 1 shifts the output by 1:\n",
    "\n",
    "$$\n",
    "I'(x, y) = I(x-1, y) \\quad \\Rightarrow \\quad (I' * K) = (I * K)'\n",
    "$$\n",
    "\n",
    "This **preserves spatial relationships** and is why CNNs excel for vision.\n",
    "\n",
    "\n",
    "\n",
    "## Pooling\n",
    "\n",
    "Pooling reduces **spatial resolution** and adds **translation invariance**. Common operations:\n",
    "\n",
    "* **Max pooling:** $s[i,j] = \\max{S[\\text{patch around } (i,j)]}$\n",
    "* **Average pooling:** $s[i,j] = \\frac{1}{N}\\sum S[\\text{patch}]$\n",
    "* **L2 pooling:** $s[i,j] = \\sqrt{\\sum S[\\text{patch}]^2}$\n",
    "\n",
    "Pooling reduces computation in subsequent layers and increases **robustness** to small translations.\n",
    "\n",
    "\n",
    "\n",
    "### Example: 1-D Max Pooling\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "S = [1,3,2,4], \\quad \\text{pool size}=2, \\text{stride}=2\n",
    "$$\n",
    "\n",
    "* Pool region 1: $\\max(1,3) = 3$\n",
    "* Pool region 2: $\\max(2,4) = 4$\n",
    "\n",
    "Result:\n",
    "\n",
    "$$\n",
    "s_\\text{pooled} = [3,4]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Efficiency Example\n",
    "\n",
    "Edge detection in an image:\n",
    "\n",
    "* Input: $320 \\times 280$\n",
    "* Kernel: width 2, horizontal edges\n",
    "* Output: $319 \\times 280$\n",
    "\n",
    "**FLOPs for convolution:**\n",
    "\n",
    "$$\n",
    "319 \\cdot 280 \\cdot 2 \\approx 178{,}640\n",
    "$$\n",
    "\n",
    "**Fully connected layer equivalent:**\n",
    "\n",
    "$$\n",
    "320 \\cdot 280 \\cdot 319 \\cdot 280 \\approx 8\\text{ billion entries}\n",
    "$$\n",
    "\n",
    "Thus convolution is **orders of magnitude more efficient**.\n",
    "\n",
    "\n",
    "\n",
    "## 2-D Numerical Example\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "I = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}, \\quad\n",
    "K = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute **valid convolution** $S = I * K$:\n",
    "\n",
    "* $S[0,0] = 1\\cdot 1 + 2\\cdot 0 + 4\\cdot 0 + 5\\cdot(-1) = -4$\n",
    "* $S[0,1] = 2\\cdot 1 + 3\\cdot 0 + 5\\cdot 0 + 6\\cdot(-1) = -4$\n",
    "* $S[1,0] = 4\\cdot 1 + 5\\cdot 0 + 7\\cdot 0 + 8\\cdot(-1) = -4$\n",
    "* $S[1,1] = 5\\cdot 1 + 6\\cdot 0 + 8\\cdot 0 + 9\\cdot(-1) = -4$\n",
    "\n",
    "Resulting feature map:\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix} -4 & -4 \\\\ -4 & -4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Convolutional Layers\n",
    "\n",
    "1. **Convolution:** extract local features with a shared kernel\n",
    "2. **Nonlinearity:** apply activation function (ReLU, sigmoid, etc.)\n",
    "3. **Pooling:** reduce dimensions, gain invariance to small translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
