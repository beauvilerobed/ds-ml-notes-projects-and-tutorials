{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Tutorial\n",
    "\n",
    "A typical CNN has the following structure:\n",
    "\n",
    "1. **Input Layer**\n",
    "2. **Convolution Layer + Activation Function**\n",
    "3. **Pooling Layer**\n",
    "4. **Fully Connected Layer (Classification)**\n",
    "\n",
    "<img src=\"img/ex11.png\">\n",
    "\n",
    "\n",
    "\n",
    "## Input Layer\n",
    "\n",
    "* The input layer receives **raw image data**.\n",
    "* Images can be **grayscale (1 channel)** or **RGB (3 channels)**.\n",
    "* Pixel values usually range from **0 to 255**. To stabilize training, we **normalize** pixel values:\n",
    "\n",
    "$$\n",
    "x_\\text{normalized} = \\frac{x}{255}, \\quad x_\\text{normalized} \\in [0,1]\n",
    "$$\n",
    "\n",
    "* Example: Pixel value $128 \\to 128/255 \\approx 0.502$\n",
    "\n",
    "* If the input is an RGB image of size $H \\times W$, each channel is normalized separately.\n",
    "\n",
    "\n",
    "\n",
    "## Convolution Layer\n",
    "\n",
    "The **convolution layer** extracts **local features** like edges, corners, and textures.\n",
    "\n",
    "* Let input image $I$ be of size $H \\times W$ and filter (kernel) $K$ of size $k_h \\times k_w$.\n",
    "* The **feature map** $S$ is calculated as a **2-D discrete convolution**:\n",
    "\n",
    "$$\n",
    "S[i,j] = \\sum_{m=0}^{k_h-1} \\sum_{n=0}^{k_w-1} I[i+m, j+n], K[m,n]\n",
    "$$\n",
    "\n",
    "* **Stride $s$:** Number of pixels the kernel moves each step. Larger stride → smaller feature map\n",
    "* **Activation function:** Introduces **nonlinearity**. Example: ReLU\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "* Input patch:\n",
    "\n",
    "$$\n",
    "I_\\text{patch} = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 1 & 0 \\\\ 2 & 1 & 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Filter (vertical edge detector):\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute **convolution at top-left corner**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "S[0,0] &= 1\\cdot 1 + 2\\cdot 0 + 1\\cdot(-1) \\\\\n",
    "&+ 0\\cdot 1 + 1\\cdot 0 + 0\\cdot(-1) \\\\\n",
    "&+ 2\\cdot 1 + 1\\cdot 0 + 2\\cdot(-1) \\\\\n",
    "&= 1 - 1 + 0 + 2 - 2 = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* The **feature map is smaller** than the original patch.\n",
    "* Increasing stride reduces the size further.\n",
    "\n",
    "\n",
    "\n",
    "## Pooling Layer\n",
    "\n",
    "Pooling **reduces spatial dimensions** while preserving important information.\n",
    "\n",
    "* Common types:\n",
    "\n",
    "1. **Max Pooling:** $s[i,j] = \\max{S[\\text{patch around }(i,j)]}$\n",
    "2. **Average Pooling:** $s[i,j] = \\frac{1}{N} \\sum S[\\text{patch}]$\n",
    "\n",
    "* Example: 2×2 max pooling, stride 2, on\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max(1,3,2,4) = 4\n",
    "$$\n",
    "\n",
    "* Pooling **reduces computation** in later layers and adds **translation invariance** for small shifts.\n",
    "\n",
    "* Larger pooling windows reduce feature map size more aggressively.\n",
    "\n",
    "\n",
    "\n",
    "### 2-D Max Pooling Example\n",
    "\n",
    "* Input:\n",
    "\n",
    "$$\n",
    "S = \\begin{bmatrix} 1 & 3 & 2 & 4 \\\\ 5 & 6 & 7 & 8 \\\\ 2 & 1 & 0 & 3 \\\\ 4 & 5 & 2 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Pool size = 2, stride = 2\n",
    "\n",
    "* Pool regions and max values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max\\begin{bmatrix}1 & 3 \\\\ 5 & 6\\end{bmatrix} &= 6 \\\\\n",
    "\\max\\begin{bmatrix}2 & 4 \\\\ 7 & 8\\end{bmatrix} &= 8 \\\\\n",
    "\\max\\begin{bmatrix}2 & 1 \\\\ 4 & 5\\end{bmatrix} &= 5 \\\\\n",
    "\\max\\begin{bmatrix}0 & 3 \\\\ 2 & 1\\end{bmatrix} &= 3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* Resulting pooled map:\n",
    "\n",
    "$$\n",
    "S_\\text{pooled} = \\begin{bmatrix} 6 & 8 \\\\ 5 & 3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Fully Connected Layer (FC)\n",
    "\n",
    "After convolution + pooling, feature maps are **flattened** into a vector.\n",
    "\n",
    "* Each neuron in the FC layer connects to **every input feature**:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_i w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "* Example: MNIST digit classification (10 classes)\n",
    "* Final layer usually uses **softmax** to produce probabilities:\n",
    "\n",
    "$$\n",
    "P(y=i|x) = \\frac{e^{z_i}}{\\sum_{j=0}^{9} e^{z_j}}\n",
    "$$\n",
    "\n",
    "* Softmax ensures $0 \\le P(y=i) \\le 1$ and $\\sum_i P(y=i) = 1$\n",
    "\n",
    "\n",
    "\n",
    "## Digit Recognizer Example (Concrete Pipeline)\n",
    "\n",
    "1. Input: $28 \\times 28$ grayscale image of a handwritten digit\n",
    "2. **Convolution Layer:** Detect edges and local patterns\n",
    "\n",
    "   * Example: $3 \\times 3$ filters, stride 1 $\\rightarrow$ feature map $26 \\times 26$\n",
    "3. **Activation:** ReLU → apply $\\max(0,x)$ elementwise\n",
    "4. **Pooling Layer:** $2 \\times 2$ max pooling, stride 2 → reduces map to $13 \\times 13$\n",
    "5. **Flatten:** Convert pooled feature maps to vector of length $13 \\cdot 13 \\cdot \\text{num filters}$\n",
    "6. **Fully Connected Layer:** Map vector to 10 neurons → softmax → predicted digit probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(os.listdir('input/'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"input/test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "X_train = train.drop(labels = [\"label\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts(dropna=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "plt.title(\"Number of digit classes\")\n",
    "\n",
    "Y_train_vc = Y_train.value_counts(dropna=False).reset_index()\n",
    "print(Y_train_vc)\n",
    "\n",
    "Y_vc_x = Y_train_vc['label']\n",
    "Y_vc_y = Y_train_vc['count']\n",
    "\n",
    "sns.barplot(x=Y_vc_x, y=Y_vc_y, palette=\"icefire\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some samples\n",
    "img = X_train.iloc[0].to_numpy()\n",
    "img = img.reshape((28,28))\n",
    "\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.title(train.iloc[0,0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some samples\n",
    "img = X_train.iloc[3].to_numpy()\n",
    "img = img.reshape((28,28))\n",
    "\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.title(train.iloc[3,0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization, Reshape and Label Encoding\n",
    "\n",
    "- Normalization\n",
    "    - We perform a grayscale normalization to reduce the effect of illumination's differences.\n",
    "    - If we perform normalization, CNN works faster.\n",
    "- Reshape\n",
    "    - Train and test images (28 x 28)\n",
    "    - We reshape all data to 28x28x1 3D matrices.\n",
    "    - Keras needs an extra dimension in the end which correspond to channels. \n",
    "    \n",
    "Our images are gray scaled so it use only one channel.\n",
    "- Label Encoding\n",
    "    - Encode labels to one hot vectors\n",
    "        - 2 => [0,0,1,0,0,0,0,0,0,0]\n",
    "        - 4 => [0,0,0,0,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0\n",
    "\n",
    "print(\"x_train shape: \",X_train.shape)\n",
    "print(\"test shape: \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "print(\"x_train shape: \",X_train.shape)\n",
    "print(\"test shape: \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(Y_train, num_classes=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)\n",
    "\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_val.shape)\n",
    "print(\"y_train shape\",Y_train.shape)\n",
    "print(\"y_test shape\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples\n",
    "plt.imshow(X_train[2][:,:,0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer ,loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  # for better result increase the epochs\n",
    "batch_size = 250"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "- To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n",
    "- Alter the training data with small transformations to reproduce the variations of digit.\n",
    "- For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated.\n",
    "\n",
    "<img src=\"img/ex10.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # dimesion reduction\n",
    "        rotation_range=5,  # randomly rotate images in the range 5 degrees\n",
    "        zoom_range = 0.1, # Randomly zoom image 10%\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally 10%\n",
    "        height_shift_range=0.1,  # randomly shift images vertically 10%\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val), \n",
    "                              steps_per_epoch=X_train.shape[0] // batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")\n",
    "plt.title(\"Test Loss\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis = 1) \n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "\n",
    "f,ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
