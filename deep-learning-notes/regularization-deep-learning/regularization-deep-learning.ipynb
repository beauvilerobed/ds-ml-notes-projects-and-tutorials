{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central problem in machine learning is how to make an algorithm that willperform well not just on the training data, but also on new inputs. Many strategiesused in machine learning are explicitly designed to reduce the test error, possiblyat the expense of increased training error. These strategies are known collectivelyas regularization. \n",
    "\n",
    "**Definition**: We deﬁned regularization as “any modiﬁcation we make to alearning algorithm that is intended to reduce its generalization error but not itstraining error.”\n",
    "\n",
    "In the context of deep learning, most regularization strategies are based onregularizing estimators. Regularization of an estimator works by trading increasedbias for reduced variance. An eﬀective regularizer is one that makes a proﬁtabletrade, reducing variance signiﬁcantly while not overly increasing the bias.\n",
    "\n",
    "Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe. To some extent, we are always trying to ﬁt a square peg (the data-generating process) intoa round hole (our model family).\n",
    "\n",
    "We might ﬁnd—and indeed in practical deep learning scenarios,we almost always do ﬁnd—that the best ﬁtting model (in the sense of minimizinggeneralization error) is a large model that has been regularized appropriately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Norm Penalties\n",
    "\n",
    "Many regularization approaches are based on limiting the capacity of models,such as neural networks, linear regression, or logistic regression, by adding a parameter norm penalty $\\Omega(\\theta)$ to the objective function J. We denote the regularized objective function by $\\tilde{J}$:\n",
    "\n",
    "$$\n",
    "\\tilde{J}(\\theta;\\textbf{X},y) = J(\\theta;\\textbf{X},y) + \\alpha\\Omega(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\alpha\\in[0,\\infty)$ is a hyperparameter that weights the relative contribution of the norm penalty term, $\\Omega$, relative to the standard objective function J. Setting $\\alpha = 0$ results in no regularization. Larger values of $\\alpha$ correspond to more regularization. Minimizes the regularized objective function $\\tilde{J}$ it will decrease both the original objectiveJon the training data and some measure of the size of the parameters $\\theta$ (or some subset of the parameters).\n",
    "\n",
    "**Note:** for neural networks, we typically choose to use a parameter norm penalty $\\Omega$ that penalizes only the weights of the affine transformation at each layer and leaves the baises unregularized. This means that we do not induce too muchvariance by leaving the biases unregularized. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^2$ Parameter Regularization\n",
    "\n",
    "one of the simplest and most common kinds of parameter norm penalty: the $L^2$ parameter norm penalty commonly known as **weight decay**. This regularization strategy drives the weights closer to the origin1by adding a regularization term $\\Omega(θ) = \\frac{1}{2}\\Vert w \\Vert_2^2$ to the objective function. $L^2$ regularization is also known as **ridge regression**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can gain some insight into the behavior of weight decay regularizationby studying the gradient of the regularized objective function. To simplify thepresentation, we assume no bias parameter, so $\\theta$ is just $w$. Such a model has thefollowing total objective function:\n",
    "\n",
    "$$\n",
    "\\tilde{J}(w;,\\textbf{X}, y) = \\frac{\\alpha}{2}w^Tw + J(w;,\\textbf{X}, y)\n",
    "$$\n",
    "\n",
    "with corresponding parameter gradient\n",
    "\n",
    "$$\n",
    "\\nabla_w\\tilde{J}(w;,\\textbf{X}, y) = \\alpha w + \\nabla_w J(w;,\\textbf{X}, y)\n",
    "$$\n",
    "\n",
    "To take a single gradient step to update the weights, we perform this update:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_w J(w;,\\textbf{X}, y))\n",
    "$$\n",
    "$$\n",
    "\\iff\n",
    "$$\n",
    "$$\n",
    "w \\leftarrow (1-\\epsilon\\alpha)w -\\epsilon\\nabla_w J(w;\\textbf{W}, y)\n",
    "$$\n",
    "\n",
    "We can see that the addition of the weight decay term has modiﬁed the learningrule to multiplicatively shrink the weight vector by a constant factor on each step,just before performing the usual gradient update. This describes what happens ina single step. But what happens over the entire course of training?\n",
    "\n",
    "We will further simplify the analysis by making a quadratic approximationto the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost, $w_∗= \\argmin_w J(w)$. If the objectivefunction is truly quadratic, as in the case of ﬁtting a linear regression model with mean squared error, then the approximation is perfect. The approximation $\\hat{J}$ is given by\n",
    "\n",
    "$$\n",
    "\\hat{J}(\\theta) = J(w_*) + \\frac{1}{2}(w-w^*)^T\\textbf{H}(w-w^*)\n",
    "$$\n",
    "\n",
    "where H is the Hessian matrix of J with respect to w evaluated at $w^*$. There is no first-order term in this quadratic approximation, because $w^*$ is the location of a minimum of J, we can conclude that $\\textbf{H}$ is positive semidefinite. The minimus of $\\hat{J}$ occurs where its gradient\n",
    "\n",
    "$$\n",
    "\\nabla_w\\hat{J}(w) = \\textbf{H} (w-w^*)\n",
    "$$\n",
    "\n",
    "is equal to $\\textbf{0}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the effect of weight decay, modify above equation by adding the weight decay gradient. We can now solve for the minimum of the regularized version of $\\hat{J}$. We use the variable $\\hat{w}$ to represent the location of the minimum.\n",
    "\n",
    "$$\n",
    "\\alpha\\tilde{w} + \\textbf{H}(\\tilde{w}- w^*)=0\n",
    "$$\n",
    "$$\n",
    "(\\textbf{H}-\\alpha\\textbf{I})\\tilde{w} = \\textbf{H}w^*\n",
    "$$\n",
    "$$\n",
    "\\tilde{w} = (\\textbf{H}-\\alpha\\textbf{I})^{-1}\\textbf{H}w^*\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\alpha$ approaches 0, the regularized solution $\\tilde{w}$ approaches $w^∗$. But what happens as $\\alpha$ grows? Because $\\textbf{H}$ is real and symmetric, we can decompose it into a diagonal matrix $\\Lambda$ and an orthonormal basis of eigenvectors, Q, such that $H = Q\\Lambda Q^T$. So we obtain\n",
    "\n",
    "$$\n",
    "\\tilde{w} = (Q\\Lambda Q^T-\\alpha\\textbf{I})^{-1}Q\\Lambda Q^Tw^*\n",
    "$$\n",
    "$$\n",
    "\\tilde{w} = (Q(\\Lambda -\\alpha\\textbf{I})Q^T)^{-1}Q\\Lambda Q^Tw^*\n",
    "$$\n",
    "$$\n",
    "\\tilde{w} = Q(\\Lambda -\\alpha\\textbf{I})^{-1}\\Lambda Q^Tw^*\n",
    "$$\n",
    "\n",
    "We see that the eﬀect of weight decay is to rescale $w^∗$ along the axes deﬁned bythe eigenvectors of $\\textbf{H}$. Speciﬁcally, the component of $w^∗$ that is aligned with the i-th eigenvector of $\\textbf{H}$ is rescaled by a factor of $\\frac{\\lambda_i}{\\lambda_i+\\alpha}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the directions where the eigenvalues of $\\textbf{H}$ are relatively large, for example,where $\\lambda_i  >> \\alpha$, the eﬀect of regularization is relatively small. Yet components with $\\lambda_i << α$ w ill be shrunk to have nearly zero magnitude.\n",
    "\n",
    "<img src=img/download.png>\n",
    "\n",
    "An illustration of the effect of $L^2$(or weight decay) regularization on the value of the optimal w. The solid ellipses represent contours of equal value of the unregularized objective. The dotted circles represent contours of equal value of theL2regularizer. At the point $\\tilde{w}$, these competing objectives reach an equilibrium. In the ﬁrst dimension, the eigenvalue of the Hessian of J is small. The objective function does not increase much when moving horizontally away from $w^∗$. Because the objective function does not express a strong preference along this direction, the regularizer has a strong effect on this axis.The regularizer pulls $w_1$ close to zero. In the second dimension, the objective function is very sensitive to movements away from $w^∗$. The corresponding eigenvalue is large,indicating high curvature. As a result, weight decay affects the position of $w^2$ relatively little."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with actual quadratic cost function\n",
    "\n",
    "For linear regression, the cost function is the sum of squared errors\n",
    "\n",
    "$$\n",
    "(Xw - y)^T(Xw-y)\n",
    "$$\n",
    "\n",
    "when we add $L^2$ regularization, the objective function changes to \n",
    "\n",
    "$$\n",
    "(Xw - y)^T(Xw-y) + \\frac{1}{2}\\alpha w^Tw\n",
    "$$\n",
    "\n",
    "This changes the normal equations for the solution from\n",
    "\n",
    "$$\n",
    "w = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "to\n",
    "\n",
    "$$\n",
    "w = (X^TX + \\alpha I)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "The diagonal entries of this matrix correspond to the variance of each input feature. We can see that $L^2$ regularization causes the learning algorithm to “perceive” the input X as having higher variance, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L^1$ Regularization\n",
    "\n",
    "Formally, $L^1$ regularization on the model parameter w is deﬁned as\n",
    "\n",
    "$$\n",
    "\\Omega(\\theta) = \\Vert w \\Vert_1 = \\sum_i |w_i|\n",
    "$$\n",
    "\n",
    "We are interested in delineating the differences between $L^1$ and $L^2$ forms of regularization. $L^1$ weight decay controls the strengthof the regularization by scaling the penalty $\\Omega$ using a positive hyperparameter $\\alpha$.\n",
    "\n",
    "Thus, the regularized objective function $\\tilde{J}(w; X, y)$ is given by\n",
    "$$\n",
    "\\tilde{J}(w;X,y) = \\alpha\\Vert w \\Vert_1 + J(w;X,y)\n",
    "$$\n",
    "\n",
    "with corresponing graadient (actually, sub gradient)\n",
    "\n",
    "$$\n",
    "\\nabla_w\\tilde{J}(w;X,y) = \\alpha sign(w)+ \\nabla_w J(X,y;w)\n",
    "$$\n",
    "\n",
    "Where sign(w) is simply the sign of w applied element-wise. We can see that the regularization contribution to the gradient no longer scales linearly with each $w_i$; instead it is a constant factor with a sign equal to $sign(w_i)$. One consequence of this form of the gradient is that we will not necessarily see clean algebraic solutions to quadratic approximations of $J(X, y;w$) as we did for $L^2$ regularization.\n",
    "\n",
    "Because the $L^1$ penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we will also make the further simplifying assumption that the Hessian is diagonal, $H = diag([H11, . . . , Hnn])$, where each $H_{ii}>0$.\n",
    "\n",
    "This assumption holds if the data for the linear regression problem has beenpreprocessed to remove all correlation between the input features, which may beaccomplished using PCA\n",
    "\n",
    "Our quadratic approximation of the $L^1$ regularized objective function decomposes into a sum over the parameters:\n",
    "\n",
    "$$\n",
    "\\hat{J}(w;X,y) = J(w^*;X,y) + \\sum_i\\left[\\frac{1}{2} H_{ii}(w_i-w_i^*)^2+\\alpha|w_i|\\right]\n",
    "$$\n",
    "\n",
    "The problem of minimizing this approximate cost function has an analytical solution(for each dimension i), with the following form:\n",
    "\n",
    "$$\n",
    "w_i = sign(w_i^*)\\max \\left\\{|w_i^*| - \\frac{\\alpha}{H_{ii}}, 0\\right\\}\n",
    "$$\n",
    "\n",
    "Consider the situation where $w_i^* > 0$ for all i. There are two possible outcomes:\n",
    "\n",
    "1. The case where $w_i^* \\leq \\frac{\\alpha}{H_{ii}}$. Here the optimal value of $w_i$ under the regularized objective is simply $w_i= 0$. This occurs because the contribution of $J(w;X, y)$ to the regularized objective $\\tilde{J}(w;X, y)$ is overwhelmed—in direction i—by the $L^1$ regularization, which pushes the value of $w_i$ to zero.\n",
    "\n",
    "2. The case where $w^∗_i > \\alpha H_{ii}$. In this case, the regularization does not move the optimal value of $w_i$ to zero but instead just shifts it in that direction by adistance equal to $\\frac{\\alpha}{H_{ii}}$\n",
    "\n",
    "A similar process happens when $w^∗_i < 0$, but with the $L^1$ penalty making $w_i$ less negative by $\\alpha H_{ii}$ or 0. In comparison to $L^2$ regularization, $L^1$ regularization results in a solution thatis more **sparse**. Sparsity in this context refers to the fact that some parametershave an optimal value of zero. \n",
    "\n",
    "The sparsity of $L^1$ regularization is a qualitatively different behavior than arises with $L^2$ regularization. Using the assumption of a diagonal and positive deﬁnite Hessian $\\textbf{H}$ that we introduced for our analysis of $L^1$ regularization, we ﬁnd that \n",
    "$\\tilde{w_i} = \\frac{H_{ii}}{H_{ii} + \\alpha} w^∗_i$. \n",
    "If $w_i^*$ was nonzero, then $\\tilde{w_i}$ remains nonzero. This demonstrates that $L^2$ regularization does not cause the parameters to become sparse, while $L^1$ regularization may do so for large enough $\\alpha$.\n",
    "\n",
    "The sparsity property induced by $L^1$ regularization has been used extensively as a **feature selection** mechanism. Feature selection simpliﬁes a machine learning problem by choosing which subset of the available features should be used. In particular, the well known LASSO (Tibshirani, 1995) (least absolute shrinkage and selection operator) model integrates an $L^1$ penalty with a linear model and a least-squares cost function. The $L^1$ penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that many regularization strategies can be interpretedas MAP Bayesian inference, and that in particular, $L^2$ regularization is equivalent to MAP Bayesian inference with a Gaussian prior on the weights. For $L^1$ regularization, the penalty $\\alpha \\Omega(w) = \\alpha\\sum_i|w_i|$ used to regularize a cost function is equivalent to the log-prior term that is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution over $w \\in \\mathbb{R}^n$.\n",
    "\n",
    "$$\n",
    "\\log p(w) = \\sum_i \\log Laplace\\left(w_i;0, \\frac{1}{\\alpha}\\right) = \n",
    "-\\alpha\\Vert w\\Vert_1 + n\\log\\alpha - n\\log 2\n",
    "$$\n",
    "\n",
    "From the point of view of learning via maximization w.r.t. to w, we can ignore the $\\log \\alpha - \\log 2$ terms because they do not depend on w."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of regularization using python\n",
    "\n",
    "Overfitting is a big concern for any business that uses deep learning models to make predictions. For example, if a company wants to predict customer retention, an overfit model may represent random noise and outliers in the data as significant statistical trends. As a result, the model will perform poorly when used to predict if a customer will make a repeat purchase in the future, resulting in significant revenue loss for the company. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telco Customer Churn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('telco_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'] = np.where(df['Churn'] == 'Yes', 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write a function that takes a list of categorical column names and modifies our data frame to include the categorical codes for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categories(cat_list):\n",
    "    for col in cat_list:\n",
    "        df[col] = df[col].astype('category')\n",
    "        df[f'{col}_cat'] = df[f'{col}_cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = ['gender', 'Partner', 'Dependents', 'PhoneService', \n",
    "'MultipleLines', 'InternetService',\n",
    "                  'OnlineSecurity', 'OnlineBackup', \n",
    "'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                  'StreamingMovies', 'Contract', 'PaperlessBilling', \n",
    "'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_categories(category_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  ['gender_cat', 'Partner_cat', 'Dependents_cat', 'PhoneService_cat', 'MultipleLines_cat', 'InternetService_cat',\n",
    "                  'OnlineSecurity_cat', 'OnlineBackup_cat', 'DeviceProtection_cat', 'TechSupport_cat', 'StreamingTV_cat',\n",
    "                  'StreamingMovies_cat', 'Contract_cat', 'PaperlessBilling_cat', 'PaymentMethod_cat','MonthlyCharges'\n",
    "                  'TotalCharges', 'SeniorCitizen', 'tenure']\n",
    "X = df[cols]\n",
    "y = df['Churn']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s import the train/test split method for the model selection module in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_hold_out, y_train, y_test_hold_out = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add a dense layer using the add method. \n",
    "- We need to pass in the number of features, which is the length of the columns list, and the input, which is a tuple with the length of the column list. \n",
    "- We will also initialize weight values according to a normal distribution and using a rectified linear unit (ReLu) activation function (activation functions are what simulates neurons firing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(len(cols), input_shape=(len(cols),), kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to add the output layer, which will have one neuron and a softmax activation function. This will allow our model to output class probabilities for predicting whether a customer will churn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,epochs =20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_hold_out)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_pred, y_test_hold_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1 Regularization) \n",
    "\n",
    "Let’s see if we can improve performance with lasso regression. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras makes implementing lasso regression with neural network models straightforward. The regularizers package in Keras has a method that we can call, named l1,  in the layers of our neural network. This will apply penalty terms to the weights in the layers which will help prevent overfitting.\n",
    "\n",
    "Typically, lasso regression sends insignificant feature weights to zero, allowing the model to include the most important features for making accurate predictions. Let’s import the regularizer package from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the input layer, we will pass in a value for the kernel_regularizer using the l1 method from the regularizers package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.add(Dense(len(cols),input_shape=(len(cols),), \n",
    "kernel_initializer='normal', activation='relu', kernel_regularizer = \n",
    "regularizers.l1(1e-6)))\n",
    "\n",
    "model_lasso.add(Dense(32, activation='relu'))\n",
    "model_lasso.add(Dense(32, activation='relu'))\n",
    "model_lasso.add(Dense(1, activation='sigmoid'))\n",
    "model_lasso.compile(optimizer = 'adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "model_lasso.fit(X_train, y_train,epochs =20)\n",
    "\n",
    "y_pred = model_lasso.predict(X_test_hold_out)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(\"Accuracy With Lasso: \", accuracy_score(y_pred, y_test_hold_out))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (L2)\n",
    "\n",
    "We simply need to call a method name l2 in the layers of our neural network. The difference between lasso and ridge is that the former tends to discard insignificant values altogether, whereas ridge simply decreases the magnitude of the weights in our neural network across all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Sequential()\n",
    "\n",
    "model_ridge.add(Dense(len(cols),input_shape=(len(cols),), \n",
    "kernel_initializer='normal', activation='relu', kernel_regularizer = \n",
    "regularizers.l2(1e-6)))\n",
    "\n",
    "model_ridge.add(Dense(32, activation='relu'))\n",
    "model_ridge.add(Dense(32, activation='relu'))\n",
    "model_ridge.add(Dense(1, activation='sigmoid'))\n",
    "model_ridge.compile(optimizer = 'adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
    "model_ridge.fit(X_train, y_train,epochs =20)\n",
    "\n",
    "y_pred = model_lasso.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(\"Accuracy With Ridge: \", accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ridge, the accuracy is slightly better than the first neural network we built as well as the neural network with lasso. Choosing the best regularization method to use depends on the use case. If using all of the input features in your model is important, ridge regression may be a better choice for regularization. This may be the case where certain features need to be kept for training our model.\n",
    "\n",
    "For example, a weak feature may still be useful as a lever to a company.They may want to see how model predictions change as the value of the weak feature changes even if it does not strongly contribute to performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
