{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR (exclusive or) function provides the target function $y = f^*(x)$ that we want to learn. Our model provides a function \n",
    "$y = f(x; \\theta)$ and our learning algorithm will adapt the parameters $\\theta$ to make f as similar as possible to $f^*$. We\n",
    "want our network to perform correclty on the four points $X = \\{ [0,0]^T, [0,1]^T, [1,0]^T, [1,1]^T\\}$.\n",
    "\n",
    "We can treat this problem as a regressino problme and use a mean squared error loss function. Note in practice, MSE is usually not an appropriate cost function for modeling binary data.\n",
    "\n",
    "Evaluated on our whole training set, the MSE loss function is \n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4}\\sum_{x\\in X} (f^*(x) - f(x; \\theta))^2\n",
    "$$\n",
    "\n",
    "Now we must choose the form of our model, $f(x; \\theta)$.  If we choose a linear model, with $\\theta$ consisting of $w$ and b. Out model is defined to be\n",
    "\n",
    "$$\n",
    "f(x; w, b) = x^Tw + b\n",
    "$$\n",
    "\n",
    "We can minimize $J(\\theta)$ in closed form with respect to w and b using the normal equations.\n",
    "\n",
    "After solving the normal equations, we obtain $w = 0$ and $b = \\frac{1}{2}$. The linear model simply outputs 0.5 everywhere.\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "Solving the XOR problem by learning a representation. The bold numbers printed on the plot indicate the value that the learned function must output at each\n",
    "point. (left) A linear model applied directly to the original input cannot implement the XOR function. When $x_1 = 0$, th emodel's output must increase as $x_2$ increases.\n",
    "When $x_1 = 1$ the model's output must decrease as $x_2$ increases. A linear model must apply a fixed coefficient $w_2$ to $x_2$. The linear model therefore cannot us the value\n",
    "$x_1$ to change the coefficient on $x_2$ and cannot solve the problem. (right) In the transformed space represented by the feature extracted by a neural network, a linear model can now solve\n",
    "the problem.\n",
    "\n",
    "The image above shows hows a linear model is not able to represent the XOR function. One way to solve this problems to use a model that learns a different\n",
    "feature space in which a linear model is able to represent the solution.\n",
    "\n",
    "Specifically, we will introduce a simple feedforward network with one hidden layer containing two hidden units, see image below. This feedforward network has a vector of hidden unit $h$\n",
    "that are computed by a function $f^{(1)}(x;W,c)$. The values of these hidden units are then used as the input for a second layer. Now the ouput layer is still a linear regression model, but now applied\n",
    "to $h$ rather than x. Thus $h = f^{(1)}(x;W,c)$ and $y = f^{(2)}(x;w,b)$ with $f(x; W, c, w, b) = f^{(2)}f^{(1)}(x))$.\n",
    "\n",
    "![title](img/picture2.png)\n",
    "\n",
    "An example of a feedforward network, drawn in two different styles. Specifically, this is the feedforward network we use to solve the XOR example. It has a single hidden layer\n",
    "containing two units. (left) In this style, we draw every unit as a node in the graph. this style is explicit and unambiguous, but for networks larger than this example,\n",
    "it can consume too much space. (right) In this style, we draw a node in the graph for each entire vector representing a layer's activations.\n",
    "\n",
    "Most neural networks do so using affine transformation controlled by learned parameters, followed by a fixed nonlinear function called an activation function. Hence\n",
    "$h = g(W^Tx +c)$ where $W$ provides the weights of a linear transformation and $c$ the biases. The activation function g is typically chosen to be a function that is applied element-wise\n",
    "with $h_i = g(x^TW:, i + c_i)$. In modern neural networks, the default recommendation is to use the **rectified linear unit** or ReLU, defined by the activation function\n",
    "$g(z) max\\{0,z\\}$\n",
    "\n",
    "![title](img/picture3.png)\n",
    "\n",
    "The rectified linear activation function. This acitvation function is the default activation function recommended for use with most feedfoward neural networks.\n",
    "\n",
    "Now we can specity our complete network as \n",
    "$$\n",
    "f(x; W, c, w, b) = w^T\\max\\{ 0, W^Tx + c \\} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to the XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then specify a solution to the XOR problem. Let\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "c = \n",
    "\\begin{bmatrix}\n",
    "0  \\\\\n",
    "-1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "c = \n",
    "\\begin{bmatrix}\n",
    "1  \\\\\n",
    "-2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $b=0$\n",
    "Now let \n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first step in the neural network is to multiply the input matrix by first layer's weight matrix:\n",
    "\n",
    "$$\n",
    "XW = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\ \n",
    "1 & 1 \\\\\n",
    "2 & 2 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, we add the bias vector c, to obtain\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\ \n",
    "1 & 0 \\\\\n",
    "2 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As shown in the first image above, they now lie in a space where a linear model can solve the problem.\n",
    "We finish with multiplying by the weight vector $w$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\ \n",
    "1 \\\\\n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The neural network has obtained the correct answer for every example in the batch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
