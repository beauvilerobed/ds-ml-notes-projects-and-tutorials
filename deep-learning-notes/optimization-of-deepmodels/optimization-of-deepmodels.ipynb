{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Training Deep Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning algorithms involve optimization in many contexts. For example,performing inference in models such as PCA involves solving an optimizationproblem. We often use analytical optimization to write proofs or design algorithms.Of all the many optimization problems involved in deep learning, the most diﬃcultis neural network training.\n",
    "\n",
    "It is quite common to invest days to months of time onhundreds of machines to solve even a single instance of the neural network trainingproblem. Because this problem is so important and so expensive, a specializedset of optimization techniques have been developed for solving it. This chapterpresents these optimization techniques for neural network training.\n",
    "\n",
    "We will focus on one particular case of optimization: ﬁnding the parameters $\\theta$ of a neural network that signiﬁcantly reduce a cost function $J(\\theta)$, which typically includes a performance measure evaluated on the entire training set as well as additional regularization terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Learning Differs from Pure Optimization?\n",
    "\n",
    "In most machine learning scenarios, we care about some performance measure $P$, that is deﬁned with respect to the test set. We therefore optimize $P$ only indirectly. We reduce a diﬀerent cost function $J(\\theta)$ in the hope that doing so will improve $P$. This is in contrast to pure optimization,where minimizing $J$ is a goal in and of itself. Optimization algorithms for training deep models also typically include some specialization on the speciﬁc structure of machine learning objective functions. This function can be written as an average over the training set:\n",
    "\n",
    "$$\n",
    "J(\\theta) = E_{(x,y)\\sim \\hat{P}_{data}} L(f(x;\\theta), y)\n",
    "$$\n",
    "\n",
    "where $L$ is the per-example loss function,$f(x;θ)$ is the predicted output when the input is $x$, and $\\hat{P}_{data}$ the empirical distribution. In the supervised learning case, $y$ is the target output. \n",
    "\n",
    "We would usually prefer to minimize the corresponding objective function where the expectation is taken across the data-generating distribution $P_{data}$ rather than just over the ﬁnite training set:\n",
    "\n",
    "$$\n",
    "J^*(\\theta) = E_{(x,y)\\sim P_{data}} L(f(x;\\theta), y)\n",
    "$$\n",
    "\n",
    "### Empirical Risk Minimization\n",
    "\n",
    "The goal of a machine learning algorithm is to reduce the expected generalization error. This quantity is known as the **risk**. We emphasize here that the expectation is taken over the true underlying distribution $P_{data}$. If we knew the true distribution $P_{data}(x, y)$, risk minimization would be an optimization task solvable by an optimization algorithm. When we do not know $P_{data}(x, y)$ but only have a training set of samples, however, we have a machine learning problem.\n",
    "\n",
    "The simplest way to convert a machine learning problem back into an optimization problem is to minimize the expected loss on the training set. This means replacing the true distribution $p(x, y)$ with the empirical distribution $\\hat{P}_{data}(x, y)$ deﬁned by the training set. We now minimize the **empirical risk**\n",
    "\n",
    "$$\n",
    "E_{(x,y)\\sim \\hat{P}_{data}} L(f(x;\\theta), y) = \\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})  \n",
    "$$\n",
    "\n",
    "where $m$ is the number of training examples. The training process based on minimizing this average training error is known as **empirical risk minimization**. Nonetheless, empirical risk minimization is prone to overﬁtting. Models withhigh capacity can simply memorize the training set. In many cases, empiricalrisk minimization is not really feasible. The most eﬀective modern optimizationalgorithms are based on gradient descent, but many useful loss functions, suchas 0-1 loss, have no useful derivatives (the derivative is either zero or undeﬁnedeverywhere). These two problems mean that, in the context of deep learning, werarely use empirical risk minimization. Instead, we must use a slightly diﬀerentapproach, in which the quantity that we actually optimize is even more diﬀerentfrom the quantity that we truly want to optimize\n",
    "\n",
    "### Surrogate Loss Functions and Early Stopping\n",
    "\n",
    "Sometimes, the loss function we actually care about (say, classiﬁcation error) is notone that can be optimized eﬃciently. For example, exactly minimizing expected 0-1loss is typically intractable (exponential in the input dimension). In such situations, one typically optimizes a surrogate loss function instead. \n",
    "\n",
    "### Batch and Minibatch Algorithms\n",
    "\n",
    "One aspect of machine learning algorithms that separates them from generaloptimization algorithms is that the objective function usually decomposes as a sumover the training examples. Optimization algorithms for machine learning typically compute each update to the parameters based on an expected value of the costfunction estimated using only a subset of the terms of the full cost function. For example maximum likelihood estimation problems when viewed in logspace, decompose into a sum over each example:\n",
    "\n",
    "$$\n",
    "\\theta_{ML} \\argmax_{\\theta} \\sum_{i=1}^m log P_{model}(x^{(i)}, y^{(i)}; \\theta) \n",
    "$$\n",
    "\n",
    "Maximizing this sum is equivalent to maximizing the expectation over theempirical distribution deﬁned by the training set:\n",
    "\n",
    "$$\n",
    "J(\\theta) = E_{x,y\\sim \\hat{P}_{data}} log P_{model}(x,y;\\theta)\n",
    "$$\n",
    "\n",
    "Most of the properties of the objective function $J$ used by most of our optimization algorithms are also expectations over the training set. For example, the most commonly used property is the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) = E_{x,y\\sim \\hat{P}_{data}} \\nabla_\\theta log P_{model}(x,y;\\theta)\n",
    "$$\n",
    "\n",
    "Computing this expectation exactly is very expensive because it requiresevaluating the model on every example in the entire dataset. In practice, we cancompute these expectations by randomly sampling a small number of examplesfrom the dataset, then taking the average over only those examples.\n",
    "\n",
    "Challenges in Neural Network Optimization include\n",
    "- **Ill-Conditioning**: Ill-conditioning can manifest by causing SGD to get“stuck” in the sense that even very small steps increase the cost function.\n",
    "\n",
    "- **Local Minima**: With nonconvex functions, such as neural nets, it is possible to have manylocal minima. Indeed, nearly any deep model is essentially guaranteed to havean extremely large number of local minima.\n",
    "\n",
    "- **Plateaus, Saddle Points and Other Flat Regions**: In low-dimensional spaces, local minima are common. In higher-dimensional spaces, localminima are rare, and saddle points are more common.\n",
    "\n",
    "- **Cliﬀs and Exploding Gradients**: On the face of an extremely steep cliﬀ structure, thegradient update step can move the parameters extremely far, usually jumping oﬀthe cliﬀ structure altogether.\n",
    "\n",
    "- **Long-Term Dependencies**: Another diﬃculty that neural network optimization algorithms must overcomearises when the computational graph becomes extremely deep. Feedforwardnetworks with many layers have such deep computational graphs.\n",
    "\n",
    "- **Inexact Gradients**: Most optimization algorithms are designed with the assumption that we haveaccess to the exact gradient or Hessian matrix. In practice, we usually have onlya noisy or even biased estimate of these quantities. Nearly every deep learningalgorithm relies on sampling-based estimates, at least insofar as using a minibatchof training examples to compute the gradient.\n",
    "\n",
    "- **Poor Correspondence between Local and Global Structure**: Many of the problems we have discussed so far correspond to properties of the loss function at a single point—it can be diﬃcult to make a single step if $J(\\theta)$ is poorly conditioned at the current point $\\theta$, or if $\\theta$ lies on a cliff, or if $\\theta$ is a saddlepoint hiding the opportunity to make progress downhill from the gradient.\n",
    "\n",
    "- **Theoretical Limits of Optimization**: Some theoretical results showthat there exist problem classes that are intractable, but it can be diﬃcult to tellwhether a particular problem falls into that class. Other results show that ﬁndinga solution for a network of a given size is intractable, but in practice we can ﬁnd asolution easily by using a larger network for which many more parameter settingscorrespond to an acceptable solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) and its variants are probably the most usedoptimization algorithms for machine learning in general and for deep learningin particular. It is possible to obtain an unbiasedestimate of the gradient by taking the average gradient on a minibatch ofmexamples drawn i.i.d from the data-generating distribution\n",
    "\n",
    "A crucial parameter for the SGD algorithm is the learning rate. Previously, wehave described SGD as using a ﬁxed learning rate $\\epsilon$. In practice, it is necessary togradually decrease the learning rate over time, so we now denote the learning rateat iteration $k$ as $\\epsilon_k$. This is because the SGD gradient estimator introduces a source of noise (the random sampling of $m$ training examples) that does not vanish even when we arriveat a minimum. By comparison, the true gradient of the total cost function becomessmall and then0when we approach and reach a minimum using batch gradient descent, so batch gradient descent can use a ﬁxed learning rate. A suﬃcient condition to guarantee convergence of SGD is that\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^\\infty \\epsilon_k = \\infty\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^\\infty \\epsilon_k^2 < \\infty\n",
    "$$\n",
    "\n",
    "In practice, it is common to decay the learning rate linearly until iteration $\\tau$:\n",
    "\n",
    "$$\n",
    "\\epsilon_k = (1-\\alpha)\\epsilon_0 + \\alpha\\epsilon_{\\tau}\n",
    "$$\n",
    "\n",
    "with $\\alpha = \\frac{k}{\\tau}$. After iteration $\\tau$ , it is common to leave $\\epsilon$ constant. The learning rate may be chosen by trial and error, but it is usually best to choose it by monitoring learning curves that plot the objective function as a function of time. This is more of an art than a science, and most guidance on thissubject should be regarded with some skepticism.\n",
    "\n",
    "Algorithm: Stochastic gradient descent (SGD) update\n",
    "\n",
    "Require: Learning rate schedule $\\epsilon_1, \\epsilon_2,...$\n",
    "\n",
    "Require: Initial parameter $\\theta$\n",
    "\n",
    "- k $\\leftarrow$ 1\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i}$\n",
    "    - Compute gradiet estimate: $\\hat{g}\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\theta), y^{(i)})$\n",
    "    - Apply update: $\\theta \\leftarrow \\theta -\\epsilon_k\\hat{g}$\n",
    "    - $k\\leftarrow k+1$\n",
    "- end while\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "While stochastic gradient descent remains a popular optimization strategy, learning with it can sometimes be slow. The method of momentum is designed to accelerate learning, especially in the face of high curvature, small butconsistent gradients, or noisy gradients. The momentum algorithm accumulates an exponentially decaying moving average of past gradients and continues to move in their direction.\n",
    "\n",
    "<img src=\"img\\download.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "Formally, the momentum algorithm introduces a variable $v$ that plays the role of velocity—it is the direction and speed at which the parameters move through parameter space. The velocity is set to an exponentially decaying average of the negative gradient. A hyperparameter $\\alpha \\in [0,1)$ determines how quickly the contributions of previous gradients exponentially decay.The update rule is given by\n",
    "\n",
    "$$\n",
    "v \\leftarrow \\alpha v - \\epsilon\\nabla_\\theta \\left( \\frac{1}{m}\\sum_{i=1}^m L(f(x^{(i)};\\theta), y^{(i)})\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + v\n",
    "$$\n",
    "\n",
    "Algorithm: Stochastic gradient descent (SGD) with momentum\n",
    "\n",
    "Require: Learning rate $\\epsilon$, momentum parameter $\\alpha$\n",
    "\n",
    "Require: Initial parameter $\\theta$, initial velocity $v$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i}$\n",
    "    - Compute gradiet estimate: $\\hat{g}\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\theta), y^{(i)})$\n",
    "    - Compute velocity update: $v\\leftarrow \\alpha v - \\epsilon g$\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + v$\n",
    "- end while\n",
    "\n",
    "Previously, the size of the step was simply the norm of the gradient multipliedby the learning rate. Now, the size of the step depends on how large and howaligned a sequence of gradients are. The step size is largest when many successivegradients point in exactly the same direction.\n",
    "\n",
    "It is thus helpful to think of the momentum hyperparameter in terms of $\\frac{1}{1−\\alpha}$. For example, $\\alpha = 0.9$ corresponds to multiplying the maximum speed by 10 relative tothe gradient descent algorithm.\n",
    "\n",
    "Common values ofαused in practice include 0.5, 0.9, and 0.99. Like thelearning rate, $\\alpha$ may also be adapted over time. Typically it begins with a small value and is later raised. Adapting $\\alpha$ over time is less important than shrinking $\\epsilon$ over time.\n",
    "\n",
    "### Nesterov Momentum\n",
    "\n",
    "With Nesterov momentum, the gradient isevaluated after the current velocity is applied. Thus one can interpret Nesterovmomentum as attempting to add a correction factor to the standard methodof momentum.\n",
    "\n",
    "Algorithm: Stochastic gradient descent (SGD) with Nesterov's momentum\n",
    "\n",
    "Require: Learning rate $\\epsilon$, momentum parameter $\\alpha$\n",
    "\n",
    "Require: Initial parameter $\\theta$, initial velocity $v$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i}$\n",
    "    - Apply interim update:  $\\tilde{\\theta}\\leftarrow \\theta+\\alpha v$\n",
    "    - Compute gradiet estimate: $\\hat{g}\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\tilde{\\theta}), y^{(i)})$\n",
    "    - Compute velocity update: $v\\leftarrow \\alpha v - \\epsilon g$\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + v$\n",
    "- end while"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms with Adaptive Learning Rates\n",
    "\n",
    "Neural network researchers have long realized that the learning rate is reliablyone of the most diﬃcult to set hyperparameters because it signiﬁcantly affects model performance. The cost is often highly sensitive to some directions in parameter space and insensitive to others. Themomentum algorithm can mitigate these issues somewhat, but it does so at theexpense of introducing another hyperparameter.\n",
    "\n",
    "If we believe that the directions of sensitivity aresomewhat axis aligned, it can make sense to use a separate learning rate for eachparameter and automatically adapt these learning rates throughout the course oflearning.\n",
    "\n",
    "More recently, a number of incremental (or mini batch-based) methods havebeen introduced that adapt the learning rates of model parameters. In this section,we brieﬂy review a few of these algorithms\n",
    "\n",
    "### Algorithm: The AdaGrad algorithm\n",
    "\n",
    "Require: Global Learning rate $\\epsilon$\n",
    "\n",
    "Require: Initial parameter $\\theta$\n",
    "\n",
    "Require: Small constant $\\delta$, perhaps $10^{-7}$, for numerical stability\n",
    "- Initialize gradient accumulation variable $r=0$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i)}$\n",
    "    - Compute gradient: $g\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\theta), y^{(i)})$\n",
    "    - Accumulate squared gradient: $r \\leftarrow r + g \\odot g$.\n",
    "    - Compute update: $\\Delta\\theta \\leftarrow -\\frac{\\epsilon}{\\delta+\\sqrt\n",
    "    {r}}\\odot g$ (Division and square root applied element-wise)\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + \\Delta\\theta$\n",
    "- end while\n",
    "\n",
    "### Algorithm: The RMSProp algorithm\n",
    "\n",
    "Require: Global Learning rate $\\epsilon$, decay rate $\\rho$\n",
    "\n",
    "Require: Initial parameter $\\theta$\n",
    "\n",
    "Require: Small constant $\\delta$, perhaps $10^{-6}$, used to stabalize division by small numbers\n",
    "- Initialize gradient accumulation variable $r=0$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i)}$\n",
    "    - Compute gradient: $g\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\theta), y^{(i)})$\n",
    "    - Accumulate squared gradient: $r \\leftarrow \\rho r + (1-\\rho) g\\odot g$.\n",
    "    - Compute update: $\\Delta\\theta \\leftarrow -\\frac{\\epsilon}{\\delta+\\sqrt{r}}\\odot g$ (Division and square root applied element-wise)\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + \\Delta\\theta$\n",
    "- end while\n",
    "\n",
    "### Algorithm: The RMSProp algorithm with Nestrov momentum\n",
    "\n",
    "Require: Global Learning rate $\\epsilon$, decay rate $\\rho$\n",
    "\n",
    "Require: Initial parameter $\\theta$\n",
    "\n",
    "Require: Small constant $\\delta$, perhaps $10^{-6}$, used to stabalize division by small numbers\n",
    "- Initialize gradient accumulation variable $r=0$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i)}$\n",
    "    - Compute interim update: $\\tilde{\\theta} \\leftarrow \\theta + \\alpha v$\n",
    "    - Compute gradient: $g\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\tilde{\\theta} ), y^{(i)})$\n",
    "    - Accumulate gradient: $r \\leftarrow \\rho r + (1-\\rho) g\\odot g$.\n",
    "    - Compute update: $v \\leftarrow \\alpha v -\\frac{\\epsilon}{\\sqrt{r}}\\odot g$ (Division and square root applied element-wise)\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + v$\n",
    "- end while\n",
    "\n",
    "### Algorithm: The Adam algorithm\n",
    "\n",
    "Require: Step size $\\epsilon$ (Suggested default: 0.001)\n",
    "\n",
    "Require: Exponential decay rates for moment estimates, $\\rho_1$ and $\\rho_2$ in [0,1).(Suggested defaults: 0.9 and 0.999 respectively)\n",
    "\n",
    "Require: Small constant $\\delta$, used for numerical stabilization (Suggested default: $10^{−8}$)\n",
    "\n",
    "Require: Initial parameters $\\theta$\n",
    "- Initialize 1st and 2nd moment variables $s = 0$, $r = 0$\n",
    "- Initialize time step $t = 0$\n",
    "- **while** stopping criterion not met **do**\n",
    "    - Sample a minibatch of me examples from the training set $\\{x^{(1)},..., x^{(m)}\\}$ with corresponding targets $y^{(i)}$\n",
    "    - Compute gradient: $g\\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_i L(f(x^{(i)};\\theta ), y^{(i)})$\n",
    "    - $t\\leftarrow t+1$\n",
    "    - Update biased ﬁrst moment estimate: $s \\leftarrow \\rho_1s + (1 − \\rho_1)g$\n",
    "    - Update biased second moment estimate: $r \\leftarrow \\rho_2r + (1 − \\rho_2)g \\odot g$\n",
    "    - Correct bias in ﬁrst moment: $\\hat{s} \\leftarrow \\frac{s}{1−\\rho^t_1}$\n",
    "    - Correct bias in second moment: $\\hat{r} \\leftarrow \\frac{r}{1−\\rho^t_2}$\n",
    "    - Compute update: $\\Delta \\theta = −\\epsilon \\frac{\\hat{s}}{\\sqrt{\\hat{r}+\\delta}}$ (operations applied element-wise)\n",
    "    - Apply update: $\\theta \\leftarrow \\theta + \\Delta\\theta$\n",
    "- end while"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/121381obtV.gif\">\n",
    "<img src=\"img/56201contours_evaluation_optimizers.gif\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Optimizers with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "x_test =  x_test.reshape(x_test.shape[0],28,28,1)\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train) \n",
    "y_test = keras.utils.np_utils.to_categorical(y_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "input_shape = (28,28,1)\n",
    "\n",
    "def build_model(optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer= optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 34s 35ms/step - loss: 2.2782 - accuracy: 0.1459 - val_loss: 2.2228 - val_accuracy: 0.3174\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 40s 43ms/step - loss: 2.1887 - accuracy: 0.3049 - val_loss: 2.1213 - val_accuracy: 0.6197\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 39s 42ms/step - loss: 2.0875 - accuracy: 0.4569 - val_loss: 2.0019 - val_accuracy: 0.7057\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 1.9704 - accuracy: 0.5544 - val_loss: 1.8638 - val_accuracy: 0.7474\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 1.8359 - accuracy: 0.6152 - val_loss: 1.7099 - val_accuracy: 0.7736\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 1.6921 - accuracy: 0.6535 - val_loss: 1.5487 - val_accuracy: 0.7928\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 1.5433 - accuracy: 0.6831 - val_loss: 1.3900 - val_accuracy: 0.8092\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 1.4021 - accuracy: 0.6992 - val_loss: 1.2432 - val_accuracy: 0.8194\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 1.2757 - accuracy: 0.7164 - val_loss: 1.1137 - val_accuracy: 0.8275\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 1.1663 - accuracy: 0.7314 - val_loss: 1.0028 - val_accuracy: 0.8337\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 1.7035 - accuracy: 0.5212 - val_loss: 0.8946 - val_accuracy: 0.8293\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.8019 - accuracy: 0.7735 - val_loss: 0.5032 - val_accuracy: 0.8791\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.5975 - accuracy: 0.8243 - val_loss: 0.4045 - val_accuracy: 0.8958\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.5146 - accuracy: 0.8468 - val_loss: 0.3573 - val_accuracy: 0.9049\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.4658 - accuracy: 0.8624 - val_loss: 0.3273 - val_accuracy: 0.9099\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.4346 - accuracy: 0.8717 - val_loss: 0.3056 - val_accuracy: 0.9163\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.4090 - accuracy: 0.8789 - val_loss: 0.2895 - val_accuracy: 0.9204\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.3884 - accuracy: 0.8852 - val_loss: 0.2753 - val_accuracy: 0.9233\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 31s 33ms/step - loss: 0.3723 - accuracy: 0.8892 - val_loss: 0.2633 - val_accuracy: 0.9257\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 103s 110ms/step - loss: 0.3609 - accuracy: 0.8935 - val_loss: 0.2535 - val_accuracy: 0.9282\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.2290 - accuracy: 0.9317 - val_loss: 0.0656 - val_accuracy: 0.9788\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0906 - accuracy: 0.9724 - val_loss: 0.0495 - val_accuracy: 0.9827\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.0670 - accuracy: 0.9796 - val_loss: 0.0394 - val_accuracy: 0.9870\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.0537 - accuracy: 0.9828 - val_loss: 0.0418 - val_accuracy: 0.9865\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.0338 - val_accuracy: 0.9884\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 0.0353 - val_accuracy: 0.9886\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.0334 - accuracy: 0.9891 - val_loss: 0.0347 - val_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 35s 37ms/step - loss: 0.0318 - accuracy: 0.9897 - val_loss: 0.0323 - val_accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.0331 - val_accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 34s 37ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0330 - val_accuracy: 0.9886\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 40s 41ms/step - loss: 0.2360 - accuracy: 0.9282 - val_loss: 0.0721 - val_accuracy: 0.9777\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 38s 41ms/step - loss: 0.0928 - accuracy: 0.9722 - val_loss: 0.0573 - val_accuracy: 0.9810\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 38s 41ms/step - loss: 0.0729 - accuracy: 0.9790 - val_loss: 0.0541 - val_accuracy: 0.9828\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.0480 - val_accuracy: 0.9857\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 39s 41ms/step - loss: 0.0600 - accuracy: 0.9828 - val_loss: 0.0453 - val_accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0596 - accuracy: 0.9830 - val_loss: 0.0420 - val_accuracy: 0.9874\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 47s 50ms/step - loss: 0.0570 - accuracy: 0.9835 - val_loss: 0.0481 - val_accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 44s 47ms/step - loss: 0.0552 - accuracy: 0.9840 - val_loss: 0.0447 - val_accuracy: 0.9870\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0580 - accuracy: 0.9837 - val_loss: 0.0488 - val_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 41s 43ms/step - loss: 0.0562 - accuracy: 0.9841 - val_loss: 0.0469 - val_accuracy: 0.9860\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.8495 - accuracy: 0.7438 - val_loss: 0.3122 - val_accuracy: 0.9119\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 33s 36ms/step - loss: 0.3903 - accuracy: 0.8814 - val_loss: 0.2336 - val_accuracy: 0.9333\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.3161 - accuracy: 0.9040 - val_loss: 0.1938 - val_accuracy: 0.9443\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 33s 36ms/step - loss: 0.2748 - accuracy: 0.9183 - val_loss: 0.1687 - val_accuracy: 0.9515\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.2467 - accuracy: 0.9244 - val_loss: 0.1492 - val_accuracy: 0.9566\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 32s 34ms/step - loss: 0.2263 - accuracy: 0.9315 - val_loss: 0.1367 - val_accuracy: 0.9598\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 33s 35ms/step - loss: 0.2123 - accuracy: 0.9357 - val_loss: 0.1279 - val_accuracy: 0.9630\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.1982 - accuracy: 0.9405 - val_loss: 0.1189 - val_accuracy: 0.9644\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 34s 36ms/step - loss: 0.1896 - accuracy: 0.9430 - val_loss: 0.1144 - val_accuracy: 0.9663\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 35s 38ms/step - loss: 0.1796 - accuracy: 0.9463 - val_loss: 0.1078 - val_accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "optimizers = ['Adadelta', 'Adagrad', 'Adam', 'RMSprop', 'SGD']\n",
    "\n",
    "models_history = {}\n",
    "for i in optimizers:\n",
    "    model = build_model(i)\n",
    "    hist=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test,y_test))\n",
    "    models_history[i] = hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run our model with a batch size of 64 for 10 epochs. After trying the different optimizers, the results we get are pretty interesting. Before analyzing the results, what do you think will be the best optimizer for this dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "- Epoch 1/10 34s 35ms/step - loss: 2.2782 - accuracy: 0.1459 - val_loss: 2.2228 - val_accuracy: 0.3174\n",
    "- Epoch 2/10 40s 43ms/step - loss: 2.1887 - accuracy: 0.3049 - val_loss: 2.1213 - val_accuracy: 0.6197\n",
    "- Epoch 3/10 39s 42ms/step - loss: 2.0875 - accuracy: 0.4569 - val_loss: 2.0019 - val_accuracy: 0.7057\n",
    "- Epoch 4/10 34s 36ms/step - loss: 1.9704 - accuracy: 0.5544 - val_loss: 1.8638 - val_accuracy: 0.7474\n",
    "- Epoch 5/10 33s 35ms/step - loss: 1.8359 - accuracy: 0.6152 - val_loss: 1.7099 - val_accuracy: 0.7736\n",
    "- Epoch 6/10 34s 36ms/step - loss: 1.6921 - accuracy: 0.6535 - val_loss: 1.5487 - val_accuracy: 0.7928\n",
    "- Epoch 7/10 34s 36ms/step - loss: 1.5433 - accuracy: 0.6831 - val_loss: 1.3900 - val_accuracy: 0.8092\n",
    "- Epoch 8/10 33s 35ms/step - loss: 1.4021 - accuracy: 0.6992 - val_loss: 1.2432 - val_accuracy: 0.8194\n",
    "- Epoch 9/10 34s 36ms/step - loss: 1.2757 - accuracy: 0.7164 - val_loss: 1.1137 - val_accuracy: 0.8275\n",
    "- Epoch 10/10 35s 37ms/step - loss: 1.1663 - accuracy: 0.7314 - val_loss: 1.0028 - val_accuracy: 0.8337\n",
    "\n",
    "## Adagrad\n",
    "\n",
    "- Epoch 1/10 33s 35ms/step - loss: 1.7035 - accuracy: 0.5212 - val_loss: 0.8946 - val_accuracy: 0.8293\n",
    "- Epoch 2/10 33s 35ms/step - loss: 0.8019 - accuracy: 0.7735 - val_loss: 0.5032 - val_accuracy: 0.8791\n",
    "- Epoch 3/10 33s 35ms/step - loss: 0.5975 - accuracy: 0.8243 - val_loss: 0.4045 - val_accuracy: 0.8958\n",
    "- Epoch 4/10 32s 34ms/step - loss: 0.5146 - accuracy: 0.8468 - val_loss: 0.3573 - val_accuracy: 0.9049\n",
    "- Epoch 5/10 33s 35ms/step - loss: 0.4658 - accuracy: 0.8624 - val_loss: 0.3273 - val_accuracy: 0.9099\n",
    "- Epoch 6/10 32s 34ms/step - loss: 0.4346 - accuracy: 0.8717 - val_loss: 0.3056 - val_accuracy: 0.9163\n",
    "- Epoch 7/10 34s 36ms/step - loss: 0.4090 - accuracy: 0.8789 - val_loss: 0.2895 - val_accuracy: 0.9204\n",
    "- Epoch 8/10 35s 37ms/step - loss: 0.3884 - accuracy: 0.8852 - val_loss: 0.2753 - val_accuracy: 0.9233\n",
    "- Epoch 9/10 31s 33ms/step - loss: 0.3723 - accuracy: 0.8892 - val_loss: 0.2633 - val_accuracy: 0.9257\n",
    "- Epoch 10/10 103s 110ms/step - loss: 0.3609 - accuracy: 0.8935 - val_loss: 0.2535 - val_accuracy: 0.9282\n",
    "\n",
    "## Adam\n",
    "\n",
    "- Epoch 1/10 35s 37ms/step - loss: 0.2290 - accuracy: 0.9317 - val_loss: 0.0656 - val_accuracy: 0.9788\n",
    "- Epoch 2/10 36s 38ms/step - loss: 0.0906 - accuracy: 0.9724 - val_loss: 0.0495 - val_accuracy: 0.9827\n",
    "- Epoch 3/10 35s 37ms/step - loss: 0.0670 - accuracy: 0.9796 - val_loss: 0.0394 - val_accuracy: 0.9870\n",
    "- Epoch 4/10 35s 37ms/step - loss: 0.0537 - accuracy: 0.9828 - val_loss: 0.0418 - val_accuracy: 0.9865\n",
    "- Epoch 5/10 36s 38ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.0338 - val_accuracy: 0.9884\n",
    "- Epoch 6/10 33s 35ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 0.0353 - val_accuracy: 0.9886\n",
    "- Epoch 7/10 34s 36ms/step - loss: 0.0334 - accuracy: 0.9891 - val_loss: 0.0347 - val_accuracy: 0.9892\n",
    "- Epoch 8/10 35s 37ms/step - loss: 0.0318 - accuracy: 0.9897 - val_loss: 0.0323 - val_accuracy: 0.9897\n",
    "- Epoch 9/10 34s 36ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.0331 - val_accuracy: 0.9895\n",
    "- Epoch 10/10 34s 37ms/step - loss: 0.0258 - accuracy: 0.9918 - val_loss: 0.0330 - val_accuracy: 0.9886\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "- Epoch 1/10 40s 41ms/step - loss: 0.2360 - accuracy: 0.9282 - val_loss: 0.0721 - val_accuracy: 0.9777\n",
    "- Epoch 2/10 38s 41ms/step - loss: 0.0928 - accuracy: 0.9722 - val_loss: 0.0573 - val_accuracy: 0.9810\n",
    "- Epoch 3/10 38s 41ms/step - loss: 0.0729 - accuracy: 0.9790 - val_loss: 0.0541 - val_accuracy: 0.9828\n",
    "- Epoch 4/10 39s 41ms/step - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.0480 - val_accuracy: 0.9857\n",
    "- Epoch 5/10 39s 41ms/step - loss: 0.0600 - accuracy: 0.9828 - val_loss: 0.0453 - val_accuracy: 0.9862\n",
    "- Epoch 6/10 68s 72ms/step - loss: 0.0596 - accuracy: 0.9830 - val_loss: 0.0420 - val_accuracy: 0.9874\n",
    "- Epoch 7/10 47s 50ms/step - loss: 0.0570 - accuracy: 0.9835 - val_loss: 0.0481 - val_accuracy: 0.9856\n",
    "- Epoch 8/10 44s 47ms/step - loss: 0.0552 - accuracy: 0.9840 - val_loss: 0.0447 - val_accuracy: 0.9870\n",
    "- Epoch 9/10 41s 43ms/step - loss: 0.0580 - accuracy: 0.9837 - val_loss: 0.0488 - val_accuracy: 0.9869\n",
    "- Epoch 10/10 41s 43ms/step - loss: 0.0562 - accuracy: 0.9841 - val_loss: 0.0469 - val_accuracy: 0.9860\n",
    "\n",
    "## SGD\n",
    "\n",
    "- Epoch 1/10 34s 36ms/step - loss: 0.8495 - accuracy: 0.7438 - val_loss: 0.3122 - val_accuracy: 0.9119\n",
    "- Epoch 2/10 33s 36ms/step - loss: 0.3903 - accuracy: 0.8814 - val_loss: 0.2336 - val_accuracy: 0.9333\n",
    "- Epoch 3/10 34s 36ms/step - loss: 0.3161 - accuracy: 0.9040 - val_loss: 0.1938 - val_accuracy: 0.9443\n",
    "- Epoch 4/10 33s 36ms/step - loss: 0.2748 - accuracy: 0.9183 - val_loss: 0.1687 - val_accuracy: 0.9515\n",
    "- Epoch 5/10 33s 35ms/step - loss: 0.2467 - accuracy: 0.9244 - val_loss: 0.1492 - val_accuracy: 0.9566\n",
    "- Epoch 6/10 32s 34ms/step - loss: 0.2263 - accuracy: 0.9315 - val_loss: 0.1367 - val_accuracy: 0.9598\n",
    "- Epoch 7/10 33s 35ms/step - loss: 0.2123 - accuracy: 0.9357 - val_loss: 0.1279 - val_accuracy: 0.9630\n",
    "- Epoch 8/10 34s 36ms/step - loss: 0.1982 - accuracy: 0.9405 - val_loss: 0.1189 - val_accuracy: 0.9644\n",
    "- Epoch 9/10 34s 36ms/step - loss: 0.1896 - accuracy: 0.9430 - val_loss: 0.1144 - val_accuracy: 0.9663\n",
    "- Epoch 10/10 35s 38ms/step - loss: 0.1796 - accuracy: 0.9463 - val_loss: 0.1078 - val_accuracy: 0.9681\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Analysis\n",
    "The above table shows the validation accuracy and loss at different epochs. It also contains the total time that the model took to run on 10 epochs for each optimizer. From the above table, we can make the following analysis.\n",
    "\n",
    "- The adam optimizer shows the best accuracy in a satisfactory amount of time.\n",
    "- RMSprop shows similar accuracy to that of Adam but with a comparatively much larger computation time.\n",
    "- Surprisingly, the SGD algorithm took the least time to train and produced good results as well. But to reach the accuracy of the Adam optimizer, SGD will require more iterations, and hence the computation time will increase.\n",
    "- SGD with momentum shows similar accuracy to SGD with unexpectedly larger computation time. This means the value of momentum taken needs to be optimized.\n",
    "- Adadelta shows poor results both with accuracy and relative computation time (before final epoch).\n",
    "\n",
    "You can analyze the accuracy of each optimizer with each epoch from the below graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_axis = [num+1 for num in range(10)]\n",
    "for optimizer in models_history:\n",
    "    optimizer_model = models_history[optimizer]\n",
    "    y_axis = optimizer_model.history['accuracy']\n",
    "    plt.plot(x_axis, y_axis, label=optimizer)\n",
    "plt.title('accuracy for ' +optimizer)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "SGD is a very basic algorithm and is hardly used in applications now due to its slow computation speed. One more problem with that algorithm is the constant learning rate for every epoch. Moreover, it is not able to handle saddle points very well. Adagrad works better than stochastic gradient descent generally due to frequent updates in the learning rate. It is best when used for dealing with sparse data. RMSProp shows similar results to that of the gradient descent algorithm with momentum, it just differs in the way by which the gradients are calculated.\n",
    "\n",
    "Lastly comes the Adam optimizer that inherits the good features of RMSProp and other algorithms. The results of the Adam optimizer are generally better than every other optimization algorithm, have faster computation time, and require fewer parameters for tuning. Because of all that, Adam is recommended as the default optimizer for most of the applications. Choosing the Adam optimizer for your application might give you the best probability of getting the best results.\n",
    "\n",
    "But by the end, we learned that even Adam optimizer has some downsides. Also, there are cases when algorithms like SGD might be beneficial and perform better than Adam optimizer. So, it is of utmost importance to know your requirements and the type of data you are dealing with to choose the best optimization algorithm and achieve outstanding results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This article taught us how an optimization algorithm could affect the deep learning model in terms of accuracy, speed, and efficiency. We learned about various algorithms, and hopefully, you were able to compare the algorithms with one another. We also learned when to use which algorithm and what could be the downsides of using that algorithm.\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "- Gradient Descent, Stochastic Gradient Descent, Mini-batch Gradient Descent, Adagrad, RMS Prop, AdaDelta, and Adam are all popular deep-learning optimizers.\n",
    "- Each optimizer has its own strengths and weaknesses, and the choice of optimizer will depend on the specific deep-learning task and the characteristics of the data being used.\n",
    "- The choice of optimizer can significantly impact the speed and quality of convergence during training, as well as the final performance of the deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
