{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing and training a neural network is not much different from training any other nmachine learning model with gradient descent.\n",
    "The Largest difference between the linear models and neural networks is that the nonlinearity of a neural network causes interesting loss\n",
    "functions to become nonconvex. So neural networks are usually trained by using iterative, gradient-based optimizers that merely drive the cost\n",
    "function to a very low value, rather than the linear equation solvers used to train linear regression models or the convex optimization algorithms\n",
    "with global convergence starting from any initial parameters.\n",
    "\n",
    "For the moment, it suffices to understand that the training algorithm is almost always based on using the gradient to descend the cost\n",
    "function in one way or another. The specific algorithms are improvements and refinements on the idea of gradient descent.\n",
    "\n",
    "As with other machine learning models, to apply gradient-based learning we must choose a cost function, and we choose how to represent the output of\n",
    "the model. We now revisit these design considerations with special emphasis on the neural networks scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect of the design of a deep neural network is the choice of the cost function.\n",
    "\n",
    "In most cases, our parametric model defines a distribution $p(y|x;\\theta)$ and we simply use the principle\n",
    "of maximum likelihood. This means we use the cross-entropy between the training data $x,y \\sim \\hat{p}_{data}$ and \n",
    "the model's predictions $p_{model} (y|x)$ as the cost function.\n",
    "\n",
    "This cost function is given by\n",
    "$$\n",
    " J(\\theta) = - \\mathbb{E}_{x,y \\sim \\hat{p}_{data}} (\\log p_{model} (y|x))\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}_{x,y\\sim\\hat{p}_{data}}$ is the expected value operator with respect to the empirical distribution $hat{p}_{data}$.\n",
    "Note that if $p_{model} (y|x) = \\mathcal{N}(y; f(x;\\theta), I)$ then we can recover the mean squared error cost\n",
    "$$\n",
    " J(\\theta) = \\frac{1}{2}\\mathbb{E}_{x,y \\sim \\hat{p}_{data}} \\Vert y - f(x;\\theta) \\Vert^2 + C\n",
    "$$\n",
    "\n",
    "where $f(x;\\theta)$ is a linear model, up to a scaling factor $\\frac{1}{2}$ and a term that does not depend on $\\theta$. The discarded \n",
    "constant is based on the variance of the Gaussian distribution.\n",
    "\n",
    "Note that the equivalence between the maximum likelihood estimation with an output distribution and minimization of mean squared error holds\n",
    "for a linear model, but in fact, the equivalence holds regardless of the $f(x;\\theta)$ use to predict the mean of the Gaussian.\n",
    "\n",
    "The advantage of this approach is specifying a model $p(y|x)$ automatically determines a cost function $\\log p(y|x)$. One unusual property of the\n",
    "cross-entropy cost used to perform maximum likelihood estimation is that it usualy does not have a minimum value when applied to the models\n",
    "commonly used in practice. For discrete output variables, most models are parametrized in such a way that they cannot represent a probability of zero\n",
    "or one, but can come arbitrarily close to doing so. Logisic regression is an example of such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of learning a full probability distribution $p(y|x;\\theta)$, we often want to learn just one conditional statistic\n",
    "of y given x.\n",
    "\n",
    "For example, we may have a predictor $f(x;\\theta)$ that we wish to employ to predict the mean of y. If we use a sufficiently \n",
    "powerful neural netowrk, we can think of the neural network as being able to represent any function $f$ from a wide class of functions,\n",
    "with this class being limited only by features such as continuity and boundedness rather than by having a specific parametric\n",
    "form. We can view the cost functionas as being a **functional** rather than just a function. A functional is a mapping from \n",
    "frunctions to real numbers. We can design out cost functional to have its minimum occur at some specific function we desire.\n",
    "\n",
    "For example, we can design the cost functional to have its minimum lie on the function that maps x to the expected value of y\n",
    "given x. Solving an optimization problem with respect to a function requires a mathmatical tool called **calculus of variations**.\n",
    "\n",
    "Our first result derived using calculus of variations is that solving the optimization problem\n",
    "$$\n",
    "f^* = \\argmin_{f} \\mathbb{E}_{x,y \\sim p_{data}} \\Vert y - f(x) \\Vert^2\n",
    "$$\n",
    "\n",
    "yields\n",
    "$$\n",
    "f^*(x) = \\mathbb{E}_{y \\sim p_{data}(y|x)} [y]\n",
    "$$\n",
    "\n",
    "So long as this function lies within the class we optimize over. I.e. if we could train this function on infinitely many samples from the\n",
    "true data generating distribution, minimizing the mean squared error cost function would give a function that predicts the mean of y for \n",
    "each value of x.\n",
    "\n",
    "A second result derived using calculus of variations is that\n",
    "$$\n",
    "f^* = \\argmin_{f} \\mathbb{E}_{x,y \\sim p_{data}} \\Vert y - f(x) \\Vert_1\n",
    "$$\n",
    "\n",
    "yields a function that predicts the median value of y for each x, as long as such a function may be described by the family of functions we\n",
    "optimize over. This cost function is commonly called **mean absolute error**.\n",
    "\n",
    "Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. This is one\n",
    "reason that the cross-entropy cost function is more popular that mean squared error or mean absolute error, even when it is not necessary to\n",
    "estimate an entire distribution $p(y|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of cost function is tightly coupled with the choice of output unit. The choice of how to represent the output then\n",
    "determines the form of the cross-entropy function.\n",
    "\n",
    "We will suppse that the feedforward networkk provides a set of hidden features defined by $h=f(x;\\theta)$. The role of the output\n",
    "layer is then to provide some additional transformation from the features to complete the task that the network must perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple kind of output unit is based on an affine transformation with no nonlinearity. These are often just\n",
    "called linear units.\n",
    "\n",
    "Given features $h$, a layer of linear output units produces a vector $\\hat{y} = W^Th + b$. Linear output layers are often \n",
    "used to produce the mean of a conditional Gaussian distribution:\n",
    "$$\n",
    "p(y|x) = \\mathcal{N}(y; \\hat{y}, I).\n",
    "$$\n",
    "\n",
    "Maximizing the log-likelihood is then equivalent to minimizing the mean squared error. \n",
    "\n",
    "Because linear units do not saturate, they pose little difficulty for gradient-based optimization algorithms and may be used\n",
    "with a wide variety of optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood approach is to define a Bernoulli distribution over y conditioned on x.\n",
    "\n",
    "A Bernoulli distribution is defined by just a single number. The neural net needs to predict only\n",
    "$ P(y=1|x)$. For this number to be a valid probability, it must lie in the interval $[0,1]$.\n",
    "\n",
    "Suppose we were to use a linear unit and threshold its value to obtain a valid probability:\n",
    "$$\n",
    "P(y=1|x) = \\max \\{ 0, \\min \\{ 1, w^Th+b \\} \\}\n",
    "$$\n",
    "\n",
    "This would indeed define a valid conditional distribution, but we would not be able to train it\n",
    "very effectively with gradient descent. Any time that $w^Th + b$ strayed outside the unit interval,\n",
    "the gradient of the output of the model with respect to its parameters would be 0. A gradient of 0\n",
    "is typically problematic because the learning algorithm no longer has a guide for how to improve the \n",
    "corresponding parameters.\n",
    "\n",
    "Its better to use a different approach that ensures there is always a strong gradient whenever the\n",
    "model has the wrong answer. This approach is based on using sigmoid output units combined with \n",
    "maximum likelihood.\n",
    "\n",
    "A sigmoid output unit is defined by\n",
    "$$\n",
    "\\hat{y} = \\sigma(w^Th+b)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the logistic sigmoid function. The sigmoid can be motivated by constructing an\n",
    "unnormalized probability distribution $\\hat{P}(y)$, which does not sum to 1. We can then divide by an \n",
    "appropriate constant to obtain a valid probability distribution. If we assume that the unnormalized \n",
    "log probabilities are linear in y and z, we can exponentiate to obtain the unnormalized probabilities:\n",
    "\n",
    "$$\n",
    "\\log \\tilde{P}(y) = yz\n",
    "$$\n",
    "$$\n",
    "\\tilde{P}(y) = \\exp(yz)\n",
    "$$\n",
    "$$\n",
    "P(y) = \\frac{\\exp(yz}{\\sum_{y'=1}^1 \\exp(y'z}\n",
    "$$\n",
    "$$\n",
    "P(y) = \\sigma((2y-1)z)\n",
    "$$\n",
    "\n",
    "Probability distributions based on exponentiation and normalization are common throughout the statistical\n",
    "modeling literature. The z variable defining such a distribution over binary variables is called a **logit**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach to predicting the probabilities in log space is natural to use with maximum likelihood learning.\n",
    "Because the cost function used with maximum likelihood is $-\\log P(y|x)$, the log in the cost function undoes the exp\n",
    "of the sigmoid. The loss function for maximum likelihood learning of a Bernoulli parametrized by a sigmoid is\n",
    "$$\n",
    "J(\\theta) = -\\log P(y|x)\n",
    "$$\n",
    "$$\n",
    "= -\\log \\sigma((2y-1)z)\n",
    "$$\n",
    "$$\n",
    "= \\zeta ((2y-1)z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time we wish to represent a probability distribution over a discrete variable with $n$ possible values,\n",
    "we may use the softmax function. This can be seen as a generalization of the sigmoid function, which was used\n",
    "to represent a probability distribution over a binary variable.\n",
    "\n",
    "Softmax functions are most often used as the output of a classifier, to represent the probability distribution over\n",
    "n different options for some internal variable.\n",
    "\n",
    "In the case of binary variables, we wished to produce a single number\n",
    "$$\n",
    "\\hat{y} = P(y=1|x)\n",
    "$$\n",
    "\n",
    "Because this number needed to lie between 0 and 1, and because we wanted the logarithm of the number to be well\n",
    "behaved for gradient-based optimization of the log-likelihood, we chose to instead predict a number \n",
    "$z = \\log \\tilde{P}(y=1|x)$. Exponentiating and normalizing gave us a Bernoulli distribution controlled by the \n",
    "sigmoid function.\n",
    "\n",
    "To generalize to the case of a discrete variable with n values, we now need to produce a vector $\\hat{y}$, with\n",
    "$\\hat{y_i} = P(y=i|x)$. We require that the entire vector sum is equal to 1 so that it represents a valid \n",
    "probability distribution. The same approach that worked for the Bernoulli distribution generalizes to the multinoulli\n",
    "distribution: \n",
    "$$\n",
    "z = W^Th+b\n",
    "$$\n",
    "were $z_i = \\log \\tilde{P}(y=i|x)$. The softmax function can then exponentiate and normalize z to obtain the desired\n",
    "$\\hat{y}$. Formally, the softmax function is given by\n",
    "$$\n",
    "\\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{y'=1}^1 \\exp(z_i)}\n",
    "$$\n",
    "As with the logistic sigmoid, the use of the exp function works well when training the softmax to output a target value\n",
    "y using maximum log-likelihood. In this case, we wish to maximize $\\log P(y=i; z) = \\log softmax(z)_i$. Defining the\n",
    "softmax in terms of exp in natural because the log in the log-likelihood can undo the exp of the softmax:\n",
    "$$\n",
    "\\log softmax(z)_i = z_i - \\log \\sum_{j} \\exp(z_i)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
