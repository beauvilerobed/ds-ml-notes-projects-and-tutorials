{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep feedforward networks** (or **feedforward neural networks**, or **multilayer perceptrons** (MLPS)) are the quintessential deep learning models.\n",
    "The goal of a feedforward network is to approximate some function $f^*$. \n",
    "\n",
    "Example: \n",
    "\n",
    "For a classifier $y = f^*(x)$ maps an input $x$ to a category $y$. A feedforward network defines a mapping $y=f(x;\\theta)$ and learns\n",
    "the value of the paremeters $\\theta$ that result in the best function approximation.\n",
    "\n",
    "These models are called **feedforward** because there are no **feedback** connections in which outputs of the model are fed back into itself.\n",
    "When feedforward neural networks are extended to include feedback connections, they are caled **recurrent neural networks**. In commercial applications\n",
    "the convolutional network used for object recognition from photos are a specialized kind of feedforward network. Feedforward neural networks are called\n",
    "**networks** since they are typically represented by composing together  many different functions, associated with a directed acyclic graph.\n",
    "\n",
    "Example:\n",
    "\n",
    "Given $f^{(1)}$ (**first layer**), $f^{(2)}$ (**second layer**), and $f^{(3)}$ (**third layer**) connected in a chain to form $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$ with **depth** of 3.\n",
    "\n",
    "\n",
    "These chain structures are the most commonly used structures of neural networks. The final layer of a feedforward network is called the **outer layer**. During neural network training, we drive\n",
    "$f(x)$ to match $f^*(x).\n",
    "\n",
    "Example:\n",
    "\n",
    "Each example $x$ is accompanied by a label $y \\approx f^*(x)$. The learning algorithm must decide how to use those layers to produce the desired output, but the training\n",
    "data do not say what each indiviudual layer should do. Instead, the learning algorithm must decide how to use these layers to best implement an approximation of $f^*$. Since\n",
    "the training data does not show the desired output for each of these layers, they are called **hidden layers**.\n",
    "\n",
    "Finally, these networks are called *neural* because they are loosely inspired by neuroscience. Each hidden layer of the network is typically vector valued. The dimensionality of \n",
    "these hidden layers determine the **width** of the model. \n",
    "\n",
    "First, training a feedforward network requires making many of the same design decisions as are necessary for a linear model: choosing the optimizer, the cost function, and the form of the output\n",
    "units. Feedfoward networks have introduced the concept of a hidden layer, and this requires us to choose the **activation functions** that will be used to compute the hidden layer values.\n",
    "We must also design the architecture of the network, like how many layers, how these layers should be connected, and how many **units** should be in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Learning XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR (exclusive or) function provides the target function $y = f^*(x)$ that we want to learn. Our model provides a function \n",
    "$y = f(x; \\theta)$ and our learning algorithm will adapt the parameters $\\theta$ to make f as similar as possible to $f^*$. We\n",
    "want our network to perform correclty on the four points $X = \\{ [0,0]^T, [0,1]^T, [1,0]^T, [1,1]^T\\}$.\n",
    "\n",
    "We can treat this problem as a regressino problme and use a mean squared error loss function. Note in practice, MSE is usually not an appropriate cost function for modeling binary data.\n",
    "\n",
    "Evaluated on our whole training set, the MSE loss function is \n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4}\\sum_{x\\in X} (f^*(x) - f(x; \\theta))^2\n",
    "$$\n",
    "\n",
    "Now we must choose the form of our model, $f(x; \\theta)$.  If we choose a linear model, with $\\theta$ consisting of $w$ and b. Out model is defined to be\n",
    "\n",
    "$$\n",
    "f(x; w, b) = x^Tw + b\n",
    "$$\n",
    "\n",
    "We can minimize $J(\\theta)$ in closed form with respect to w and b using the normal equations.\n",
    "\n",
    "After solving the normal equations, we obtain $w = 0$ and $b = \\frac{1}{2}$. The linear model simply outputs 0.5 everywhere.\n",
    "\n",
    "![title](img/picture.png)\n",
    "\n",
    "Solving the XOR problem by learning a representation. The bold numbers printed on the plot indicate the value that the learned function must output at each\n",
    "point. (left) A linear model applied directly to the original input cannot implement the XOR function. When $x_1 = 0$, th emodel's output must increase as $x_2$ increases.\n",
    "When $x_1 = 1$ the model's output must decrease as $x_2$ increases. A linear model must apply a fixed coefficient $w_2$ to $x_2$. The linear model therefore cannot us the value\n",
    "$x_1$ to change the coefficient on $x_2$ and cannot solve the problem. (right) In the transformed space represented by the feature extracted by a neural network, a linear model can now solve\n",
    "the problem.\n",
    "\n",
    "The image above shows hows a linear model is not able to represent the XOR function. One way to solve this problems to use a model that learns a different\n",
    "feature space in which a linear model is able to represent the solution.\n",
    "\n",
    "Specifically, we will introduce a simple feedforward network with one hidden layer containing two hidden units, see image below. This feedforward network has a vector of hidden unit $h$\n",
    "that are computed by a function $f^{(1)}(x;W,c)$. The values of these hidden units are then used as the input for a second layer. Now the ouput layer is still a linear regression model, but now applied\n",
    "to $h$ rather than x. Thus $h = f^{(1)}(x;W,c)$ and $y = f^{(2)}(x;w,b)$ with $f(x; W, c, w, b) = f^{(2)}f^{(1)}(x))$.\n",
    "\n",
    "![title](img/picture2.png)\n",
    "\n",
    "An example of a feedforward network, drawn in two different styles. Specifically, this is the feedforward network we use to solve the XOR example. It has a single hidden layer\n",
    "containing two units. (left) In this style, we draw every unit as a node in the graph. this style is explicit and unambiguous, but for networks larger than this example,\n",
    "it can consume too much space. (right) In this style, we draw a node in the graph for each entire vector representing a layer's activations.\n",
    "\n",
    "Most neural networks do so using affine transformation controlled by learned parameters, followed by a fixed nonlinear function called an activation function. Hence\n",
    "$h = g(W^Tx +c)$ where $W$ provides the weights of a linear transformation and $c$ the biases. The activation function g is typically chosen to be a function that is applied element-wise\n",
    "with $h_i = g(x^TW:, i + c_i)$. In modern neural networks, the default recommendation is to use the **rectified linear unit** or ReLU, defined by the activation function\n",
    "$g(z) max\\{0,z\\}$\n",
    "\n",
    "![title](img/picture3.png)\n",
    "\n",
    "The rectified linear activation function. This acitvation function is the default activation function recommended for use with most feedfoward neural networks.\n",
    "\n",
    "Now we can specity our complete network as \n",
    "$$\n",
    "f(x; W, c, w, b) = w^T\\max\\{ 0, W^Tx + c \\} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to the XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then specify a solution to the XOR problem. Let\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "c = \n",
    "\\begin{bmatrix}\n",
    "0  \\\\\n",
    "-1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "c = \n",
    "\\begin{bmatrix}\n",
    "1  \\\\\n",
    "-2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $b=0$\n",
    "Now let \n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\\n",
    "1 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first step in the neural network is to multiply the input matrix by first layer's weight matrix:\n",
    "\n",
    "$$\n",
    "XW = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\ \n",
    "1 & 1 \\\\\n",
    "2 & 2 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Next, we add the bias vector c, to obtain\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\ \n",
    "1 & 0 \\\\\n",
    "2 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "As shown in the first image above, they now lie in a space where a linear model can solve the problem.\n",
    "We finish with multiplying by the weight vector $w$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\ \n",
    "1 \\\\\n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The neural network has obtained the correct answer for every example in the batch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
