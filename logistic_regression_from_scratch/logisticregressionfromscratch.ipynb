{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a logistic regression model for classifying whether a patient has diabetes or not. We will\n",
    "only use python to build functions for reading, normalizing data, optimizing parameters, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a **supervised** machine learning algorithm used for **classification** purposes.\n",
    "Logistic Regression is somewhat that same as linear regression but is has a different **cost functino** \n",
    "and **prediction function**.\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid Function: } g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Hypothesis: } h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "Note that the range of g is $[0,1]$ where values that are above and include 0.5 represent the class 1 and values below\n",
    "0.5 represent the class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost functions** find the error between that **actual value** and the **predicted value** of our\n",
    "algorithm. There error should be as small as possible. In the case of linear regression, the formula is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "Where m is the number of examples of rows in the data set, $x^i$ is the feature values of the i-th example,\n",
    "and $y^i$ is the actual outcome of the ith example.\n",
    "\n",
    "Note that this formula **cannot** be used for **logistic regression** since $h_\\theta$ is not convex so there\n",
    "is a chance of finding the local minima thus missing the global minima.\n",
    "\n",
    "If we apply $\\log$ to our cost function in such a way then we obtain\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$\n",
    "\n",
    "then we will obtain a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of an ML algorithm is to find the set of parameters that **minimizes**\n",
    "the **cost function**. Here is where we use optimization techniques. One of them\n",
    "is called gradient descent. \n",
    "\n",
    "First, we start with random values of parameters (in most cases **zero**) then\n",
    "keep changing the parameters to reduce $J(\\theta)$, the formula is:\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$\n",
    "\n",
    "where the \n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **Pima Indians Diabetes Dataset**. The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n",
    "\n",
    "The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skinfold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class variable (0 or 1).\n",
    "\n",
    "The baseline performance of predicting the most prevalent class is a classification accuracy of approximately 65%. Top results achieve a classification accuracy of approximately 77%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv file to tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0],\n",
       " [1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0, 0.0],\n",
       " [8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0, 1.0],\n",
       " [1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0, 0.0],\n",
       " [0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0, 1.0],\n",
       " [5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0, 0.0],\n",
       " [3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0, 1.0],\n",
       " [10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0, 0.0],\n",
       " [2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0, 1.0],\n",
       " [8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0, 1.0]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        read = csv.reader(f)\n",
    "        table = [val for val in read]\n",
    "    \n",
    "    return table\n",
    "\n",
    "def change_entries_to_float(table):\n",
    "    table = [[float(string) for string in row] for row in table]\n",
    "    return table\n",
    "\n",
    "table = read_file('Pima_Indians_Diabetes_Data.csv')\n",
    "data = change_entries_to_float(table)\n",
    "data[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 17.0],\n",
       " [0.0, 199.0],\n",
       " [0.0, 122.0],\n",
       " [0.0, 99.0],\n",
       " [0.0, 846.0],\n",
       " [0.0, 67.1],\n",
       " [0.078, 2.42],\n",
       " [21.0, 81.0],\n",
       " [0.0, 1.0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_minmax(dataset):\n",
    "    m = len(dataset[0])\n",
    "    min_max = []\n",
    "    for i in range(m):  \n",
    "        col_val = [row[i] for row in dataset]\n",
    "        min_max.append([min(col_val), max(col_val)]) \n",
    "\n",
    "    return min_max\n",
    "\n",
    "min_max = get_minmax(data)\n",
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.35294117647058826,\n",
       "  0.7437185929648241,\n",
       "  0.5901639344262295,\n",
       "  0.35353535353535354,\n",
       "  0.0,\n",
       "  0.5007451564828614,\n",
       "  0.23441502988898377,\n",
       "  0.48333333333333334,\n",
       "  1.0],\n",
       " [0.058823529411764705,\n",
       "  0.4271356783919598,\n",
       "  0.5409836065573771,\n",
       "  0.29292929292929293,\n",
       "  0.0,\n",
       "  0.3964232488822653,\n",
       "  0.11656703672075147,\n",
       "  0.16666666666666666,\n",
       "  0.0],\n",
       " [0.47058823529411764,\n",
       "  0.9195979899497487,\n",
       "  0.5245901639344263,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.34724292101341286,\n",
       "  0.2536293766011956,\n",
       "  0.18333333333333332,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(dataset, min_max):\n",
    "    n = len(dataset)\n",
    "    m = len(dataset[0])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            numerator = dataset[i][j] - min_max[j][0]\n",
    "            denominator = min_max[j][1] - min_max[j][0]\n",
    "            dataset[i][j] = numerator/denominator\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = normalize(data, min_max)\n",
    "data[:3]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data 80% Training Data  20% Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 614 \n",
      "Test Data: 154\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def split(dataset):\n",
    "    shuffle(dataset)\n",
    "    n = int(0.8*len(dataset))\n",
    "\n",
    "    train_data = dataset[:n]\n",
    "    test_data = dataset[n:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train, test = split(data)\n",
    "print('Train Data: {} \\nTest Data: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(pred, actual):\n",
    "    corr_val = 0\n",
    "    n = len(actual)\n",
    "    for i in range(n):\n",
    "        if pred[i] == actual[i]:\n",
    "            corr_val += 1\n",
    "    \n",
    "    return (corr_val/n)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `prediction` function is our hypothesis function that takes the whole row and parameters as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def prediction(row, param):\n",
    "    row = [1]+row[:-1]\n",
    "    m = len(row)\n",
    "\n",
    "    dot_prod = sum([row[i]*param[i] for i in range(m)])\n",
    "    sigmoid = 1/(1+math.exp(-dot_prod))\n",
    "\n",
    "    return sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `gradient_descent` function for finding the best set of parameters for our model. This function\n",
    "takes **dataset**, **epochs**(number of iterations), and **alpha**(learning rate) as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(dataset, epochs, alpha):\n",
    "\n",
    "    n = len(dataset)\n",
    "    params = [0]*n\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for row in dataset:\n",
    "            m = len(row)\n",
    "            pred = prediction(row, params)\n",
    "            params[0] = params[0] - (alpha/m)*(pred - row[-1])\n",
    "\n",
    "            for j in range(m):\n",
    "                params[j+1] = params[j+1] - (alpha/m)*(pred - row[-1])*row[j]\n",
    "\n",
    "    return params\n",
    "\n",
    "def algo(train, test, epochs=1000, alpha=0.001):\n",
    "\n",
    "    params = gradient_descent(train, epochs, alpha)\n",
    "    preds = []\n",
    "\n",
    "    for row in test:\n",
    "        pred = prediction(row, params)\n",
    "        preds.append(round(pred))\n",
    "    y_actual = [row[-1] for row in test]\n",
    "    acc = accuracy_check(preds, y_actual)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.53246753246754\n"
     ]
    }
   ],
   "source": [
    "acc = algo(train,test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully build a **Logistic Regression** model from scratch with out using **pandas**, **scikit learn**."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
