{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a logistic regression model for classifying whether a patient has diabetes or not. We will\n",
    "only use python to build functions for reading, normalizing data, optimizing parameters, and more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a **supervised** machine learning algorithm used for **classification** purposes.\n",
    "Logistic Regression is somewhat the same as linear regression but is has a different **cost function** \n",
    "and **prediction function**.\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid Function: } g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Hypothesis: } h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "Note that the range of g is $[0,1]$ where values that are above and include a threshold $\\alpha\\in (0,1)$ represent the class 1 and values below\n",
    "$\\alpha$ represent the class 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost functions** find the error between the **actual value** and the **predicted value** of our\n",
    "algorithm. The error should be as small as possible. \n",
    "\n",
    "In the case of linear regression, the formula is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)^2\n",
    "$$\n",
    "\n",
    "Where m is the number of examples of rows in the data set, $x^i$ is the feature values of the $i-th$ example,\n",
    "and $y^i$ is the actual outcome of the $i-th$ example. Note that we want each $(h_\\theta(x^i) - y^i)^2$ as small as possible but this formula **cannot** be used for **logistic regression** since $h_\\theta$ is not convex so there is a chance of finding the local minima thus missing the global minima. Let us change each $(h_\\theta(x^i) - y^i)^2$ to \n",
    "\n",
    "$$\n",
    "-y^i \\log (h_\\theta(x^i)) - (1-y^i)\\log (1 - h_\\theta(x^i))\n",
    "$$\n",
    "\n",
    " In case $y^i=1$, the output (i.e. the cost to pay) approaches to 0 as $h_\\theta(x^i)$ approaches to 1. Conversely, the cost to pay grows to infinity as $h_\\theta(x^i)$ approaches to 0. You can clearly see it in the plot below, left side. \n",
    "\n",
    "<img src='img/ex1.png'>\n",
    "\n",
    "This is a desirable property: we want a bigger penalty as the algorithm predicts something far away from the actual value. If the label is $y^i=1$ but the algorithm predicts $h_\\theta(x^i) = 0$, the outcome is completely wrong.\n",
    "\n",
    "Conversely, the same intuition applies when ùë¶=0, depicted in the plot above, right side. Bigger penalties when the label is $y^i=0$ but the algorithm predicts $h_\\theta(x^i) = 1$. Each term is convex and we want each term as small as possible so we can rewrite our new cost function as\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of an ML algorithm is to find the set of parameters that **minimizes**\n",
    "the **cost function**. Here is where we use optimization techniques. One of them\n",
    "is called gradient descent. \n",
    "\n",
    "First, we start with random values of parameters (in most cases **zero**) then\n",
    "keep changing the parameters to reduce $J(\\theta)$, the formula is:\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$ \n",
    "\n",
    "So we have\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **Pima Indians Diabetes Dataset**. The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n",
    "\n",
    "The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skinfold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class variable (0 or 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv file to tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0],\n",
       " [1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0, 0.0],\n",
       " [8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0, 1.0],\n",
       " [1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0, 0.0],\n",
       " [0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0, 1.0],\n",
       " [5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0, 0.0],\n",
       " [3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0, 1.0],\n",
       " [10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0, 0.0],\n",
       " [2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0, 1.0],\n",
       " [8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0, 1.0]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        read = csv.reader(f)\n",
    "        table = [val for val in read]\n",
    "    \n",
    "    return table\n",
    "\n",
    "def change_entries_to_float(table):\n",
    "    table = [[float(string) for string in row] for row in table]\n",
    "    return table\n",
    "\n",
    "table = read_file('Pima_Indians_Diabetes_Data.csv')\n",
    "data = change_entries_to_float(table)\n",
    "data[:10]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 17.0],\n",
       " [0.0, 199.0],\n",
       " [0.0, 122.0],\n",
       " [0.0, 99.0],\n",
       " [0.0, 846.0],\n",
       " [0.0, 67.1],\n",
       " [0.078, 2.42],\n",
       " [21.0, 81.0],\n",
       " [0.0, 1.0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_minmax(dataset):\n",
    "    m = len(dataset[0])\n",
    "    min_max = []\n",
    "    for i in range(m):  \n",
    "        col_val = [row[i] for row in dataset]\n",
    "        min_max.append([min(col_val), max(col_val)]) \n",
    "\n",
    "    return min_max\n",
    "\n",
    "min_max = get_minmax(data)\n",
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.35294117647058826,\n",
       "  0.7437185929648241,\n",
       "  0.5901639344262295,\n",
       "  0.35353535353535354,\n",
       "  0.0,\n",
       "  0.5007451564828614,\n",
       "  0.23441502988898377,\n",
       "  0.48333333333333334,\n",
       "  1.0],\n",
       " [0.058823529411764705,\n",
       "  0.4271356783919598,\n",
       "  0.5409836065573771,\n",
       "  0.29292929292929293,\n",
       "  0.0,\n",
       "  0.3964232488822653,\n",
       "  0.11656703672075147,\n",
       "  0.16666666666666666,\n",
       "  0.0],\n",
       " [0.47058823529411764,\n",
       "  0.9195979899497487,\n",
       "  0.5245901639344263,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.34724292101341286,\n",
       "  0.2536293766011956,\n",
       "  0.18333333333333332,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(dataset, min_max):\n",
    "    n = len(dataset)\n",
    "    m = len(dataset[0])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            numerator = dataset[i][j] - min_max[j][0]\n",
    "            denominator = min_max[j][1] - min_max[j][0]\n",
    "            dataset[i][j] = numerator/denominator\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "data = normalize(data, min_max)\n",
    "data[:3]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data 80% Training Data  20% Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: 614 \n",
      "Test Data: 154\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def split(dataset):\n",
    "    shuffle(dataset)\n",
    "    n = int(0.8*len(dataset))\n",
    "\n",
    "    train_data = dataset[:n]\n",
    "    test_data = dataset[n:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train, test = split(data)\n",
    "print('Train Data: {} \\nTest Data: {}'.format(len(train), len(test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_check(pred, actual):\n",
    "    corr_val = 0\n",
    "    n = len(actual)\n",
    "    for i in range(n):\n",
    "        if pred[i] == actual[i]:\n",
    "            corr_val += 1\n",
    "    \n",
    "    return (corr_val/n)*100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `prediction` function is our hypothesis function that takes the whole row and parameters as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def prediction(row, param):\n",
    "    row = [1]+row[:-1]\n",
    "    m = len(row)\n",
    "\n",
    "    dot_prod = sum([row[i]*param[i] for i in range(m)])\n",
    "    sigmoid = 1/(1+math.exp(-dot_prod))\n",
    "\n",
    "    return sigmoid\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cost function to calculate the cost with every iteration and plot that data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(dataset, params):\n",
    "    n = len(dataset)\n",
    "    cost = 0\n",
    "    for row in dataset:\n",
    "        pred = prediction(row, params)\n",
    "        target = row[-1]\n",
    "        cost += -(target*math.log(pred))+(-(1-target)*math.log(1-pred))\n",
    "    avg = cost/n\n",
    "    return avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `gradient_descent` function for finding the best set of parameters for our model. This function\n",
    "takes **dataset**, **epochs**(number of iterations), and **alpha**(learning rate) as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_descent(dataset, epochs, alpha):\n",
    "\n",
    "    n = len(dataset)\n",
    "    params = [0]*n\n",
    "    cost_hist = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for row in dataset:\n",
    "            m = len(row)\n",
    "            pred = prediction(row, params)\n",
    "            params[0] = params[0] - (alpha/m)*(pred - row[-1])\n",
    "\n",
    "            for j in range(m):\n",
    "                params[j+1] = params[j+1] - (alpha/m)*(pred - row[-1])*row[j]\n",
    "        cost_hist.append(cost_function(dataset, params))\n",
    "    return cost_hist, params\n",
    "\n",
    "def algo(train, test, epochs=1000, alpha=0.001):\n",
    "\n",
    "    cost_hist, params = gradient_descent(train, epochs, alpha)\n",
    "    preds = []\n",
    "\n",
    "    for row in test:\n",
    "        pred = prediction(row, params)\n",
    "        preds.append(round(pred))\n",
    "    y_actual = [row[-1] for row in test]\n",
    "    acc = accuracy_check(preds, y_actual)\n",
    "\n",
    "    iterations = [i for i in range(1, epochs+1)]\n",
    "    plt.plot(iterations, cost_hist)\n",
    "    plt.show()\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.28571428571429\n"
     ]
    }
   ],
   "source": [
    "acc = algo(train,test)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully built a **Logistic Regression** model from scratch with out using **pandas**, **scikit learn**.\n",
    "Note that matplotlib was not neccessary but we did use it to see how the cost function decreases for each iteration."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
