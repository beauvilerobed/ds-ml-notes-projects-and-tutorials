{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a logistic regression model for classifying whether a patient has diabetes or not. We will\n",
    "only use python to build functions for reading, normalizing data, optimizing parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a **supervised** machine learning algorithm used for **classification** purposes.\n",
    "Logistic Regression is somewhat the same as linear regression but is has a different **cost function** \n",
    "and **prediction function**.\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid Function: } g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Hypothesis: } h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "Note that the range of g is $[0,1]$ where values that are above and include a threshold $\\alpha\\in (0,1)$ represent the class 1 and values below\n",
    "$\\alpha$ represent the class 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost functions** find the error between the **actual value** and the **predicted value** of our\n",
    "algorithm. The error should be as small as possible. \n",
    "\n",
    "In the case of linear regression, the formula is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^i - y^i)^2\n",
    "$$\n",
    "\n",
    "Where m is the number of examples of rows in the data set, $x^i$ is the feature values of the $i-th$ example,\n",
    "and $y^i$ is the actual outcome of the $i-th$ example. Note that we want each $(h_\\theta(x^i) - y^i)^2$ as small as possible but this formula **cannot** be used for **logistic regression** since $h_\\theta$ is not convex so there is a chance of finding the local minima thus missing the global minima. Let us change each term in the summation above to \n",
    "\n",
    "$$\n",
    "-y^i \\log (h_\\theta(x^i)) - (1-y^i)\\log (1 - h_\\theta(x^i))\n",
    "$$\n",
    "\n",
    " In case $y^i=1$, the output (i.e. the cost to pay) approaches to 0 as $h_\\theta(x^i)$ approaches to 1. Conversely, the cost to pay grows to infinity as $h_\\theta(x^i)$ approaches to 0. You can clearly see it in the plot below, left side. \n",
    "\n",
    "<img src='img/ex1.png'>\n",
    "\n",
    "This is a desirable property: we want a bigger penalty as the algorithm predicts something far away from the actual value. If the label is $y^i=1$ but the algorithm predicts $h_\\theta(x^i) = 0$, the outcome is completely wrong.\n",
    "\n",
    "Conversely, the same intuition applies when ð‘¦=0, depicted in the plot above, right side. Bigger penalties when the label is $y^i=0$ but the algorithm predicts $h_\\theta(x^i) = 1$. Each term is convex and we want each term as small as possible so we can rewrite our new cost function as\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of an ML algorithm is to find the set of parameters that **minimizes**\n",
    "the **cost function**. Here is where we use optimization techniques. One of them\n",
    "is called gradient descent. \n",
    "\n",
    "First, we start with random values of parameters (in most cases **zero**) then\n",
    "keep changing the parameters to reduce $J(\\theta)$, the formula is:\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$ \n",
    "\n",
    "or\n",
    " \n",
    "$$\n",
    "X^T(h_\\theta(x) - y)\n",
    "$$\n",
    "\n",
    "So we have\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **Pima Indians Diabetes Dataset**. The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n",
    "\n",
    "The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skinfold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class variable (0 or 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv file to tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n",
       "        6.270e-01, 5.000e+01, 1.000e+00],\n",
       "       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n",
       "        3.510e-01, 3.100e+01, 0.000e+00],\n",
       "       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n",
       "        6.720e-01, 3.200e+01, 1.000e+00],\n",
       "       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n",
       "        1.670e-01, 2.100e+01, 0.000e+00],\n",
       "       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n",
       "        2.288e+00, 3.300e+01, 1.000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('Pima_Indians_Diabetes_Data.csv') as file:\n",
    "    table  = csv.reader(file)\n",
    "    table = [val for val in table]\n",
    "\n",
    "data = [[float(num) for num in row] for row in table]\n",
    "data = np.array(data)\n",
    "data[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "n = data.shape[1]\n",
    "\n",
    "for i in range(n):\n",
    "    data[:,i] = (data[:,i] - data[:,i].min(axis = 0))/(data[:,i].max(axis=0) - data[:,i].min(axis=0))\n",
    "\n",
    "for i in range(n):\n",
    "    minumum = data[:,i].min(axis=0)\n",
    "    maximum = data[:,i].max(axis=0)\n",
    "    print(minumum, maximum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data 80% Training Data  20% Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape (614, 9) and test shape (154, 9)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "\n",
    "m = data.shape[0]\n",
    "split_point = int(.8*m)\n",
    "\n",
    "training, test = data[:split_point], data[split_point:]\n",
    "print('training shape {} and test shape {}'.format(training.shape, test.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(actual, predicted):\n",
    "    n = len(actual)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(n):\n",
    "        if (actual[i] == 1) and (predicted[i] == 1):\n",
    "            TP += 1\n",
    "        if (actual[i] == 0) and (predicted[i] == 0):\n",
    "            TN += 1\n",
    "        if (actual[i] == 1) and (predicted[i] == 0):\n",
    "            FN += 1\n",
    "        if (actual[i] == 0) and (predicted[i] == 1):\n",
    "            FP += 1\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    f = 2*(precision*recall/(precision+recall))\n",
    "        \n",
    "    return f\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `prediction` function is our hypothesis function that takes the whole row and parameters as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp(z):\n",
    "    return 1/(1+np.e**(-z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cost function to calculate the cost with every iteration and plot that data point.\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(data, params):\n",
    "    y = data[:,-1]\n",
    "    X = np.c_[np.ones((data.shape[0],1)), data[:,:-1]]\n",
    "\n",
    "    z = np.dot(X,params)\n",
    "    logyhat = np.log2(hyp(z))\n",
    "    otheryhat = np.log2(1-hyp(z))\n",
    "\n",
    "    cost0 = y.T.dot(logyhat)\n",
    "    cost1 = (1-y).T.dot(otheryhat)\n",
    "\n",
    "    return -(cost0+cost1)/len(y)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `gradient_descent` function for finding the best set of parameters for our model. This function\n",
    "takes **dataset**, **epochs**(number of iterations), and **alpha**(learning rate) as arguments. \n",
    "$$\n",
    "X^T(h_\\theta(x) - y)\n",
    "$$\n",
    "\n",
    "So we have\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train, iter=1000, alpha=.001):\n",
    "    params = np.zeros((train.shape[1],1))\n",
    "    X = np.c_[np.c_[np.ones((train.shape[0],1)), train[:,:-1]]]\n",
    "    y = train[:,-1]\n",
    "    y = np.reshape(y,(len(y),1))\n",
    "    costs = []\n",
    "\n",
    "    for _ in range(iter):\n",
    "        z = np.dot(X,params)\n",
    "        logyhat = hyp(z)   \n",
    "        params = params - alpha*np.dot(X.T, logyhat - y)\n",
    "\n",
    "        costs.append(cost(train, params))\n",
    "\n",
    "    return costs, params\n",
    "\n",
    "\n",
    "def predict(test, params):\n",
    "    X = np.c_[np.c_[np.ones((test.shape[0],1)), test[:,:-1]]]\n",
    "    y = test[:,-1]\n",
    "\n",
    "    z = np.dot(X,params)\n",
    "    yhat = hyp(z)\n",
    "    yhat_bin = np.where(yhat > .5, 1, 0)\n",
    "\n",
    "    return yhat_bin, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score  0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "costs, params = fit(training)\n",
    "yhat_bin, yhat = predict(test, params)\n",
    "\n",
    "print('f1-score ', fscore(test[:,-1] ,yhat_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_axis = costs\n",
    "\n",
    "plt.plot(y_axis) \n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('cost') \n",
    "plt.title('cost per iteraction') \n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letâ€™s see how our logistic regression fares in comparison to sklearnâ€™s logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6249999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Pima_Indians_Diabetes_Data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=101)\n",
    "pipe = make_pipeline(MinMaxScaler(), LogisticRegression())\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "fscore = f1_score(y_pred, y_test)\n",
    "print('f1 score: ', fscore)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully built a **Logistic Regression** model from scratch with out using **pandas**, **scikit learn**.\n",
    "Note that matplotlib was not neccessary but we did use it to see how the cost function decreases for each iteration."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
