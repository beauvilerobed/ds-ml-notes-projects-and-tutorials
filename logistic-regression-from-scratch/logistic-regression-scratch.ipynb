{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a logistic regression model for classifying whether a patient has diabetes or not. We will\n",
    "only use python to build functions for reading, normalizing data, optimizing parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Logistic Regression?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a **supervised** machine learning algorithm used for **classification** purposes.\n",
    "Logistic Regression is somewhat the same as linear regression but is has a different **cost function** \n",
    "and **prediction function**.\n",
    "\n",
    "$$\n",
    "\\text{Sigmoid Function: } g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Hypothesis: } h_\\theta(x) = \\frac{1}{1+e^{-\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "Note that the range of g is $[0,1]$ where values that are above and include a threshold $\\alpha\\in (0,1)$ represent the class 1 and values below\n",
    "$\\alpha$ represent the class 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost functions** find the error between the **actual value** and the **predicted value** of our\n",
    "algorithm. The error should be as small as possible. \n",
    "\n",
    "In the case of linear regression, the formula is\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (\\theta^T x^i - y^i)^2\n",
    "$$\n",
    "\n",
    "Where m is the number of examples of rows in the data set, $x^i$ is the feature values of the $i-th$ example,\n",
    "and $y^i$ is the actual outcome of the $i-th$ example. Note that we want each $(h_\\theta(x^i) - y^i)^2$ as small as possible but this formula **cannot** be used for **logistic regression** since $h_\\theta$ is not convex so there is a chance of finding the local minima thus missing the global minima. Let us change each term in the summation above to \n",
    "\n",
    "$$\n",
    "-y^i \\log (h_\\theta(x^i)) - (1-y^i)\\log (1 - h_\\theta(x^i))\n",
    "$$\n",
    "\n",
    " In case $y^i=1$, the output (i.e. the cost to pay) approaches to 0 as $h_\\theta(x^i)$ approaches to 1. Conversely, the cost to pay grows to infinity as $h_\\theta(x^i)$ approaches to 0. You can clearly see it in the plot below, left side. \n",
    "\n",
    "<img src='img/ex1.png'>\n",
    "\n",
    "This is a desirable property: we want a bigger penalty as the algorithm predicts something far away from the actual value. If the label is $y^i=1$ but the algorithm predicts $h_\\theta(x^i) = 0$, the outcome is completely wrong.\n",
    "\n",
    "Conversely, the same intuition applies when ùë¶=0, depicted in the plot above, right side. Bigger penalties when the label is $y^i=0$ but the algorithm predicts $h_\\theta(x^i) = 1$. Each term is convex and we want each term as small as possible so we can rewrite our new cost function as\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of an ML algorithm is to find the set of parameters that **minimizes**\n",
    "the **cost function**. Here is where we use optimization techniques. One of them\n",
    "is called gradient descent. \n",
    "\n",
    "First, we start with random values of parameters (in most cases **zero**) then\n",
    "keep changing the parameters to reduce $J(\\theta)$, the formula is:\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j} J(\\theta)\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$ \n",
    "\n",
    "or\n",
    " \n",
    "$$\n",
    "X^T(h_\\theta(x) - y)\n",
    "$$\n",
    "\n",
    "So we have\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **Pima Indians Diabetes Dataset**. The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n",
    "\n",
    "The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n",
    "\n",
    "1. Number of times pregnant.\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "3. Diastolic blood pressure (mm Hg).\n",
    "4. Triceps skinfold thickness (mm).\n",
    "5. 2-Hour serum insulin (mu U/ml).\n",
    "6. Body mass index (weight in kg/(height in m)^2).\n",
    "7. Diabetes pedigree function.\n",
    "8. Age (years).\n",
    "9. Class variable (0 or 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let's Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv file to tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n",
       "        6.270e-01, 5.000e+01, 1.000e+00],\n",
       "       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n",
       "        3.510e-01, 3.100e+01, 0.000e+00],\n",
       "       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n",
       "        6.720e-01, 3.200e+01, 1.000e+00],\n",
       "       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n",
       "        1.670e-01, 2.100e+01, 0.000e+00],\n",
       "       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n",
       "        2.288e+00, 3.300e+01, 1.000e+00]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('Pima_Indians_Diabetes_Data.csv') as file:\n",
    "    table  = csv.reader(file)\n",
    "    table = [val for val in table]\n",
    "\n",
    "data = [[float(num) for num in row] for row in table]\n",
    "data = np.array(data)\n",
    "data[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n",
      "0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "n = data.shape[1]\n",
    "\n",
    "for i in range(n):\n",
    "    data[:,i] = (data[:,i] - data[:,i].min(axis = 0))/(data[:,i].max(axis=0) - data[:,i].min(axis=0))\n",
    "\n",
    "for i in range(n):\n",
    "    minumum = data[:,i].min(axis=0)\n",
    "    maximum = data[:,i].max(axis=0)\n",
    "    print(minumum, maximum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data 80% Training Data  20% Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape (614, 9) and test shape (154, 9)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "\n",
    "m = data.shape[0]\n",
    "split_point = int(.8*m)\n",
    "\n",
    "training, test = data[:split_point], data[split_point:]\n",
    "print('training shape {} and test shape {}'.format(training.shape, test.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(actual, predicted):\n",
    "    n = len(actual)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(n):\n",
    "        if (actual[i] == 1) and (predicted[i] == 1):\n",
    "            TP += 1\n",
    "        if (actual[i] == 0) and (predicted[i] == 0):\n",
    "            TN += 1\n",
    "        if (actual[i] == 1) and (predicted[i] == 0):\n",
    "            FN += 1\n",
    "        if (actual[i] == 0) and (predicted[i] == 1):\n",
    "            FP += 1\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    f = 2*(precision*recall/(precision+recall))\n",
    "        \n",
    "    return f\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `prediction` function is our hypothesis function that takes the whole row and parameters as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp(z):\n",
    "    return 1/(1+np.e**(-z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cost function to calculate the cost with every iteration and plot that data point.\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big[y^i \\log (h_\\theta(x^i)) + (1-y^i)\\log (1 - h_\\theta(x^i))\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(data, params):\n",
    "    y = data[:,-1]\n",
    "    X = np.c_[np.ones((data.shape[0],1)), data[:,:-1]]\n",
    "\n",
    "    z = np.dot(X,params)\n",
    "    logyhat = np.log2(hyp(z))\n",
    "    otheryhat = np.log2(1-hyp(z))\n",
    "\n",
    "    cost0 = y.T.dot(logyhat)\n",
    "    cost1 = (1-y).T.dot(otheryhat)\n",
    "\n",
    "    return -(cost0+cost1)/len(y)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `gradient_descent` function for finding the best set of parameters for our model. This function\n",
    "takes **dataset**, **epochs**(number of iterations), and **alpha**(learning rate) as arguments. \n",
    "$$\n",
    "X^T(h_\\theta(x) - y)\n",
    "$$\n",
    "\n",
    "So we have\n",
    "\n",
    "Repeat:\n",
    "$$\n",
    "\\theta_j:= \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^i) - y^i)x_j^i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train, iter=1000, alpha=.001):\n",
    "    params = np.zeros((train.shape[1],1))\n",
    "    X = np.c_[np.c_[np.ones((train.shape[0],1)), train[:,:-1]]]\n",
    "    y = train[:,-1]\n",
    "    y = np.reshape(y,(len(y),1))\n",
    "    costs = []\n",
    "\n",
    "    for _ in range(iter):\n",
    "        z = np.dot(X,params)\n",
    "        logyhat = hyp(z)   \n",
    "        params = params - alpha*np.dot(X.T, logyhat - y)\n",
    "\n",
    "        costs.append(cost(train, params))\n",
    "\n",
    "    return costs, params\n",
    "\n",
    "\n",
    "def predict(test, params):\n",
    "    X = np.c_[np.c_[np.ones((test.shape[0],1)), test[:,:-1]]]\n",
    "    y = test[:,-1]\n",
    "\n",
    "    z = np.dot(X,params)\n",
    "    yhat = hyp(z)\n",
    "    yhat_bin = np.where(yhat > .5, 1, 0)\n",
    "\n",
    "    return yhat_bin, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score  0.6122448979591837\n"
     ]
    }
   ],
   "source": [
    "costs, params = fit(training)\n",
    "yhat_bin, yhat = predict(test, params)\n",
    "\n",
    "print('f1-score ', fscore(test[:,-1] ,yhat_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_axis = costs\n",
    "\n",
    "plt.plot(y_axis) \n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('cost') \n",
    "plt.title('cost per iteraction') \n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let‚Äôs see how our logistic regression fares in comparison to sklearn‚Äôs logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.6249999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Pima_Indians_Diabetes_Data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=101)\n",
    "pipe = make_pipeline(MinMaxScaler(), LogisticRegression())\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "fscore = f1_score(y_pred, y_test)\n",
    "print('f1 score: ', fscore)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully built a **Logistic Regression** model from scratch with out using **pandas**, **scikit learn**.\n",
    "Note that matplotlib was not neccessary but we did use it to see how the cost function decreases for each iteration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
