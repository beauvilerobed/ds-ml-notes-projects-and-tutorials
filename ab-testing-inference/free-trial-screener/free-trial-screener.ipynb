{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Tests with Python \"free trial\" Screener\n",
    "**Experiment Name**: \"Free Trial\" Screener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Setup\n",
    "\n",
    "- Udacity course pages currently show two options: **“Start free trial”** and **“Access course materials.”**\n",
    "  - **Start free trial:** Students enter credit card info and get a 14-day trial of the paid version. They’re charged automatically after 14 days unless they cancel.  \n",
    "  - **Access course materials:** Students can view videos and take quizzes for free, but don’t get coaching, feedback, or a certificate.\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "- Udacity tested adding a **time commitment question** after clicking “Start free trial.”  \n",
    "  - If a student said they could spend **5+ hours/week**, they continued to checkout as usual.  \n",
    "  - If they said **less than 5 hours/week**, they saw a message explaining that Udacity courses usually need more time and suggesting they might prefer the free materials option.  \n",
    "  - Students could then **either continue with the free trial** or **choose the free materials**.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "The change would help set realistic expectations and reduce cancellations from students who didn’t have enough time—without significantly lowering the number of paying or completing students.  \n",
    "This could improve student satisfaction and let coaches focus on learners likely to finish.\n",
    "\n",
    "## Experiment Details\n",
    "\n",
    "- **Unit of diversion:** cookie  \n",
    "- If a student enrolls in a free trial, tracking switches to **user ID** (a user can’t start multiple free trials).  \n",
    "- Users who don’t enroll aren’t tracked by user ID, even if they were signed in.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Choice\n",
    "\n",
    "A successful experiment needs two types of metrics: **Invariant** and **Evaluation** metrics.\n",
    "\n",
    "- **Invariant metrics**: Should *not* be affected by the experiment. They help verify that randomization worked and groups are comparable.\n",
    "- **Evaluation metrics**: Measure the impact of the experiment and relate directly to business goals.\n",
    "\n",
    "Each metric includes a **$D_{min}$** — the minimum meaningful change for the business.  \n",
    "(Example: a retention increase below 2% may be statistically significant but not practically useful.)\n",
    "\n",
    "### Invariant Metrics – Sanity Checks\n",
    "\n",
    "| Metric Name | Formula | $D_{min}$ | Notation |\n",
    "|--------------|----------|------------|-----------|\n",
    "| Cookies on course overview page | # unique daily cookies on page | 3000 cookies | $C_k$ |\n",
    "| Clicks on Free Trial button | # unique daily cookies who clicked | 240 clicks | $C_l$ |\n",
    "| Free Trial Click-Through Probability | $\\frac{C_l}{C_k}$ | 0.01 | $CTP$ |\n",
    "\n",
    "### Evaluation Metrics – Performance Indicators\n",
    "\n",
    "| Metric Name | Formula | $D_{min}$ | Notation |\n",
    "|--------------|----------|------------|-----------|\n",
    "| **Gross Conversion** | $\\frac{enrolled}{C_l}$ | 0.01 | $Conversion_{Gross}$ |\n",
    "| **Retention** | $\\frac{paid}{enrolled}$ | 0.01 | $Retention$ |\n",
    "| **Net Conversion** | $\\frac{paid}{C_l}$ | 0.0075 | $Conversion_{Net}$ |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the baseline values of metrics \n",
    "\n",
    "Before we start our experiment we should know how these metrics behave before the change - that is, what are their baseline values.\n",
    "\n",
    "### Collecting estimators data \n",
    "|Item|Description|Estimator|\n",
    "|--|--|--|\n",
    "|Number of cookies|\tDaily unique cookies to view course overview page|\t40,000|\n",
    "|Number of clicks|\tDaily unique cookies to click Free Trial button\t|3,200|\n",
    "|Number of enrollments\t|Free Trial enrollments per day|\t660|\n",
    "|CTP\t|CTP on Free Trial button\t|0.08|\n",
    "|Gross Conversion\t|Probability of enrolling, given a click\t|0.20625|\n",
    "|Retention\t|Probability of payment, given enrollment\t|0.53|\n",
    "|Net Conversion\t|Probability of payment, given click\t|0.109313|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the baseline estimators in a dictionary for easy access\n",
    "baseline = {\n",
    "    \"Cookies\": 40_000,\n",
    "    \"Clicks\": 3_200,\n",
    "    \"Enrollments\": 660,\n",
    "    \"CTP\": 0.08,\n",
    "    \"GConversion\": 0.20625,\n",
    "    \"Retention\": 0.53,\n",
    "    \"NConversion\": 0.109313\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Standard Deviation\n",
    "\n",
    "After collecting metric estimates, we calculate the **standard deviation** to use in **sample size** and **confidence interval** calculations.  \n",
    "A higher variance means it’s harder to detect a statistically significant effect.\n",
    "\n",
    "Assuming **5,000 cookies** visit the course overview page per day (as stated in the project instructions), we’ll estimate the standard deviation for the **evaluation metrics only**.  \n",
    "This sample size is smaller than the total population but large enough to form two comparison groups.\n",
    "\n",
    "### Scaling Collected Data\n",
    "\n",
    "Before calculating variance, we need to **scale** our collected metric counts to match the sample size used for variance estimation.  \n",
    "In this case, we scale from **40,000 unique cookies per day** (original data) down to **5,000 cookies per day**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale counts from 40,000 cookies to 5,000 cookies\n",
    "scale_factor = 5_000 / baseline[\"Cookies\"]\n",
    "\n",
    "baseline[\"Cookies\"] = 5_000\n",
    "baseline[\"Clicks\"] = baseline[\"Clicks\"] * scale_factor\n",
    "baseline[\"Enrollments\"] = baseline[\"Enrollments\"] * scale_factor\n",
    "\n",
    "baseline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Analytically\n",
    "\n",
    "To estimate variance analytically, we assume metrics that represent probabilities $(\\hat{p})$ follow a **binomial distribution**.  \n",
    "The standard deviation can then be calculated using:\n",
    "\n",
    "$$\n",
    "SD = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
    "$$\n",
    "\n",
    "This assumption holds only when the **unit of diversion** (how users are split) matches the **unit of analysis** (the denominator in the metric formula).  \n",
    "\n",
    "### What Happens if Units Differ?\n",
    "\n",
    "Suppose the **unit of diversion** is a user ID, but a single user may click multiple times:\n",
    "\n",
    "- User A clicks 3 times → potentially 3 “trials” in the denominator.  \n",
    "- User B clicks once → 1 trial.  \n",
    "\n",
    "Outcomes from the same user are **correlated** (if User A doesn’t enroll once, they probably won’t enroll on other clicks).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "- **Independent Bernoulli trials:**  \n",
    "$\n",
    "Var(X) = n p (1-p)\n",
    "$\n",
    "\n",
    "- **Correlated trials:**  \n",
    "$\n",
    "Var(X) > n p (1-p)\n",
    "$ (assuming positive correlation between all users)\n",
    "\n",
    "**Why?** Because repeated measures from the same user aren’t adding as much independent information. The effective sample size is smaller than the count of trials. The actual variance may vary, and it’s better to estimate it **empirically** using collected data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gross Conversion** – The baseline represents the probability of enrollment given a click.\n",
    "\n",
    "> In this case, the **unit of diversion** (cookies) — how users are split between control and experiment — is the same as the **unit of analysis** (cookies who click), which is the denominator in the Gross Conversion formula.  \n",
    "\n",
    "Because these units match, the **analytical estimate of variance** is valid and sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute parameters for Gross Conversion (GC)\n",
    "GC = {}\n",
    "\n",
    "# Minimum detectable effect (if needed later)\n",
    "GC[\"d_min\"] = 0.01\n",
    "\n",
    "# Probability of conversion (given or calculated as enrollments/clicks)\n",
    "GC[\"p\"] = baseline[\"GConversion\"]\n",
    "\n",
    "# Number of trials (clicks)\n",
    "GC[\"n\"] = baseline[\"Clicks\"]\n",
    "\n",
    "# Standard deviation for a proportion: sqrt(p * (1 - p) / n), rounded to 4 decimals\n",
    "GC[\"sd\"] = round(mt.sqrt(GC[\"p\"] * (1 - GC[\"p\"]) / GC[\"n\"]), 4)\n",
    "\n",
    "GC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retention** - The baseline is the probability of payment, given enrollment. The sample size is the number of enrolled users. \n",
    "\n",
    ">In this case, unit of diversion is not equal to unit of analysis (users who enrolled) so an analytical estimation is not enough \n",
    "\n",
    "If we had the data for these estimates, we would want to estimate this variance empirically as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute parameters for Retention (R)\n",
    "R = {}\n",
    "\n",
    "# Minimum detectable effect\n",
    "R[\"d_min\"] = 0.01\n",
    "\n",
    "# Probability of retention\n",
    "R[\"p\"] = baseline[\"Retention\"]\n",
    "\n",
    "# Number of trials (enrollments)\n",
    "R[\"n\"] = baseline[\"Enrollments\"]\n",
    "\n",
    "# Standard deviation for a proportion: sqrt(p * (1 - p) / n), rounded to 4 decimals\n",
    "R[\"sd\"] = round(mt.sqrt(R[\"p\"] * (1 - R[\"p\"]) / R[\"n\"]), 4)\n",
    "\n",
    "R\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Net Conversion** - The baseline is the probability of payment, given a click. The sample size is the number of cookies that clicked. \n",
    "> In this case, the unit of analysis and diversion are equal so we expect a good enough estimation analytically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute parameters for Net Conversion (NC)\n",
    "NC = {}\n",
    "\n",
    "# Minimum detectable effect\n",
    "NC[\"d_min\"] = 0.0075\n",
    "\n",
    "# Probability of net conversion\n",
    "NC[\"p\"] = baseline[\"NConversion\"]\n",
    "\n",
    "# Number of trials (clicks)\n",
    "NC[\"n\"] = baseline[\"Clicks\"]\n",
    "\n",
    "# Standard deviation for a proportion: sqrt(p * (1 - p) / n), rounded to 4 decimals\n",
    "NC[\"sd\"] = round(mt.sqrt(NC[\"p\"] * (1 - NC[\"p\"]) / NC[\"n\"]), 4)\n",
    "\n",
    "NC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Sizing\n",
    "\n",
    "Given  $\\alpha = 0.05$ (significance level) and  $\\beta = 0.2$ (power), we want to estimate how many total pageviews (cookies who viewed the course overview page) are needed in the experiment. This total will be divided into the two groups: control and experiment.\n",
    "\n",
    "The minimum sample size for control and experiment groups, which provides a probability of **Type I Error** $\\alpha$, **Power** $1−\\beta$, **detectable effect** $d$, and **baseline conversion rate** $p$ (simple hypothesis)  \n",
    "\n",
    "$$\n",
    "H_0: P_{cont} - P_{exp} = 0\n",
    "$$\n",
    "\n",
    "against the simple alternative  \n",
    "\n",
    "$$\n",
    "H_A: P_{cont} - P_{exp} = d\n",
    "$$\n",
    "\n",
    "is:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\left(Z_{1-\\frac{\\alpha}{2}} sd_1 + Z_{1-\\beta} sd_2 \\right)^2}{d^2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "sd_1 = \\sqrt{2 p (1-p)}, \\quad sd_2 = \\sqrt{p (1-p) + (p+d)(1-(p+d))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Size Formula for Two-Sample Proportions\n",
    "\n",
    "Suppose we want to compare two proportions:\n",
    "\n",
    "- Control group proportion: $p_1 = p$  \n",
    "- Experiment group proportion: $p_2 = p + d$  \n",
    "- Detectable difference: $d = p_2 - p_1$  \n",
    "\n",
    "We aim for:\n",
    "\n",
    "- Significance level: $\\alpha$ (Type I error)  \n",
    "- Power: $1 - \\beta$ (probability of detecting a true effect)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Test Statistic\n",
    "\n",
    "For a two-sided z-test comparing proportions, the test statistic is:\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{p_1 (1-p_1)}{n} + \\frac{p_2 (1-p_2)}{n}}}\n",
    "$$\n",
    "\n",
    "where $n$ is the sample size per group.  \n",
    "\n",
    "- **Under $H_0$**: $p_1 = p_2 = p$\n",
    "\n",
    "$$\n",
    "SD_0 = \\sqrt{\\frac{p(1-p)}{n} + \\frac{p(1-p)}{n}} = \\sqrt{\\frac{2 p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "- **Under $H_A$**: $p_1 = p, p_2 = p+d$\n",
    "\n",
    "$$\n",
    "SD_A = \\sqrt{\\frac{p(1-p) + (p+d)(1-(p+d))}{n}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Critical Values for Significance and Power\n",
    "\n",
    "- Two-sided test significance $\\alpha$:\n",
    "\n",
    "$$\n",
    "Z_{\\text{crit}} = Z_{1-\\frac{\\alpha}{2}}\n",
    "$$\n",
    "\n",
    "- To achieve power $1-\\beta$, we require:\n",
    "\n",
    "$$\n",
    "P(\\text{Reject } H_0 \\mid H_A) = 1 - \\beta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relating Detectable Difference to z-Scores\n",
    "\n",
    "Let $X = \\hat{p}_1 - \\hat{p}_2$. Under $H_A$, \n",
    "\n",
    "$$\n",
    "X \\sim N(d, SD_A^2)\n",
    "$$\n",
    "\n",
    "Standardizing:\n",
    "\n",
    "$$\n",
    "P\\left( \\frac{X - d}{SD_A} > \\frac{Z_{\\text{crit}} SD_0 - d}{SD_A} \\right) = 1 - \\beta\n",
    "$$\n",
    "\n",
    "By the definition of the standard normal quantile:\n",
    "\n",
    "$$\n",
    "\\frac{Z_{\\text{crit}} SD_0 - d}{SD_A} = -Z_{1-\\beta}\n",
    "$$\n",
    "\n",
    "Rearranging gives:\n",
    "\n",
    "$$\n",
    "d = Z_{1-\\frac{\\alpha}{2}} SD_0 + Z_{1-\\beta} SD_A\n",
    "$$\n",
    "\n",
    "Substitute $SD_0$ and $SD_A$:\n",
    "\n",
    "$$\n",
    "d = Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{2 p(1-p)}{n}} + Z_{1-\\beta} \\sqrt{\\frac{p(1-p) + (p+d)(1-(p+d))}{n}}\n",
    "$$\n",
    "\n",
    "Factor out $1/\\sqrt{n}$:\n",
    "\n",
    "$$\n",
    "d = \\frac{1}{\\sqrt{n}} \\Bigg( Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))} \\Bigg)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Solve for Sample Size $n$\n",
    "\n",
    "$$\n",
    "\\sqrt{n} = \\frac{Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))}}{d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "n = \\frac{\\Big( Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))} \\Big)^2}{d^2}\n",
    "}\n",
    "$$\n",
    "\n",
    "This gives the **required sample size per group** to detect a difference $d$ with significance level $\\alpha$ and power $1-\\beta$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding inputs, we have all the data we need: Type 1 error $(\\alpha)$, power $(1−\\beta)$, detectable change $(d=Dmin)$ and baseline conversion rate, our $\\hat{p}$. What we need to calculate:\n",
    "\n",
    "- Get Z score for $1−\\frac{α}{2}$ and for $1−\\beta$\n",
    " \n",
    "- Get standard deviations 1 & 2, that is for both the baseline and for expected changed rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import math as mt\n",
    "\n",
    "# Function to get z-score for a given alpha\n",
    "# Input: alpha (significance level)\n",
    "# Returns: corresponding z-score\n",
    "def get_z_score(alpha):\n",
    "    return norm.ppf(alpha)\n",
    "\n",
    "\n",
    "# Function to compute standard deviations for baseline and expected change\n",
    "# Inputs:\n",
    "#   p : baseline conversion rate\n",
    "#   d : minimum detectable change\n",
    "# Returns: list [sd_baseline, sd_expected_change]\n",
    "def get_sds(p, d):\n",
    "    sd_baseline = mt.sqrt(2 * p * (1 - p))           # SD for baseline\n",
    "    sd_expected = mt.sqrt(p * (1 - p) + (p + d) * (1 - (p + d)))  # SD for expected change\n",
    "    return [sd_baseline, sd_expected]\n",
    "\n",
    "\n",
    "# Function to calculate minimum sample size per group\n",
    "# Inputs:\n",
    "#   sds   : list of standard deviations [baseline_sd, expected_sd]\n",
    "#   alpha : significance level\n",
    "#   beta  : type II error rate\n",
    "#   d     : minimum detectable effect\n",
    "# Returns: minimum sample size per group\n",
    "def get_sampSize(sds, alpha, beta, d):\n",
    "    n = ((get_z_score(1 - alpha / 2) * sds[0] + get_z_score(1 - beta) * sds[1]) ** 2) / (d ** 2)\n",
    "    return n\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sample Size per Metric\n",
    "\n",
    "### Gross Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC[\"d\"] = 0.01\n",
    "R[\"d\"] = 0.01\n",
    "NC[\"d\"] = 0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sample size per group for Gross Conversion\n",
    "sds_gc = get_sds(GC[\"p\"], GC[\"d_min\"])\n",
    "GC[\"SampSize\"] = round(get_sampSize(sds_gc, alpha=0.05, beta=0.2, d=GC[\"d_min\"]))\n",
    "\n",
    "GC[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This means we need at least 25,835 cookies who click the Free Trial button - per group! \n",
    "- That means that if we got 400 clicks out of 5000 pageviews (400/5000 = 0.08) -> So, we are going to need `GC[\"SampSize\"]/0.08 = 322,938` pageviews, again ; per group! \n",
    "- Finally, the total amount of samples per the Gross Conversion metric is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust sample size to account for Click-Through Probability (CTP) and two groups\n",
    "# GC[\"SampSize\"] originally gives number of clicks needed per group\n",
    "# Divide by CTP to get the number of visitors needed\n",
    "# Multiply by 2 to account for both control and experiment groups\n",
    "GC[\"SampSize\"] = round(GC[\"SampSize\"] / baseline[\"CTP\"] * 2)\n",
    "\n",
    "GC[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sample size per group for Retention\n",
    "sds_r = get_sds(R[\"p\"], R[\"d_min\"])\n",
    "R[\"SampSize\"] = round(get_sampSize(sds_r, alpha=0.05, beta=0.2, d=R[\"d_min\"]))\n",
    "\n",
    "R[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we need 39,087 users who enrolled per group! We have to first convert this to cookies who clicked, and then to cookies who viewed the page, then finally to multipky by two for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust sample size to account for CTP, GC, and two groups\n",
    "# R[\"SampSize\"] is originally the number of enrollments per group\n",
    "# Divide by CTP and GC to get the number of visitors\n",
    "# Multiply by 2 for control + experiment groups\n",
    "R[\"SampSize\"] = round(R[\"SampSize\"] / (baseline[\"CTP\"] * baseline[\"GConversion\"]) * 2)\n",
    "\n",
    "R[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes us as high as over 4 million page views total, this is practically impossible because we know we get about 40,000 a day, this would take well over 100 days. This means we have to drop this metric and not continue to work with it because results from our experiment (which is much smaller) will be biased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sample size per group for Net Conversion\n",
    "sds_nc = get_sds(NC[\"p\"], NC[\"d_min\"])\n",
    "NC[\"SampSize\"] = round(get_sampSize(sds_nc, alpha=0.05, beta=0.2, d=NC[\"d_min\"]))\n",
    "\n",
    "NC[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, needing 27,413 cookies who click per group takes us all the way up to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust sample size to account for CTP and two groups\n",
    "# NC[\"SampSize\"] originally gives the number of clicks per group\n",
    "# Divide by CTP to get the number of visitors\n",
    "# Multiply by 2 to account for both control and experiment groups\n",
    "NC[\"SampSize\"] = round(NC[\"SampSize\"] / baseline[\"CTP\"] * 2)\n",
    "\n",
    "NC[\"SampSize\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all the way up to 685,325 cookies who view the page. This is more than what was needed for Gross Conversion, so this will be our number. Assuming we take 80% of each days pageviews, the data collection period for this experiment (the period in which the experiment is revealed) will be about 3 weeks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pandas to load datasets\n",
    "control = pd.read_csv(\"control_data.csv\")\n",
    "experiment = pd.read_csv(\"experiment_data.csv\")\n",
    "control.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "We have 3 Invariant metrics::\n",
    "\n",
    "- Number of Cookies in Course Overview Page\n",
    "- Number of Clicks on Free Trial Button\n",
    "- Free Trial button Click-Through-Probability (CTP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A significant difference will imply a biased experiment \n",
    "# that we should not rely on it's results.\n",
    "\n",
    "pageviews_cont = control['Pageviews'].sum()\n",
    "pageviews_exp = experiment['Pageviews'].sum()\n",
    "pageviews_total = pageviews_cont + pageviews_exp\n",
    "\n",
    "print (\"number of pageviews in control:\", pageviews_cont)\n",
    "print (\"number of Pageviewsin experiment:\" , pageviews_exp)\n",
    "print (\"number of Pageviewsin + pageviews experiment:\" , pageviews_total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, these numbers look pretty close. Now let's check that this difference is not significant and is random, as we expected. We can model this variation as follows:\n",
    "\n",
    "We expect the number of pageviews in the **control group** to be about half (50%) of the total pageviews in both groups. We can define a random variable to describe this.\n",
    "\n",
    "A **binomial random variable** represents the number of successes in N experiments, given the probability of a single success. If we treat being assigned to the control group as a \"success\" with probability 0.5 (random!), then the number of samples assigned to the control group is the value of this binomial variable.\n",
    "\n",
    "Thanks to the **Central Limit Theorem**, we can approximate the binomial distribution with a normal distribution (for large N), with:\n",
    "\n",
    "- **Mean:** $\\mu = p$\n",
    "- **Standard deviation:** $\\sigma = \\sqrt{\\frac{p(1-p)}{N}}$\n",
    "\n",
    "$$\n",
    "X \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{N}}\\right)\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to test is whether our observed $\\hat{p}$ (number of samples in control divided by total number of damples in both groups) is not significantly different than $p=0.5$. \n",
    "\n",
    "In order to do that we can calculate the margin of error acceptable at a 95% confidence level:\n",
    "\n",
    "$$\n",
    "ME = Z_{1-\\frac{\\alpha}{2}}SD\n",
    "$$\n",
    "with confidence interval\n",
    "$$\n",
    "CI = [\\hat{p}-ME,\\hat{p}+ME]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "p = 0.5                       # assumed population proportion under null\n",
    "alpha = 0.05                  # significance level (for 95% CI)\n",
    "\n",
    "# Compute sample proportion\n",
    "p_hat = round(pageviews_cont / pageviews_total, 4)\n",
    "\n",
    "# Compute standard deviation of the sampling distribution\n",
    "sd = mt.sqrt(p * (1 - p) / pageviews_total)\n",
    "\n",
    "# Compute margin of error\n",
    "ME = round(get_z_score(1 - alpha / 2) * sd, 4)\n",
    "\n",
    "# Display results\n",
    "lower = round(p - ME, 4)\n",
    "upper = round(p + ME, 4)\n",
    "\n",
    "print(f\"The confidence interval is between {lower} and {upper}; \"\n",
    "      f\"Is {p_hat} inside this range? {'Yes' if lower <= p_hat <= upper else 'No'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observed  $\\hat{p}$ is inside this range which means the difference in number of samples between groups is expected. So far so good, since this invariant metric sanity test passes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total clicks\n",
    "clicks_cont = control[\"Clicks\"].sum()\n",
    "clicks_exp = experiment[\"Clicks\"].sum()\n",
    "clicks_total = clicks_cont + clicks_exp\n",
    "\n",
    "# Compute observed proportion of clicks in control\n",
    "p_hat = round(clicks_cont / clicks_total, 4)\n",
    "\n",
    "# Expected proportion under H0 (equal allocation between groups)\n",
    "p = 0.5\n",
    "alpha = 0.05\n",
    "\n",
    "# Standard deviation for a proportion\n",
    "sd = mt.sqrt(p * (1 - p) / clicks_total)\n",
    "\n",
    "# Margin of error for 95% confidence interval\n",
    "ME = round(get_z_score(1 - (alpha / 2)) * sd, 4)\n",
    "\n",
    "# Confidence interval\n",
    "lower = round(p - ME, 4)\n",
    "upper = round(p + ME, 4)\n",
    "\n",
    "# Print results\n",
    "print(f\"The confidence interval is between {lower} and {upper}; \"\n",
    "      f\"Is {p_hat} inside this range? {'Yes' if lower <= p_hat <= upper else 'No'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have another pass! Great, so far it still seems all is well with our experiment results. Now, for the final metric which is a probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks for Differences Between Probabilities\n",
    "\n",
    "**Click-through Probability of the Free Trial Button**\n",
    "\n",
    "We want to ensure that the proportion of clicks given a pageview (our observed **CTP**) is about the same in both groups.  \n",
    "To check this, we calculate the **CTP** in each group and then compute a **confidence interval** for the expected difference between them.\n",
    "\n",
    "We expect to see no difference:\n",
    "\n",
    "$$\n",
    "CTP_{exp} - CTP_{cont} = 0\n",
    "$$\n",
    "\n",
    "with an acceptable margin of error.  \n",
    "The key adjustment here is in the calculation of the **standard error**, which uses a **pooled standard error**:\n",
    "\n",
    "$$\n",
    "SD_{pool} = \\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{N_{cont}}+\\frac{1}{N_{exp}}\\right)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{p}_{pool} = \\frac{x_{cont}+x_{exp}}{N_{cont}+N_{exp}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute click-through probabilities\n",
    "ctp_cont = clicks_cont / pageviews_cont\n",
    "ctp_exp = clicks_exp / pageviews_exp\n",
    "\n",
    "# Observed difference\n",
    "d_hat = round(ctp_exp - ctp_cont, 4)\n",
    "\n",
    "# Pooled click-through probability\n",
    "p_pooled = clicks_total / pageviews_total\n",
    "\n",
    "# Pooled standard deviation\n",
    "sd_pooled = mt.sqrt(p_pooled * (1 - p_pooled) *\n",
    "                    ((1 / pageviews_cont) + (1 / pageviews_exp)))\n",
    "\n",
    "# Margin of error for 95% confidence interval\n",
    "alpha = 0.05\n",
    "ME = round(get_z_score(1 - (alpha / 2)) * sd_pooled, 4)\n",
    "\n",
    "# Print results\n",
    "print(f\"The confidence interval is between {-ME:.4f} and {ME:.4f}; \"\n",
    "      f\"Is {d_hat:.4f} within this range? {'Yes' if -ME <= d_hat <= ME else 'No'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful. It seems this test has passed with flying colors as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Effect Size\n",
    "\n",
    "The next step is to examine the changes between the control and experiment groups with respect to our evaluation metrics.  \n",
    "We want to ensure that the observed difference:\n",
    "\n",
    "1. **Exists** — there is a measurable difference between groups.  \n",
    "2. **Is statistically significant** — the difference is unlikely due to random chance.  \n",
    "3. **Is practically significant** — the difference is large enough to make the experimental change beneficial to the company."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**  \n",
    "> A metric is **statistically significant** if the confidence interval does **not include 0**  \n",
    "> (meaning you can be confident there was a change).  \n",
    ">  \n",
    "> A metric is **practically significant** if the confidence interval does **not include the practical significance boundary**  \n",
    "> (meaning you can be confident the change is large enough to matter to the business)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total clicks from complete records only\n",
    "clicks_cont = control.loc[control[\"Enrollments\"].notnull(), \"Clicks\"].sum()\n",
    "clicks_exp = experiment.loc[experiment[\"Enrollments\"].notnull(), \"Clicks\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross Conversion - Enrollments divided by Clicks\n",
    "enrollments_cont = control[\"Enrollments\"].sum()\n",
    "enrollments_exp = experiment[\"Enrollments\"].sum()\n",
    "\n",
    "GC_cont = enrollments_cont / clicks_cont\n",
    "GC_exp = enrollments_exp / clicks_exp\n",
    "\n",
    "# Pooled Gross Conversion rate\n",
    "GC_pooled = (enrollments_cont + enrollments_exp) / (clicks_cont + clicks_exp)\n",
    "\n",
    "# Pooled standard deviation\n",
    "GC_sd_pooled = mt.sqrt(GC_pooled * (1 - GC_pooled) *\n",
    "                       ((1 / clicks_cont) + (1 / clicks_exp)))\n",
    "\n",
    "# Margin of error for 95% CI\n",
    "GC_ME = round(get_z_score(1 - alpha/2) * GC_sd_pooled, 4)\n",
    "\n",
    "# Observed difference\n",
    "GC_diff = round(GC_exp - GC_cont, 4)\n",
    "\n",
    "# Results\n",
    "print(f\"The change due to the experiment is {GC_diff*100:.2f}%\")\n",
    "print(f\"Confidence Interval: [{GC_diff - GC_ME:.4f}, {GC_diff + GC_ME:.4f}]\")\n",
    "print(\"The change is statistically significant if the CI doesn't include 0.\")\n",
    "print(f\"In that case, it is practically significant if {-GC['d_min']} is not in the CI as well.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this result, the experiment caused a change that was both **statistically** and **practically significant**.  \n",
    "\n",
    "We observed a **negative change of 2.06%**, while we were willing to accept any change greater than 1%. This means the **Gross Conversion rate** of the experiment group (those exposed to the change, i.e., asked how many hours they can devote to studying) **decreased by about 2%**.  \n",
    "\n",
    "In practical terms, this indicates that **fewer people enrolled in the Free Trial** after seeing the pop-up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Conversion - Payments divided by Clicks\n",
    "payments_cont = control[\"Payments\"].sum()\n",
    "payments_exp = experiment[\"Payments\"].sum()\n",
    "\n",
    "NC_cont = payments_cont / clicks_cont\n",
    "NC_exp = payments_exp / clicks_exp\n",
    "\n",
    "# Pooled Net Conversion rate\n",
    "NC_pooled = (payments_cont + payments_exp) / (clicks_cont + clicks_exp)\n",
    "\n",
    "# Pooled standard deviation\n",
    "NC_sd_pooled = mt.sqrt(NC_pooled * (1 - NC_pooled) *\n",
    "                       ((1 / clicks_cont) + (1 / clicks_exp)))\n",
    "\n",
    "# Margin of error for 95% CI\n",
    "NC_ME = round(get_z_score(1 - alpha/2) * NC_sd_pooled, 4)\n",
    "\n",
    "# Observed difference\n",
    "NC_diff = round(NC_exp - NC_cont, 4)\n",
    "\n",
    "# Results\n",
    "print(f\"The change due to the experiment is {NC_diff*100:.2f}%\")\n",
    "print(f\"Confidence Interval: [{NC_diff - NC_ME:.4f}, {NC_diff + NC_ME:.4f}]\")\n",
    "print(\"The change is statistically significant if the CI doesn't include 0.\")\n",
    "print(f\"In that case, it is practically significant if {NC['d_min']} is not in the CI as well.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we got a change size of less than a 0.5%, a very small decrease which is not statistically significant, and as such not practically significant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Check with Sign Tests\n",
    "\n",
    "A **sign test** provides another perspective on our results.  \n",
    "It checks whether the trend of change we observed (increase or decrease) is consistently evident in the **daily data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge control and experiment datasets side by side\n",
    "full = control.join(\n",
    "    other=experiment,\n",
    "    how=\"inner\",           # keep only rows present in both datasets\n",
    "    lsuffix=\"_cont\",       # suffix for control columns\n",
    "    rsuffix=\"_exp\"         # suffix for experiment columns\n",
    ")\n",
    "\n",
    "# Inspect the first few rows\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only complete records\n",
    "full = full.loc[full[\"Enrollments_cont\"].notnull()]\n",
    "\n",
    "# Count remaining rows per column\n",
    "full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross Conversion (GC) daily comparison\n",
    "x = full['Enrollments_cont'] / full['Clicks_cont']\n",
    "y = full['Enrollments_exp'] / full['Clicks_exp']\n",
    "full['GC'] = np.where(y > x, 1, 0)  # 1 if experiment GC > control GC\n",
    "\n",
    "# Net Conversion (NC) daily comparison\n",
    "z = full['Payments_cont'] / full['Clicks_cont']\n",
    "w = full['Payments_exp'] / full['Clicks_exp']\n",
    "full['NC'] = np.where(w > z, 1, 0)  # 1 if experiment NC > control NC\n",
    "\n",
    "# Inspect first few rows\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of days where experiment outperformed control\n",
    "GC_x = full.GC[full[\"GC\"] == 1].count()\n",
    "NC_x = full.NC[full[\"NC\"] == 1].count()\n",
    "\n",
    "# Total number of observations\n",
    "n = full.NC.count()\n",
    "\n",
    "# Print results\n",
    "print(f\"\"\"\n",
    "No. of cases where experiment GC > control GC: {GC_x}\n",
    "No. of cases where experiment NC > control NC: {NC_x}\n",
    "Total number of cases: {n}\n",
    "\"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Sign Test\n",
    "\n",
    "After counting the number of days in which the **experiment group** had a higher metric value than the **control group**, we want to determine if that number is likely to occur by **random chance** in a new experiment (i.e., test for significance).  \n",
    "\n",
    "We assume that the chance of a day like this is **50%**, and then use the **binomial distribution** with $p=0.5$ and the number of days $n$ to calculate the probability of observing this many “successful” days by random chance.\n",
    "\n",
    "According to the binomial distribution:\n",
    "\n",
    "$$\n",
    "p(\\text{successes}) = \\frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $n$ = total number of days  \n",
    "- $x$ = number of days with a higher metric in the experiment  \n",
    "- $p = 0.5$  \n",
    "\n",
    "Because we are doing a **two-tailed test**, we double this probability to get the **p-value**. We then compare the p-value to our significance level $\\alpha$:  \n",
    "- If $p > \\alpha$, the result is **not significant**  \n",
    "- If $p \\leq \\alpha$, the result is **significant**  \n",
    "\n",
    "**Recall:** A p-value is the probability of observing a test statistic as extreme or more extreme than the one observed.  \n",
    "\n",
    "For example, if we observe 2 days like that, the p-value is:\n",
    "\n",
    "$$\n",
    "p = P(x \\leq 2) = p(0) + p(1) + p(2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of exactly x successes out of n trials (p=0.5)\n",
    "def get_prob(x, n):\n",
    "    prob = mt.factorial(n) / (mt.factorial(x) * mt.factorial(n - x)) * 0.5**n\n",
    "    return round(prob, 4)\n",
    "\n",
    "# Two-sided p-value for observing x or fewer successes\n",
    "def get_2side_pvalue(x, n):\n",
    "    p = 0\n",
    "    for i in range(0, x + 1):\n",
    "        p += get_prob(i, n)\n",
    "    return 2 * p\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to conduct the **sign test** itself, we calculate the **p-value** for each metric using the counts `GC_x`, `NC_x`, and `n`, along with the function `get_2side_pvalue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistical significance for GC and NC\n",
    "print(\"GC Change is significant if\", get_2side_pvalue(GC_x, n), \"is smaller than 0.05\")\n",
    "print(\"NC Change is significant if\", get_2side_pvalue(NC_x, n), \"is smaller than 0.05\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same conclusions as we got from our effect size calculation: the change in Gross conversion was indeed significant, while the change in Net conversion was not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Recommendations\n",
    "\n",
    "At this point, having observed that the underlying goal—**increasing the fraction of paying users by asking them in advance if they have time to invest in the course**—was not achieved, our recommendation is to **not continue with this change**.  \n",
    "\n",
    "While the experiment may have caused a change in **Gross Conversion**, it did **not improve Net Conversion**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
