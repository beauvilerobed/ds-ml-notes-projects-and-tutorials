{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing: Conversion Rate Optimization\n",
    "\n",
    "In this notebook, we’ll go through the process of analyzing an A/B experiment: from formulating a hypothesis, testing it, to interpreting the results.\n",
    "\n",
    "**Scenario:** Imagine you work on the product team at a medium-sized online e-commerce business. The UX designer has created a new product page design with the hope of increasing the conversion rate. The product manager (PM) told you that the current conversion rate is around 13% on average throughout the year. The team considers the new design a success if it increases the conversion rate to 15% (a 2% increase).\n",
    "\n",
    "Before rolling out the change to all users, the team wants to test it on a small subset. You suggest running an A/B test on this subset.\n",
    "\n",
    "## Designing Our Experiment\n",
    "\n",
    "### Formulating a Hypothesis\n",
    "Since we do not know if the new design will perform better, worse, or the same as the current design, we will choose a **two-tailed test**:\n",
    "\n",
    "$$\n",
    "H_0: p = p_0\n",
    "$$\n",
    "$$\n",
    "H_1: p \\neq p_0\n",
    "$$\n",
    "\n",
    "We set a **confidence level** of 95%:\n",
    "\n",
    "$$\n",
    "\\alpha = 0.05\n",
    "$$\n",
    "\n",
    "The $\\alpha$ value is the threshold we use to determine significance: if the probability of observing a result as extreme or more (the p-value) is lower than $\\alpha$, we reject the null hypothesis.\n",
    "\n",
    "### Choosing the Variables\n",
    "We need two groups for the test:\n",
    "\n",
    "- **Control Group:** Shown the old design.\n",
    "- **Treatment (Experimental) Group:** Shown the new design.\n",
    "\n",
    "The **independent variable** is the design version (old vs. new). We include two groups to control for external factors like seasonality.\n",
    "\n",
    "The **dependent variable** is the conversion rate, which can be coded per user session as a binary variable:\n",
    "\n",
    "- 0 — User did not purchase during the session.\n",
    "- 1 — User purchased during the session.\n",
    "\n",
    "### Choosing a Sample Size\n",
    "Since we will not test the entire user base, the observed conversion rates are estimates of the true population rates.\n",
    "\n",
    "- **Larger sample sizes** → more precise estimates (smaller confidence intervals) → higher likelihood of detecting a difference if one exists.\n",
    "- **Smaller sample sizes** → cheaper and easier to implement but less precise.\n",
    "\n",
    "Sample size is estimated using **Power Analysis**, which depends on:\n",
    "\n",
    "- **Power of the test ($1 - \\beta$):** Probability of detecting a true difference if one exists (commonly set to 0.8).  \n",
    "- **Alpha ($\\alpha$):** Significance level (set at 0.05).  \n",
    "- **Effect size:** Expected difference between conversion rates (here, 2%, from 13% to 15%).\n",
    "\n",
    "We can use these values to calculate the minimum number of users needed in each group to detect a significant effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import statsmodels.stats.api as sms\n",
    "from math import ceil\n",
    "\n",
    "# Expected proportions in control and treatment groups\n",
    "p_control = 0.13\n",
    "p_treatment = 0.15\n",
    "\n",
    "# Calculate Cohen's h effect size for proportions\n",
    "effect_size = sms.proportion_effectsize(p_control, p_treatment)\n",
    "print(f\"Effect size (Cohen's h): {effect_size:.4f}\")\n",
    "\n",
    "# Calculate required sample size per group\n",
    "power_analysis = sms.NormalIndPower()\n",
    "required_n = power_analysis.solve_power(\n",
    "    effect_size=effect_size, \n",
    "    power=0.8,      # Desired statistical power\n",
    "    alpha=0.05,     # Significance level\n",
    "    ratio=1         # Equal sample sizes in both groups\n",
    ")\n",
    "\n",
    "# Round up to next whole number\n",
    "required_n = ceil(required_n)\n",
    "print(f\"Required sample size per group: {required_n}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’d need **at least 4720 observations for each group.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set the **power parameter** to 0.8 means that if there is an actual difference in conversion rates between our designs—assuming the difference is the one we estimated (13% vs. 15%)—we have approximately an 80% chance of detecting it as statistically significant in our test, given the sample size we calculated.\n",
    "\n",
    "## Collecting and Preparing the Data\n",
    "\n",
    "Since we’ll use a dataset sourced online to simulate this scenario, we will:\n",
    "\n",
    "1. Load the dataset.\n",
    "2. Read the data into a pandas DataFrame.\n",
    "3. Check and clean the data as needed.\n",
    "4. Randomly sample $n = 4{,}720$ rows from the DataFrame for each group.*\n",
    "\n",
    "**Note:** Normally, step 4 would not be necessary; it is included here purely for the sake of the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure all the control group are seeing the old page and viceversa\n",
    "pd.crosstab(df['group'], df['landing_page'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **294478 rows** in the DataFrame, each representing a user session, as well as **5 columns**:\n",
    "\n",
    "- user_id - The user ID of each session\n",
    "- timestamp - Timestamp for the session\n",
    "- group - Which group the user was assigned to for that session {control, treatment}\n",
    "- landing_page - Which design each user saw on that session {old_page, new_page}\n",
    "- converted - Whether the session ended in a conversion or not (binary, 0=not converted, 1=converted)\n",
    "\n",
    "Before we go ahead and sample the data to get our subset, let’s make sure there are no users that have been sampled multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many sessions each user has\n",
    "session_counts = df['user_id'].value_counts()\n",
    "\n",
    "# Count users with more than one session\n",
    "multi_users = (session_counts > 1).sum()\n",
    "\n",
    "print(f\"There are {multi_users} users that appear multiple times in the dataset.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are, in fact, 3894 users that appear more than once. Since the number is pretty low, we’ll go ahead and remove them from the DataFrame to avoid sampling the same users twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify users with more than one session\n",
    "users_to_drop = session_counts[session_counts > 1].index\n",
    "\n",
    "# Remove these users from the dataset\n",
    "df = df[~df['user_id'].isin(users_to_drop)]\n",
    "\n",
    "print(f\"The updated dataset now has {df.shape[0]} entries.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that our DataFrame is nice and clean, we can proceed and sample $n=4720$ entries for each of the groups. We can use pandas` DataFrame.sample()` method to do this, which will perform Simple Random Sampling for us.\n",
    "\n",
    "Note: We’ve set $\\text{random\\_state}=22$ so that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_sample = df[df['group'] == 'control'].sample(n=required_n, random_state=22)\n",
    "treatment_sample = df[df['group'] == 'treatment'].sample(n=required_n, random_state=22)\n",
    "\n",
    "ab_test = pd.concat([control_sample, treatment_sample], axis=0)\n",
    "ab_test.reset_index(drop=True, inplace=True)\n",
    "ab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test['group'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results\n",
    "\n",
    "The first thing we can do is to calculate some basic statistics to get an idea of what our samples look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rates = ab_test.groupby('group')['converted'].agg([\"mean\", 'std', 'sem'])\n",
    "conversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']\n",
    "conversion_rates.style.format('{:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the stats above, it does look like **our two designs performed very similarly**, with our new design performing slightly better, approx. **12.3% vs. 12.6% conversion rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data will make these results easier to grasp:\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x='group', y='converted', data=ab_test, errorbar=('ci', 95))\n",
    "plt.title('Conversion rate by group')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion rates for our groups are indeed very close. Also note that the conversion rate of the control group is lower than what we would have expected given what we knew about our avg. conversion rate (12.3% vs. 13%). This goes to show that there is some variation in results when sampling from a population.\n",
    "\n",
    "So… the treatment group's value is higher. **Is this difference statistically significant?**\n",
    "\n",
    "## Testing the hypothesis\n",
    "\n",
    "The last step of our analysis is testing our hypothesis. Since we have a very large sample, we can use the normal approximation for calculating our p-value (i.e. z-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "\n",
    "\n",
    "control_results = ab_test[ab_test['group'] == 'control']['converted']\n",
    "treatment_results = ab_test[ab_test['group'] == 'treatment']['converted']\n",
    "\n",
    "n_con = control_results.count()\n",
    "n_con_succ = control_results.sum()\n",
    "\n",
    "n_treat = treatment_results.count()\n",
    "n_treat_succ = treatment_results.sum()\n",
    "\n",
    "successes = [n_con_succ, n_treat_succ]\n",
    "nobs = [n_con, n_treat]\n",
    "\n",
    "z_stat, pval = proportions_ztest(successes, nobs)\n",
    "(lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)\n",
    "\n",
    "print(f'z statistic: {z_stat:.2f}')\n",
    "print(f'p-value: {pval:.3f}')\n",
    "print(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')\n",
    "print(f'ci 95% for treatment group: [{lower_treat:.3f}, {upper_treat:.3f}]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Conclusions\n",
    "\n",
    "Since our **p-value = 0.732** is well above our threshold of **α = 0.05**, we **cannot reject the Null hypothesis $H_0$**. This means that the new design did **not perform significantly differently** (let alone better) than the old design.\n",
    "\n",
    "Additionally, if we look at the **confidence interval** for the treatment group ([0.116, 0.135] or 11.6%–13.5%), we notice that:\n",
    "\n",
    "- It **includes** our baseline conversion rate of 13%.\n",
    "- It **does not include** our target value of 15% (the 2% uplift we were hoping for).\n",
    "\n",
    "This indicates that the true conversion rate of the new design is more likely to be **similar to the baseline** rather than the 15% target. In other words, the new design is **unlikely to be an improvement** over the old one, and unfortunately, we are **back to the drawing board**!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
