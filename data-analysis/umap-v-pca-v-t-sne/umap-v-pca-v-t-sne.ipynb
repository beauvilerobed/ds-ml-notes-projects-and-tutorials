{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537d3c36",
   "metadata": {},
   "source": [
    "# Comparative Analysis of PCA, t-SNE, and UMAP\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dimensionality reduction is a crucial technique in data science and machine learning. This document compares three widely used algorithms:\n",
    "\n",
    "* **PCA**: Principal Component Analysis\n",
    "* **t-SNE**: t-distributed Stochastic Neighbor Embedding\n",
    "* **UMAP**: Uniform Manifold Approximation and Projection\n",
    "\n",
    "We will explore their **mathematical foundations**, **visual outputs**, and **pros and cons**.\n",
    "\n",
    "## 1. Principal Component Analysis (PCA)\n",
    "\n",
    "### Description\n",
    "\n",
    "PCA is a linear dimensionality reduction method that projects data onto orthogonal components capturing the most variance.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{n \\times d}$ be the data matrix.\n",
    "\n",
    "1. Center the data:\n",
    "\n",
    "   $$\n",
    "   \\tilde{X} = X - \\mu, \\quad \\text{where } \\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "   $$\n",
    "\n",
    "2. Compute the covariance matrix:\n",
    "\n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{n} \\tilde{X}^T \\tilde{X}\n",
    "   $$\n",
    "\n",
    "3. Perform eigendecomposition:\n",
    "\n",
    "   $$\n",
    "   \\Sigma v_i = \\lambda_i v_i\n",
    "   $$\n",
    "\n",
    "   Retain the top $k$ eigenvectors $v_1, \\ldots, v_k$.\n",
    "\n",
    "4. Project:\n",
    "\n",
    "   $$\n",
    "   X_{\\text{PCA}} = \\tilde{X} V_k\n",
    "   $$\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros                       | Cons                         |\n",
    "| -------------------------- | ---------------------------- |\n",
    "| Fast, interpretable        | Linear only                  |\n",
    "| Preserves global structure | Sensitive to feature scaling |\n",
    "\n",
    "## 2. t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "### Description\n",
    "\n",
    "t-SNE focuses on preserving **local structure** using a probabilistic model.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Given pairwise distances $d_{ij}$ in high-dimensional space:\n",
    "\n",
    "1. Compute conditional probabilities:\n",
    "\n",
    "   $$\n",
    "   p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n",
    "   $$\n",
    "\n",
    "   Symmetrize:\n",
    "\n",
    "   $$\n",
    "   p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "   $$\n",
    "\n",
    "2. Define low-dimensional similarities using Student t-distribution:\n",
    "\n",
    "   $$\n",
    "   q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "   $$\n",
    "\n",
    "3. Minimize the Kullback–Leibler divergence:\n",
    "\n",
    "   $$\n",
    "   KL(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "   $$\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros                         | Cons                                  |\n",
    "| ---------------------------- | ------------------------------------- |\n",
    "| Excellent cluster separation | Poor global structure                 |\n",
    "| Captures local neighborhoods | Slow on large datasets                |\n",
    "| Visual appeal                | Non-parametric (no inverse transform) |\n",
    "\n",
    "## 3. Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "### Description\n",
    "\n",
    "UMAP is a **non-linear dimensionality reduction** technique based on **manifold learning** and **topology**.  \n",
    "It constructs a **topological representation of the high-dimensional data** and then optimizes a low-dimensional embedding that preserves both local and global structures.\n",
    "\n",
    "UMAP is particularly useful for visualizing high-dimensional data while preserving neighborhood relationships, and it is often faster and more scalable than t-SNE.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "UMAP can be rigorously understood in terms of **fuzzy topological representations** and **graph theory**. The method consists of three main steps:\n",
    "\n",
    "#### Step 1: Construct the High-Dimensional Graph\n",
    "\n",
    "1. For each data point $x_i \\in \\mathbb{R}^d$, compute its $k$ nearest neighbors in the dataset using a distance metric $d(x_i, x_j)$ (often Euclidean).  \n",
    "\n",
    "2. Define a **local connectivity distance** $\\rho_i$ for each point, typically the distance to its closest neighbor, to ensure that every point has at least one neighbor within a minimum distance:\n",
    "\n",
    "   $$\n",
    "   \\rho_i = \\min_{j} d(x_i, x_j)\n",
    "   $$\n",
    "\n",
    "3. Define a **local scaling parameter** $\\sigma_i$ to control the smoothness of the neighborhood distribution for each point. This is usually determined numerically to satisfy a target entropy:\n",
    "\n",
    "   $$\n",
    "   \\sum_{j} \\exp\\left(-\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i}\\right) = \\text{constant}\n",
    "   $$\n",
    "\n",
    "4. Construct the **high-dimensional edge weights** using a **fuzzy membership function**:\n",
    "\n",
    "   $$\n",
    "   p_{ij} = \\exp\\left( -\\frac{\\max(0, d(x_i, x_j) - \\rho_i)}{\\sigma_i} \\right)\n",
    "   $$\n",
    "\n",
    "   - $p_{ij} \\in [0, 1]$ represents the **fuzzy probability** that points $x_i$ and $x_j$ are connected in the manifold.  \n",
    "   - This yields a **weighted k-NN graph**, which encodes the local structure of the data.\n",
    "\n",
    "5. Symmetrize the graph to ensure undirected edges:\n",
    "\n",
    "   $$\n",
    "   p_{ij}^{\\text{sym}} = p_{ij} + p_{ji} - p_{ij} p_{ji}\n",
    "   $$\n",
    "\n",
    "#### Step 2: Construct a Fuzzy Simplicial Set in Low Dimensions\n",
    "\n",
    "1. Let $y_i \\in \\mathbb{R}^r$ denote the low-dimensional embedding of $x_i$.  \n",
    "\n",
    "2. Define a **low-dimensional fuzzy similarity** $q_{ij}$ using a differentiable function that decays with distance (often a heavy-tailed distribution to allow separation):\n",
    "\n",
    "   $$\n",
    "   q_{ij} = \\frac{1}{1 + a \\|y_i - y_j\\|^{2b}}\n",
    "   $$\n",
    "\n",
    "   - $a, b > 0$ are parameters fit to approximate a smooth decay similar to the high-dimensional space.  \n",
    "   - $q_{ij}$ is also interpreted as the probability that $y_i$ and $y_j$ are connected in the low-dimensional embedding.\n",
    "\n",
    "#### Step 3: Optimize the Low-Dimensional Embedding\n",
    "\n",
    "UMAP minimizes a **cross-entropy loss** between the high-dimensional and low-dimensional fuzzy simplicial sets:\n",
    "\n",
    "$$\n",
    "C = \\sum_{i \\neq j} \\Big[ p_{ij}^{\\text{sym}} \\log \\frac{p_{ij}^{\\text{sym}}}{q_{ij}} + (1 - p_{ij}^{\\text{sym}}) \\log \\frac{1 - p_{ij}^{\\text{sym}}}{1 - q_{ij}} \\Big]\n",
    "$$\n",
    "\n",
    "- This encourages **edges present in high dimensions** (large $p_{ij}$) to be close in the low-dimensional embedding ($q_{ij}$ large) and **non-edges** to be far apart.  \n",
    "- Gradient descent (usually stochastic) is used to optimize $y_i$ for all $i$.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. UMAP preserves **local neighborhood structure** while retaining some **global relationships**.  \n",
    "2. Faster and more scalable than t-SNE for large datasets.  \n",
    "3. Embeddings are relatively consistent across runs due to graph construction.  \n",
    "4. Can be applied to **new points** via the learned transformation (unlike t-SNE).\n",
    "\n",
    "### Summary\n",
    "\n",
    "UMAP is rigorously grounded in **manifold learning and topological data analysis**.  \n",
    "- High-dimensional data is represented as a **fuzzy simplicial set**.  \n",
    "- Low-dimensional embeddings are optimized to **minimize cross-entropy** with the high-dimensional structure.  \n",
    "- The result is a compact, informative low-dimensional representation that preserves neighborhood relations and global structure.\n",
    "\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros                        | Cons                         |\n",
    "| --------------------------- | ---------------------------- |\n",
    "| Fast and scalable           | Sensitive to hyperparameters |\n",
    "| Retains global + local info | Can distort distances        |\n",
    "| Can transform new points    | More complex implementation  |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "| Method | Local Structure | Global Structure | Transform New Data | Speed | Use Cases                    |\n",
    "| ------ | --------------- | ---------------- | ------------------ | ----- | ---------------------------- |\n",
    "| PCA    | No              | Yes              | Yes                | Yes   | Preprocessing                |\n",
    "| t-SNE  | Yes             | No               | No                 | No    | Visualization                |\n",
    "| UMAP   | Yes             | Partial          | Yes                | Yes   | Visualization, preprocessing |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111535f",
   "metadata": {},
   "source": [
    "# Example of Dimensionality Reduction\n",
    "\n",
    "**Objective:**  \n",
    "Visualize high-dimensional data (64 features) in 2D using three techniques:\n",
    "\n",
    "* **PCA (Principal Component Analysis)**\n",
    "* **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "* **UMAP (Uniform Manifold Approximation and Projection)**\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "* `load_digits()` from `sklearn.datasets`\n",
    "* Contains 1,797 images of handwritten digits (0–9), each an 8x8 image (64-dimensional)\n",
    "\n",
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "**Method:**\n",
    "\n",
    "* Linear transformation\n",
    "* Projects data onto directions (principal components) that maximize variance\n",
    "\n",
    "**Output Characteristics:**\n",
    "\n",
    "* Fast and deterministic\n",
    "* Captures global structure\n",
    "* Often useful as a preprocessing step\n",
    "\n",
    "**Visualization Insight:**\n",
    "\n",
    "* Clear clusters, but overlap exists\n",
    "* Not ideal for preserving local relationships\n",
    "\n",
    "### t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "**Method:**\n",
    "\n",
    "* Non-linear technique\n",
    "* Focuses on preserving **local similarity**\n",
    "* Computes probabilities of similarity and minimizes divergence between high- and low-dimensional spaces\n",
    "\n",
    "**Output Characteristics:**\n",
    "\n",
    "* More separation between digit clusters\n",
    "* Computationally intensive\n",
    "* Results can vary between runs (stochastic)\n",
    "\n",
    "**Visualization Insight:**\n",
    "\n",
    "* Better at separating digit classes than PCA\n",
    "* More expressive for local neighborhood structures\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "**Method:**\n",
    "\n",
    "* Non-linear dimensionality reduction\n",
    "* Preserves both local and some global structures\n",
    "* Based on manifold learning and nearest-neighbor graphs\n",
    "\n",
    "**Output Characteristics:**\n",
    "\n",
    "* Faster than t-SNE\n",
    "* More consistent than t-SNE across runs\n",
    "* Preserves more of the data's structure\n",
    "\n",
    "**Visualization Insight:**\n",
    "\n",
    "* Strong separation between digit classes\n",
    "* Compact clusters with meaningful relationships\n",
    "\n",
    "### Combined Visualization\n",
    "\n",
    "**Graph Summary:**\n",
    "\n",
    "* **Left:** PCA – captures global structure, some overlaps  \n",
    "* **Middle:** t-SNE – excellent local clustering  \n",
    "* **Right:** UMAP – good balance of local/global structure, fast and accurate  \n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "* **PCA:** Good first step, linear  \n",
    "* **t-SNE:** Great local structure, slower  \n",
    "* **UMAP:** Best of both worlds – speed and clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Load dataset: 8x8 images of digits (64-dimensional)\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# --- Dimensionality Reduction ---\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# UMAP\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "embeddings = [X_pca, X_tsne, X_umap]\n",
    "titles = ['PCA', 't-SNE', 'UMAP']\n",
    "\n",
    "for i, (X_embedded, title) in enumerate(zip(embeddings, titles), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.scatterplot(\n",
    "        x=X_embedded[:, 0],\n",
    "        y=X_embedded[:, 1],\n",
    "        hue=y,\n",
    "        palette='tab10',\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        legend='full'  # show legend for classes\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de232e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
