{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle components analysis (PCA) is a standard way to reduce the dimension \n",
    " (which can be quite large) to something more manageable, given a $n\\times p$ matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA tries to find “components” that capture the maximal variance within the data. \n",
    "\n",
    "For three dimensional data, this is the basic image you may have come across:\n",
    "\n",
    "<img src=img/pca_classic.png width=\"30%\" height=\"30%\">\n",
    "\n",
    "## PCA in a nutshell\n",
    "\n",
    "Each blue point represents one observation (a row of **X**). There are $n = 20$ observations, each with $p = 3$ features. PCA reduces the data from three dimensions to $r = 2$ by finding two new, perpendicular directions (red arrows) that capture as much of the data’s variance as possible. These directions define a two-dimensional plane (grey) that best represents the original 3D data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical formulation of PCA\n",
    "\n",
    "We now formalize the geometric intuition of PCA.  \n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ denote the data matrix, where each row corresponds to an observation and each column to a feature. Assume that each column of $\\mathbf{X}$ has been mean-centered, so that the data are centered around the origin:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n X_{ij} = 0, \\quad \\forall j = 1, \\dots, p.\n",
    "$$\n",
    "\n",
    "The goal of PCA is to find a unit vector $c \\in \\mathbb{R}^p$ that defines a direction in feature space along which the projected data have maximal variance. Formally, this corresponds to the optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{c} \\; c^\\top \\mathbf{X}^\\top \\mathbf{X} c\n",
    "$$\n",
    "\n",
    "subject to the unit-norm constraint\n",
    "\n",
    "$$\n",
    "c^\\top c = 1.\n",
    "$$\n",
    "\n",
    "Let $w = \\mathbf{X}c$ denote the projection of the data onto this direction. Since $c^\\top c = 1$ and the data are mean-centered, the variance of the projected data is\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(w) = \\frac{1}{n} w^\\top w = \\frac{1}{n} c^\\top \\mathbf{X}^\\top \\mathbf{X} c.\n",
    "$$\n",
    "\n",
    "Thus, maximizing the variance of $w$ is equivalent to maximizing $c^\\top \\mathbf{X}^\\top \\mathbf{X} c$.  \n",
    "The optimal $c$ is therefore the eigenvector of $\\mathbf{X}^\\top \\mathbf{X}$ corresponding to its largest eigenvalue.\n",
    "\n",
    "### Proof that the optimal $c$ is the eigenvector of $\\mathbf{X}^\\top \\mathbf{X}$ with the largest eigenvalue\n",
    "\n",
    "We start from the PCA optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{c} \\; c^\\top \\mathbf{X}^\\top \\mathbf{X} c\n",
    "$$\n",
    "\n",
    "subject to the constraint\n",
    "\n",
    "$$\n",
    "c^\\top c = 1.\n",
    "$$\n",
    "\n",
    "#### Step 1: Form the Lagrangian\n",
    "\n",
    "To handle the constraint, we introduce a Lagrange multiplier $\\lambda$ and define the Lagrangian:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(c, \\lambda) = c^\\top \\mathbf{X}^\\top \\mathbf{X} c - \\lambda (c^\\top c - 1).\n",
    "$$\n",
    "\n",
    "#### Step 2: Take the derivative with respect to $c$\n",
    "\n",
    "Setting the gradient of $\\mathcal{L}$ with respect to $c$ to zero gives the first-order condition for optimality:\n",
    "\n",
    "$$\n",
    "\\nabla_c \\mathcal{L}(c, \\lambda) = 2 \\mathbf{X}^\\top \\mathbf{X} c - 2 \\lambda c = 0.\n",
    "$$\n",
    "\n",
    "Simplifying, we obtain\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} c = \\lambda c.\n",
    "$$\n",
    "\n",
    "This is the eigenvalue equation for $\\mathbf{X}^\\top \\mathbf{X}$, showing that any optimal solution $c$ must be one of its eigenvectors.\n",
    "\n",
    "#### Step 3: Determine which eigenvector maximizes the objective\n",
    "\n",
    "Substituting $\\mathbf{X}^\\top \\mathbf{X} c = \\lambda c$ into the objective function yields:\n",
    "\n",
    "$$\n",
    "c^\\top \\mathbf{X}^\\top \\mathbf{X} c = c^\\top (\\lambda c) = \\lambda (c^\\top c) = \\lambda.\n",
    "$$\n",
    "\n",
    "Since $c^\\top c = 1$, the value of the objective function equals the corresponding eigenvalue $\\lambda$.  \n",
    "Therefore, to maximize the variance, we must select the eigenvector associated with the largest eigenvalue $\\lambda_{\\max}$.\n",
    "\n",
    "#### Step 4: Interpretation\n",
    "\n",
    "- The eigenvector $c_1$ corresponding to $\\lambda_{\\max}$ defines the **first principal component direction** — the axis along which the data variance is maximal.  \n",
    "- The projected data (component scores) are given by $w = \\mathbf{X} c_1$.  \n",
    "- Subsequent principal components correspond to the remaining eigenvectors, ordered by decreasing eigenvalues, and are mutually orthogonal.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several equivalent ways to solve the PCA optimization problem and obtain $c$ and $w$.  \n",
    "The classical approach is to compute the **eigendecomposition** of the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{S} = \\mathbf{X}^\\top \\mathbf{X},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{S} \\in \\mathbb{R}^{p \\times p}$.  \n",
    "We then solve the eigenvalue problem\n",
    "\n",
    "$$\n",
    "\\mathbf{S} c = \\lambda c,\n",
    "$$\n",
    "\n",
    "and select $c$ as the eigenvector corresponding to the largest eigenvalue $\\lambda_{\\max}$.  \n",
    "This eigenvector defines the direction of maximal variance, and the associated eigenvalue represents the amount of variance captured along that direction.  \n",
    "\n",
    "In practice, the eigendecomposition of $\\mathbf{S}$ is often computed via the **singular value decomposition (SVD)** of $\\mathbf{X}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top,\n",
    "$$\n",
    "\n",
    "where the columns of $\\mathbf{V}$ are the eigenvectors of $\\mathbf{X}^\\top \\mathbf{X}$, and the squared singular values in $\\mathbf{\\Sigma}^2$ correspond to the eigenvalues.  \n",
    "\n",
    "However, this matrix-based formulation assumes that $\\mathbf{X}$ is a fully observed two-dimensional array.  \n",
    "In more complex scenarios—such as when the data are represented as **tensors** (multi-way arrays), contain **missing entries**, or possess additional structural constraints—this classical eigenvalue/SVD approach no longer applies directly.  \n",
    "In those cases, generalized formulations of PCA (e.g., tensor PCA, probabilistic PCA, or low-rank matrix completion) are required to handle the additional complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimization problem has been solved—by eigendecomposition, SVD, or another suitable method—we obtain the vectors $c$ (the principal component direction) and $w = \\mathbf{X}c$ (the corresponding component scores).  \n",
    "The best **rank-one approximation** of the data matrix can then be written as the outer product of these two vectors:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\approx w c^\\top.\n",
    "$$\n",
    "\n",
    "This approximation minimizes the reconstruction error (in the least-squares sense) among all rank-one matrices.  \n",
    "The matrix $w c^\\top$ has rank one because it is formed by the outer product of two vectors.  \n",
    "Geometrically, this reconstruction represents the projection of the data onto the one-dimensional subspace spanned by $c$.\n",
    "\n",
    "<img src=\"img/rank_one.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "**Example: Rank-one reconstruction using the first principal component.**  \n",
    "An example data matrix (left) with $n = 12$ observations and $p = 8$ features is approximated by the outer product $w c^\\top$ (middle), producing a rank-one matrix (right).  \n",
    "Here, $w$ corresponds to the **component scores** (or loadings for each observation), and $c^\\top$ represents the **principal component direction** in feature space.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data can’t be well-described by a single principal component. Typically, we compute multiple principal components by computing all eigenvectors of $\\textbf{X}^T\\textbf{X}$ and ranking them by their eigenvalues. This can be visualized by a scree plot, which plots the variance explained by each successive principal component. People may have told you to look for the “knee” or inflection point in the scree plot to determine the number of components to keep (the rest are noise).\n",
    "\n",
    "<img src=img/scree.png width=\"40%\" height=\"40%\">\n",
    "\n",
    "**Scree plot**. Principal components are ranked by the amount of variance they capture in the original dataset, a scree plot can provide some sense of how many components are needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can organize the top r principal components into a matrix $C = [c_1,c_2,...,c_r]$ and the loading weights into $W = [w_1,w_2,...,w_r]$. Our reconstruction of the data is now a sum of r outer products:\n",
    "\n",
    "$$\n",
    "\\textbf{X} \\approx \\sum_{k=1}^r w_kc_k^T \\text{ or } X\\approx WC^T\n",
    "$$\n",
    "\n",
    "<img src=img/pca_3.png width=\"50%\" height=\"50%\">\n",
    "\n",
    "**Example reconstruction of data with 3 principal components.** A data matrix (left) is approximated by the product of a $n\\times r$ matrix and a $r\\times p$ matrix (i.e. $WC^T$). This product is at most a rank-r matrix (in this example, $r=3$). Each paired column of W and row of $C^T$ form an outer product, so the full reconstruction can also be thought of as a sum of r rank-one matrices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implimentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of PCA\n",
    "\n",
    "- **Identify relationships between variables:** PCA reveals correlations and dependencies between features in the data.  \n",
    "- **Data interpretation and visualization:** By reducing dimensionality, PCA helps visualize complex datasets in 2D or 3D spaces.  \n",
    "- **Dimensionality reduction:** Reducing the number of variables simplifies further analysis and computation.  \n",
    "- **Genetic and population studies:** PCA is widely used to visualize genetic distances and relatedness among populations.\n",
    "\n",
    "### Objectives of PCA\n",
    "\n",
    "- **Dimension reduction:** PCA transforms a large number of variables into a smaller set of uncorrelated principal components, capturing most of the variance.  \n",
    "- **Pattern identification:** PCA uncovers hidden structures or relationships in the data that may not be apparent in the original feature space.  \n",
    "- **Feature extraction:** PCA creates new features (principal components) that are often more informative than the original variables.  \n",
    "- **Data compression:** By keeping only the top principal components, PCA reduces data size while preserving as much information as possible.  \n",
    "- **Noise reduction:** Components with small variance often correspond to noise; removing them improves signal quality.  \n",
    "- **Visualization of high-dimensional data:** PCA enables projection of high-dimensional datasets onto lower dimensions for easier interpretation and insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Import the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing or loading the dataset\n",
    "dataset = pd.read_csv('wine.csv')\n",
    " \n",
    "# distributing the dataset into two components X and Y\n",
    "X = dataset.iloc[:, 0:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the X and Y into the\n",
    "# Training set and Testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing preprocessing part\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    " \n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA function on training\n",
    "# and testing set of X component\n",
    "from sklearn.decomposition import PCA\n",
    " \n",
    "pca = PCA(n_components = 2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    " \n",
    "explained_variance = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(explained_variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance in PCA\n",
    "\n",
    "The **explained variance** quantifies how much of the total information (variance) in the dataset is captured by each principal component. This is important because reducing the dimensionality of the data inevitably discards some variance.  \n",
    "\n",
    "For example, if we reduce a four-dimensional dataset to two principal components, we lose a portion of the total variance. The **explained variance ratio** is typically denoted as:\n",
    "\n",
    "$$\n",
    "\\text{explained\\_variance\\_ratio\\_} = \\frac{\\text{variance of a principal component}}{\\text{total variance of the data}}.\n",
    "$$\n",
    "\n",
    "Using this measure, we can interpret the contribution of each component. For instance:  \n",
    "\n",
    "- The **first principal component** may capture $36.88\\%$ of the total variance.  \n",
    "- The **second principal component** may capture $19.32\\%$ of the total variance.  \n",
    "\n",
    "Together, the first two components account for $36.88\\% + 19.32\\% = 56.2\\%$ of the total variance.  \n",
    "\n",
    "Thus, by examining the explained variance ratio, we can decide how many components are needed to retain a sufficient amount of information while reducing dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression To the training set\n",
    "from sklearn.linear_model import LogisticRegression \n",
    " \n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set result using\n",
    "# predict function under LogisticRegression\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making confusion matrix between\n",
    "#  test set of Y and predicted value.\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set data\n",
    "X_set, y_set = X_train, y_train\n",
    "\n",
    "# Create grid\n",
    "X1, X2 = np.meshgrid(\n",
    "    np.arange(X_set[:, 0].min() - 1, X_set[:, 0].max() + 1, 0.01),\n",
    "    np.arange(X_set[:, 1].min() - 1, X_set[:, 1].max() + 1, 0.01)\n",
    ")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(\n",
    "    X1, X2,\n",
    "    classifier.predict(np.c_[X1.ravel(), X2.ravel()]).reshape(X1.shape),\n",
    "    alpha=0.75,\n",
    "    cmap=ListedColormap(('yellow', 'white', 'aquamarine'))\n",
    ")\n",
    "\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "# Scatter plot for each class\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, label in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(\n",
    "        X_set[y_set == label, 0],\n",
    "        X_set[y_set == label, 1],\n",
    "        color=colors[i],\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Logistic Regression (Training set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Set data\n",
    "X_set, y_set = X_test, y_test\n",
    "\n",
    "# Create grid\n",
    "X1, X2 = np.meshgrid(\n",
    "    np.arange(X_set[:, 0].min() - 1, X_set[:, 0].max() + 1, 0.01),\n",
    "    np.arange(X_set[:, 1].min() - 1, X_set[:, 1].max() + 1, 0.01)\n",
    ")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.contourf(\n",
    "    X1, X2,\n",
    "    classifier.predict(np.c_[X1.ravel(), X2.ravel()]).reshape(X1.shape),\n",
    "    alpha=0.75,\n",
    "    cmap=ListedColormap(('yellow', 'white', 'aquamarine'))\n",
    ")\n",
    "\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "\n",
    "# Scatter plot for each class\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, label in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(\n",
    "        X_set[y_set == label, 0],\n",
    "        X_set[y_set == label, 1],\n",
    "        color=colors[i],  # <--- use 'color' instead of 'c'\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Logistic Regression (Test set)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example of how to perform PCA using Python. The output of this code will be a scatter plot of the first two principal components and their explained variance ratio. By selecting the appropriate number of principal components, we can reduce the dimensionality of the dataset and improve our understanding of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
