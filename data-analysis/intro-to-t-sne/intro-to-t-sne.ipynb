{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to t-SNE\n",
    "\n",
    "### Goal\n",
    "\n",
    "Learn to visualize **high-dimensional data** in a **low-dimensional space** using a **nonlinear dimensionality reduction** technique.\n",
    "\n",
    "## What is t-SNE?\n",
    "\n",
    "**t-SNE (t-distributed Stochastic Neighbor Embedding)** is an **unsupervised**, **nonlinear** dimensionality reduction algorithm for **data visualization**.\n",
    "It maps high-dimensional data to **2D or 3D space**, revealing clusters and structures that are not linearly separable.\n",
    "\n",
    "Unlike linear methods such as PCA, t-SNE can capture **nonlinear relationships** and preserve **local similarities** in the data.\n",
    "\n",
    "## How t-SNE Works\n",
    "\n",
    "The algorithm measures similarity between pairs of data points in both **high** and **low** dimensions, then optimizes the low-dimensional map so these similarities match as closely as possible.\n",
    "\n",
    "### Step 1: High-Dimensional Similarities\n",
    "\n",
    "For data points $x_i$ and $x_j$, define the conditional probability that $x_j$ is a neighbor of $x_i$:\n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp\\left(-\\frac{|x_i - x_j|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{|x_i - x_k|^2}{2\\sigma_i^2}\\right)}\n",
    "$$\n",
    "\n",
    "where $\\sigma_i$ is chosen so that the **perplexity** of the distribution equals a user-defined constant:\n",
    "\n",
    "$$\n",
    "Perp(P_i) = 2^{H(P_i)} \\quad \\text{where} \\quad H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i}\n",
    "$$\n",
    "\n",
    "The joint probability is then symmetrized:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "$$\n",
    "\n",
    "### Step 2: Low-Dimensional Similarities\n",
    "\n",
    "For mapped points $y_i$ and $y_j$ in low-dimensional space:\n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + |y_i - y_j|^2)^{-1}}{\\sum_{k \\neq l} (1 + |y_k - y_l|^2)^{-1}}\n",
    "$$\n",
    "\n",
    "This uses a **Student t-distribution** with one degree of freedom (heavy-tailed), preventing points from crowding together.\n",
    "\n",
    "### Step 3: Minimize the Divergence\n",
    "\n",
    "t-SNE minimizes the **Kullback–Leibler divergence** between the two distributions:\n",
    "\n",
    "$$\n",
    "C = KL(P | Q) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "$$\n",
    "\n",
    "The gradient with respect to each point $y_i$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + |y_i - y_j|^2)^{-1}\n",
    "$$\n",
    "\n",
    "## Algorithm 1: Simple Version of t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "**Data:**\n",
    "Data set $X = {x_1, x_2, \\ldots, x_n}$\n",
    "\n",
    "**Cost function parameters:**\n",
    "Perplexity $Perp$\n",
    "\n",
    "**Optimization parameters:**\n",
    "Number of iterations $T$, learning rate $\\eta$, momentum $\\alpha(t)$\n",
    "\n",
    "**Result:**\n",
    "Low-dimensional data representation $Y^{(T)} = {y_1, y_2, \\ldots, y_n}$\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "Algorithm 1: Simple version of t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "begin\n",
    "\n",
    "1. Compute pairwise affinities $p_{j|i}$ with perplexity Perp (using Equation 1)\n",
    "       $$\n",
    "        p_{j|i} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)}\n",
    "       $$\n",
    "       \n",
    "    - Ensure that the perplexity satisfies $Perp(P_i) = 2^{H(P_i)} $ where $H(P_i) = -\\sum_j p_{j|i} \\log_2 p_{j|i} $    \n",
    "2. Symmetrize affinities:\n",
    "       $$\n",
    "        p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n",
    "       $$\n",
    "    \n",
    "3. Sample initial solution:\n",
    "       $$\n",
    "        Y^{(0)} = \\{y_1, y_2, \\ldots, y_n\\} \\sim \\mathcal{N}(0, 10^{-4}I)\n",
    "       $$\n",
    "    \n",
    "4. For $t = 1$ to $T$ do\n",
    "\n",
    "    a. Compute low-dimensional affinities $q_{ij}$ (using Equation 4)\n",
    "           $$\n",
    "            q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n",
    "           $$\n",
    "        \n",
    "    b. Compute gradient (using Equation 5)\n",
    "           $$\n",
    "            \\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}\n",
    "           $$\n",
    "        \n",
    "    c. Update the low-dimensional map:\n",
    "           $$\n",
    "            Y^{(t)} = Y^{(t-1)} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha(t) (Y^{(t-1)} - Y^{(t-2)})\n",
    "           $$\n",
    "    \n",
    "    end for\n",
    "\n",
    "end\n",
    "\n",
    "### Explanation of Terms\n",
    "\n",
    "| Symbol      | Meaning                                             |\n",
    "| ----------- | --------------------------------------------------- |\n",
    "| $X$         | Original high-dimensional data                      |\n",
    "| $Y$         | Low-dimensional embedding                           |\n",
    "| $p_{ij}$    | Similarity between points in high-dimensional space |\n",
    "| $q_{ij}$    | Similarity between points in low-dimensional space  |\n",
    "| $C$         | Kullback–Leibler divergence cost function           |\n",
    "| $\\eta$      | Learning rate                                       |\n",
    "| $\\alpha(t)$ | Momentum term at iteration $t$                      |\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Step | Formula | Purpose |  \n",
    "| ---- | ------- | ------|\n",
    "| 1    | $p_{j \\| i} = \\frac{e^{-\\|x_i - x_j\\|^2 / 2\\sigma_i^2}}{\\sum_{k \\neq i} e^{-\\|x_i - x_k\\|^2 / 2\\sigma_i^2}}$ | High-D similarity |\n",
    "| 2    | $q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}$ | Low-D similarity                                                                                |                   |\n",
    "| 3    | $C = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$                                             | Cost (KL divergence)                                                                            |                   |\n",
    "| 4    | $\\frac{\\partial C}{\\partial y_i} = 4 \\sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + \\|y_i - y_j\\|^2)^{-1}$ | Gradient                                                                                        |                   |\n",
    "| 5    | $Y^{(t)} = Y^{(t-1)} + \\eta \\frac{\\partial C}{\\partial Y} + \\alpha(t)(Y^{(t-1)} - Y^{(t-2)})$     | Update rule                                                                                     |                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Python Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=6,\n",
    "    n_classes=3,\n",
    "    n_samples=1500,\n",
    "    n_informative=2,\n",
    "    random_state=5,\n",
    "    n_clusters_per_class=1,\n",
    ")\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], color=y, opacity=0.8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Transforming PCA\n",
    "\n",
    "We will now apply the PCA algorithm on the dataset to return two PCA components. The `fit_transform` learns and transforms the dataset at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualization Python\n",
    "We can now visualize the results by displaying two PCA components on a scatter plot. \n",
    "\n",
    "- x: First component\n",
    "- y: Second companion\n",
    "- color: target variable.\n",
    "\n",
    "We have also used the `update_layout` function to add a title and rename the x-axis and y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y)\n",
    "fig.update_layout(\n",
    "    title=\"PCA visualization of Custom Classification dataset\",\n",
    "    xaxis_title=\"First Principal Component\",\n",
    "    yaxis_title=\"Second Principal Component\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Transforming t-SNE\n",
    "\n",
    "Now we will apply the t-SNE algorithm to the dataset and compare the results.  \n",
    "\n",
    "After fitting and transforming data, we will display Kullback-Leibler (KL) divergence between the high-dimensional probability distribution and the low-dimensional probability distribution. Low KL divergence is a sign of better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "tsne.kl_divergence_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE Visualization Python\n",
    "Similar to PCA, we will visualize two t-SNE components on a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_tsne[:, 0], y=X_tsne[:, 1], color=y)\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of Custom Classification dataset\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE on Customer Churn Dataset\n",
    "\n",
    "In this section, we will use the real **Customer Churn** dataset of an Iranian telecom company. The dataset contains information on the customers' activity, such as call failures and subscription length, and a churn label.\n",
    "\n",
    "Churn means the percentage of customers that stop using a particular service during a given time frame.\n",
    "\n",
    "## Data Dictionary\n",
    "| Column                  | Explanation                                             |\n",
    "|-------------------------|---------------------------------------------------------|\n",
    "| Call Failure            | number of call failures                                 |\n",
    "| Complaints              | binary (0: No complaint, 1: complaint)                  |\n",
    "| Subscription Length     | total months of subscription                            |\n",
    "| Charge Amount           | ordinal attribute (0: lowest amount, 9: highest amount) |\n",
    "| Seconds of Use          | total seconds of calls                                  |\n",
    "| Frequency of use        | total number of calls                                   |\n",
    "| Frequency of SMS        | total number of text messages                           |\n",
    "| Distinct Called Numbers | total number of distinct phone calls                    |\n",
    "| Age Group               | ordinal attribute (1: younger age, 5: older age)        |\n",
    "| Tariff Plan             | binary (1: Pay as you go, 2: contractual)               |\n",
    "| Status                  | binary (1: active, 2: non-active)                       |\n",
    "| Age                     | age of customer                                         |\n",
    "| Customer Value          | the calculated value of customer                        |\n",
    "| Churn                   | class label (1: churn, 0: non-churn)                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"customer_churn.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction\n",
    "After that, we will:\n",
    "\n",
    "- Create features (X) and target (y) using the Churn column.\n",
    "- Normalize the features using a standard scaler.\n",
    "- Split the dataset into a training and testing set.\n",
    "- Apply PCA to the training dataset.\n",
    "- Get the score using the testing dataset. The score represents the average log-likelihood of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "pca.score(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing PCA\n",
    "We will now visualize the PCA result using the Plotly Express scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train_pca[:, 0], y=X_train_pca[:, 1], color=y_train)\n",
    "fig.update_layout(\n",
    "    title=\"PCA visualization of Customer Churn dataset\",\n",
    "    xaxis_title=\"First Principal Component\",\n",
    "    yaxis_title=\"Second Principal Component\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Perplexity vs. Divergence\n",
    "\n",
    "For the t-SNE algorithm, **perplexity is a very important hyperparameter**. \n",
    "\n",
    "It controls the effective number of neighbors that each point considers during the dimensionality reduction process. We will run a loop to get the KL Divergence metric on various perplexities from 5 to 55 with 5 points gap. After that, we will display the result using the Plotly Express line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "perplexity = np.arange(5, 55, 5)\n",
    "divergence = []\n",
    "\n",
    "for i in perplexity:\n",
    "    model = TSNE(n_components=2, init=\"pca\", perplexity=i)\n",
    "    reduced = model.fit_transform(X_train)\n",
    "    divergence.append(model.kl_divergence_)\n",
    "fig = px.line(x=perplexity, y=divergence, markers=True)\n",
    "fig.update_layout(xaxis_title=\"Perplexity Values\", yaxis_title=\"Divergence\")\n",
    "fig.update_traces(line_color=\"red\", line_width=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL Divergence has become constant after 40 perplexity. So, we will use 40 perplexity in t-SNE algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=40, random_state=42)\n",
    "X_train_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "tsne.kl_divergence_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the Plotly Scatter plot to display components and target classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=X_train_tsne[:, 0], y=X_train_tsne[:, 1], color=y_train)\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of Customer Churn dataset\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have multiple clusters and sub-clusters. We can use this information to understand the pattern and come up with a strategy for retaining existing customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of t-SNE\n",
    "\n",
    "Apart from visualizing complex multi-dimensional data, t-SNE has other uses mostly in the medical field. \n",
    "\n",
    "1. **Clustering and classification**: to cluster similar data points together in lower dimensional space. It can also be used for classification and finding patterns in the data. \n",
    "2. **Anomaly detection**: to identify outliers and anomalies in the data. \n",
    "3. **Natural language processing**: to visualize word embeddings generated from a large corpus of text that makes it easier to identify similarities and relationships between words.\n",
    "4. **Computer security**: to visualize network traffic patterns and detect anomalies.\n",
    "5. **Cancer research**: to visualize molecular profiles of tumor samples and identify subtypes of cancer. \n",
    "6. **Geological domain interpretation**: to visualize seismic attributes and to identify geological anomalies. \n",
    "7. **Biomedical signal processing**: to visualize electroencephalogram (EEG) and detect patterns of brain activity. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "t-SNE is a powerful visualization tool for revealing hidden patterns and structures in complex datasets. You can use it for images, audio, biologicals, and single data to identify anomalies and patterns. \n",
    "\n",
    "In this notebook, we have learned about t-SNE, a popular dimensionality reduction technique that can visualize high-dimensional non-linear data in a low-dimensional space. We have explained the main idea behind t-SNE, how it works, and its applications. Moreover, we showed some examples of applying t-SNE to synthetics and real datasets and how to interpret the results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
