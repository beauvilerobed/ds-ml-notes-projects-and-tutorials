{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow Method for Determining the Optimal Value of k in KMeans\n",
    "\n",
    "The **Elbow Method** is a technique used to determine the optimal number of clusters ($k$) in a **KMeans** clustering algorithm. The steps are as follows:\n",
    "\n",
    "1. **Run KMeans for a range of k values**: For example, from $k=1$ to some maximum value (e.g., $k=10$ or $k=20$).\n",
    "2. **Calculate the Within-Cluster Sum of Squares (WCSS)** for each $k$:\n",
    "   - WCSS measures the total variance within each cluster.\n",
    "   - A smaller WCSS indicates tighter clusters, but increasing $k$ always decreases WCSS.\n",
    "3. **Plot WCSS vs. k**: The plot typically decreases rapidly at first and then slows down.\n",
    "4. **Identify the \"elbow point\"**:\n",
    "   - The elbow point is where the marginal gain in reducing WCSS starts to diminish.\n",
    "   - This point represents the most appropriate number of clusters, balancing variance reduction and model simplicity.\n",
    "\n",
    "**Intuition**: Before the elbow, adding clusters significantly reduces WCSS (better fit). After the elbow, adding clusters provides minimal improvement, so the extra complexity is not justified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data\n",
    "x1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6, 7, 8, 9, 8, 9, 9, 8])\n",
    "x2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, 1, 2, 1, 2, 3, 2, 3])\n",
    "X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n",
    "X = pd.DataFrame(X)\n",
    "  \n",
    "# Visualizing the data\n",
    "plt.plot()\n",
    "plt.xlim([0, 10])\n",
    "plt.ylim([0, 10])\n",
    "plt.title('Dataset')\n",
    "plt.scatter(x1, x2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Elbow Method visualization, we observe that the optimal number of clusters appears to be around **3**. However, relying solely on visual inspection may not always provide the most accurate answer. To complement this, we can calculate quantitative metrics such as **distortion**, **inertia**, and **WCSS** for different values of $k$.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "* **Distortion**: The average of the squared distances from each point to the centroid of its assigned cluster. Formally, if we have a dataset of $n$ points ${x_1, x_2, \\dots, x_n}$ and $k$ cluster centroids ${c_1, c_2, \\dots, c_k}$, then\n",
    "\n",
    "$$\n",
    "\\text{Distortion} = \\frac{1}{n} \\sum_{i=1}^{n} \\lVert x_i - c_{x_i} \\rVert^2\n",
    "$$\n",
    "\n",
    "where $c_{x_i}$ is the centroid of the cluster assigned to point $x_i$.\n",
    "\n",
    "* **Inertia / WCSS**: The sum of the squared distances from each point to the centroid of its assigned cluster. This is equivalent to the **Within-Cluster Sum of Squares (WCSS)**:\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\text{Inertia} = \\sum_{i=1}^{n} \\lVert x_i - c_{x_i} \\rVert^2\n",
    "$$\n",
    "\n",
    "Notice that distortion is simply WCSS divided by the number of points:\n",
    "\n",
    "$$\n",
    "\\text{Distortion} = \\frac{\\text{WCSS}}{n}\n",
    "$$\n",
    "\n",
    "### Procedure\n",
    "\n",
    "1. Iterate over a range of $k$ values (e.g., $k = 1$ to $9$).  \n",
    "2. Fit the KMeans algorithm for each value of $k$.  \n",
    "3. Calculate and record **distortion**, **inertia**, and **WCSS** for each $k$.  \n",
    "4. Use these metrics, in combination with the elbow plot, to determine the optimal number of clusters.\n",
    "\n",
    "> **Note**: Distortion is essentially WCSS divided by the number of points in the dataset, so all three metrics provide complementary insights into cluster compactness and fit.\n",
    "\n",
    "### Pros and Cons of Using Distortion, Inertia, and WCSS\n",
    "\n",
    "**Pros:**\n",
    "* Provides **quantitative measures** of clustering quality.\n",
    "* Easy to **compute and compare** across different values of $k$.  \n",
    "* Directly reflects **cluster compactness**.\n",
    "\n",
    "**Cons:**\n",
    "* WCSS, inertia, and distortion **always decrease** as $k$ increases, so they cannot identify the optimal $k$ on their own.  \n",
    "* Sensitive to **outliers**, which can inflate distances and distort metrics.  \n",
    "* Does not account for **cluster separation**; only measures compactness.  \n",
    "* Choosing $k$ still requires **some subjective judgment** when interpreting the elbow plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X is your DataFrame from earlier\n",
    "distortions = []\n",
    "inertias = []\n",
    "clusters = []\n",
    "K = range(1, 10)\n",
    "\n",
    "for k in K:\n",
    "    # Build and fit the KMeans model\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, random_state=0)\n",
    "    y_kmeans = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Store results\n",
    "    clusters.append(y_kmeans)\n",
    "    distortions.append(kmeans.inertia_/X.shape[0])\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot distortion (Elbow Method)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method using Distortion')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot inertia (another Elbow Method metric)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method using Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the optimal number of clusters, we select the value of $k$ at the “elbow,” i.e., the point after which the distortion/inertia starts decreasing in a roughly linear fashion.  \n",
    "\n",
    "For the given data, we conclude that the optimal number of clusters is **3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Method for Optimal Value of $k$ in KMeans\n",
    "\n",
    "The **silhouette method** can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters, providing a visual way to assess clustering quality and determine the optimal number of clusters ($k$).\n",
    "\n",
    "### Silhouette Coefficient\n",
    "\n",
    "For a given sample $i$, the **silhouette coefficient** $s(i)$ is defined as:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a(i)$ = average distance between sample $i$ and all other points in the **same cluster** (intra-cluster distance)\n",
    "* $b(i)$ = minimum average distance between sample $i$ and all points in any **other cluster** (nearest-cluster distance)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "* **$s(i) \\approx 1$** → The sample is well-matched to its own cluster and far from neighboring clusters.\n",
    "* **$s(i) \\approx 0$** → The sample is near the boundary between two clusters.\n",
    "* **$s(i) < 0$** → The sample may have been assigned to the wrong cluster.\n",
    "\n",
    "By averaging the silhouette coefficients over all samples, we get the **mean silhouette score**, which can be used to compare different values of $k$:\n",
    "\n",
    "$$\n",
    "\\text{Mean Silhouette Score} = \\frac{1}{n} \\sum_{i=1}^{n} s(i)\n",
    "$$\n",
    "\n",
    "The value of $k$ that **maximizes the mean silhouette score** is often considered the optimal number of clusters.\n",
    "\n",
    "### Pros and Cons of the Silhouette Method\n",
    "\n",
    "**Pros:**\n",
    "* Provides a **quantitative measure** of clustering quality.\n",
    "* Works for **any distance metric**, not just Euclidean.\n",
    "* Helps **identify poorly clustered samples**.\n",
    "* Easy to **visualize** with silhouette plots.\n",
    "\n",
    "**Cons:**\n",
    "* Computationally **expensive** for large datasets (requires pairwise distance calculations).\n",
    "* Can be **misleading** for clusters with very different sizes or densities.\n",
    "* Less effective when the **true cluster structure is not well-separated**.\n",
    "* Sensitive to **noise and outliers**, which can reduce the mean silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Creating the data\n",
    "x1 = np.array([3, 1, 1, 2, 1, 6, 6, 6, 5, 6, 7, 8, 9, 8, 9, 9, 8])\n",
    "x2 = np.array([5, 4, 5, 6, 5, 8, 6, 7, 6, 7, 1, 2, 1, 2, 3, 2, 3])\n",
    "X = pd.DataFrame(list(zip(x1, x2)))\n",
    "\n",
    "K = range(2, 10)\n",
    "\n",
    "for n_clusters in K:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "    # Initialize KMeans\n",
    "    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # Compute average silhouette score\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is: {silhouette_avg:.3f}\")\n",
    "\n",
    "    # Compute silhouette values for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    # Silhouette plot\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_values.sort()\n",
    "        size_cluster_i = ith_cluster_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax1.set_title(\"Silhouette plot for clusters\")\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks(np.arange(-0.1, 1.1, 0.2))\n",
    "\n",
    "    # Cluster scatter plot\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', c=colors, edgecolor='k')\n",
    "\n",
    "    # Plot cluster centers\n",
    "    centers = clusterer.cluster_centers_\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c='white', s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=f\"${i}$\", alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"Cluster visualization\")\n",
    "\n",
    "    plt.suptitle(f\"Silhouette analysis for KMeans with n_clusters = {n_clusters}\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
