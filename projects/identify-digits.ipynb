{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python: Identify The Digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help you understand what exactly is Deep Learning and how advancements\n",
    "in industry has come when machines or computer programs are actually replacing humans. \n",
    "We will be covering:\n",
    "\n",
    "1. Data Science and Its Components\n",
    "2. The need of Deep Learning\n",
    "3. What is Deep Learning?\n",
    "4. Perceptrons and Artificial Neural Networks\n",
    "5. Applications of Deep Learning?\n",
    "6. Why Python for Deep Learning?\n",
    "7. Deep Learning with Python: Perceptron Example\n",
    "8. Deep Learning With Python: Creating a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science and It's Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Science** is the extaction of knowledge from data by using different techniques and algorithms.\n",
    "\n",
    "**Artificial Intelligence** is a technique which enables machines to mimic human behavior. **Machine Learning** is\n",
    "a subset of AI technique which uses statistical methods to enable machines to improve with experience. **Deep Learning**\n",
    "is a subset of ML which make the computation of multi-layer neural network feasible. It uses Neural networks to simulate\n",
    "human-like decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is based on the idea that machines should be given access to data and should\n",
    "be left to learn and explore for themselves. It deals with the extraction of patterns from large\n",
    "data sets. Handling large data sets was not a problem.\n",
    "- Machine Learning Algorithms **cannot handle high-dimensional data**- where we have a large number\n",
    "of inputs and outputs: around thousands of dimensions. Handling and processing such type of data \n",
    "becomes very complex and resource exhaustive. This is termed as **Curse of Dimensionality**.\n",
    "- Another challenge faced was, to specify the **features to be extracted**. This plays an important\n",
    "role in predicting the outcome as well as achieving better accuracy.\n",
    "\n",
    "Deep Learning is **capable of handling the high dimensional data** and is also efficient in \n",
    "**focusing on the right features** on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is a subset of Machine Learning where similar Machine Learning Algorithms are used to train \n",
    "**Deep Neural Networks** so as to achieve better accuracy in those cases where the former was not performing up\n",
    "to the mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons and Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Artificial Neuron or a Perceptron is a linear model used for binary classification. The\n",
    "neuron computes some function on these **weighted** inputs and gives the output.\n",
    "\n",
    "It recieves n inputs then sums those inputs, applies a transformation and produces an output. It has two functions:\n",
    "- Summation\n",
    "- Transformation(Activation)\n",
    "\n",
    "The weight shows the effectiveness of a particular input. \n",
    "**The more the weight of input, the more it will have an impact on the neural network**. On the other hand,\n",
    "**Bias** is an additional parameter in the Perceptron which is used to adjust the output along with the weighted sum\n",
    "of the inputs to the neuron which helps the model in a way that it can fit best for the given data.\n",
    "\n",
    "**Activation Functions** translates the input into outputs. It uses a threshold to produce an output. There are many\n",
    "functions that are used as Activation Functions, like:\n",
    "- Linear or Identity\n",
    "- Unit of Binary Step\n",
    "- Sigmoid or Logistic\n",
    "- Tanh\n",
    "- ReLU\n",
    "- Softmax\n",
    "\n",
    "Note:\n",
    "- Single-Layer Perceptrons **cannot classify non-linearly seperable data points**.\n",
    "- Complex problems, that involve **a lot of parameters** cannot be solved by single-layer perceptrons.\n",
    "\n",
    "A Neural Network is really just a **composition of Perceptrons, connected in different ways** and operating on\n",
    "different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few of the important ones that are present in our Day to Day tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Speech Recognition\n",
    "- Machine Translation\n",
    "- Facial Recognition and Automatic Tagging\n",
    "- Virtual Personal Assistants\n",
    "- Self Driving Car\n",
    "- Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python for Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python is a **general purpose programming language** as being **easy to use** when it comes to analytical and \n",
    "quantitative computing.\n",
    "- Python is **Dynamically Typed**\n",
    "- Huge Community Support\n",
    "- A vast range of Libraries for different purposes like **Numpy, Seaborn, Matplotlib, Pandas, and Scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Python: Perceptron Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with the \"OR\" gate. The output is 1 if any of the inputs is also 1\n",
    "\n",
    "| X1| X2| Y |\n",
    "| - | - | - |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "Therefore, a Perceptron can be used as a separator or a decision line that divides that input set of OR Gate, into\n",
    "two classes. Inputs having output as 0 that lies below the decision line. Inputs having output as 1 that lies above the\n",
    "decision line or separator.\n",
    "\n",
    "Mathematically a perceptron can be though of like an equation of Weights $W$, Inputs $x$, and Bias $b$.\n",
    "$$\n",
    "f(x) = W\\cdot x + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Vector Variables for Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = [\n",
    "[0,0,1],\n",
    "[0,1,1],\n",
    "[1,0,1],\n",
    "[1,1,1]]\n",
    " \n",
    "train_out = [\n",
    "[0],\n",
    "[1],\n",
    "[1],\n",
    "[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Weight Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define the tensor variable of shape 3×1 for our weights and assign some random values to it initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal([3, 1], seed=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define placeholders for Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define placeholders so that they can accept external inputs on the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,3])\n",
    "y = tf.placeholder(tf.float32,[None,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Output and Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, the input received by a perceptron is first multiplied by the respective weights and then, all these weighted inputs are summed together. This summed value is then fed to activation for obtaining the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.relu(tf.matmul(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate the Cost or Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(output - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Minimize Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a perceptron is to minimize the Loss or Cost or Error. So here we are going to use the Gradient Descent Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Initialize all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are only defined with tf.Variable. So, we need to initialize the variables defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Training Perceptron in Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train our perceptron i.e. update values of weights and bias in the successive iteration to minimize the error or loss. Here, I will train our perceptron in 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    sess.run(train, {x:train_in,y:train_out})\n",
    "    cost = sess.run(loss,feed_dict={x:train_in,y:train_out})\n",
    "    print('Epoch--',i,'--loss--',cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning With Python: Creating a Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s continue this notebook and see how can create our own Neural Network from Scratch, where we will create an Input Layer, Hidden Layers and Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the MNIST data-set. The MNIST data-set consists of **60,000 training** samples and **10,000 testing** samples of handwritten digit images. The images are of size **28×28 pixels** and the output can lie between **0-9**.\n",
    "\n",
    "**The task here is to train a model which can accurately identify the digit present on the image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    " \n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    " \n",
    "# tf Graph input\n",
    "x = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
    "y = tf.compat.v1.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store layers weight & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_hidden_2, n_classes]))\n",
    "}\n",
    " \n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random.normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random.normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    " \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate).minimize(cost, var_list=[weights,biases])\n",
    " \n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "cost_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "print(data for data in mnist['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    " \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(num_elements/batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist['train'].next_batch(batch_size)\n",
    " \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        if epoch % display_step == 0:\n",
    " \n",
    "            correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            acu_temp = accuracy.eval({x: mnist['test'].images, y: mnist['test'].labels})\n",
    "\n",
    "            accuracy_history.append(acu_temp)\n",
    "            cost_history.append(avg_cost)\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"- cost=\", \"{:.9f}\".format(avg_cost), \"- Accuracy=\",acu_temp)\n",
    " \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(cost_history)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracy_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Accuracy:\", accuracy.eval({x: mnist['test'].images, y: mnist['test'].labels}))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
