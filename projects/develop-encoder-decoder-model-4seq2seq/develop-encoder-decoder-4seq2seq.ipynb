{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras\n",
    "\n",
    "The encoder-decoder model provides a pattern for using recurrent neural networks to address challenging sequence-to-sequence prediction problems such as machine translation.\n",
    "\n",
    "This example can provide the basis for developing encoder-decoder LSTM models for your own sequence-to-sequence prediction problems.\n",
    "\n",
    "## Tutorial Overview\n",
    "This tutorial is divided into 3 parts; they are:\n",
    "\n",
    "- Encoder-Decoder Model in Keras\n",
    "- Scalable Sequence-to-Sequence Problem\n",
    "- Encoder-Decoder LSTM for Sequence Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Model in Keras\n",
    "\n",
    "It was originally developed for machine translation problems, although it has proven successful at related sequence-to-sequence prediction problems such as text summarization and question answering.\n",
    "\n",
    "The approach involves two recurrent neural networks, one to encode the source sequence, called the encoder, and a second to decode the encoded source sequence into the target sequence, called the decoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can develop a generic function named **define_models** to define an encoder-decoder recurrent neural network. The function takes 3 arguments, as follows:\n",
    "\n",
    "- **n_input**: The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n",
    "- **n_output**: The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n",
    "- **n_units**: The number of cells to create in the encoder and decoder models, e.g. 128 or 256.\n",
    "\n",
    "The function then creates and returns 3 models, as follows:\n",
    "\n",
    "- **train**: Model that can be trained given source, target, and shifted target sequences.\n",
    "- **inference_encoder**: Encoder model used when making a prediction for a new source sequence.\n",
    "- **inference_decoder**: Decoder model use when making a prediction for a new source sequence.\n",
    "\n",
    "The model is trained given source and target sequences where the model takes both the source and a shifted version of the target sequence as input and predicts the whole target sequence.\n",
    "\n",
    "For example, one source sequence may be [1,2,3] and the target sequence [4,5,6]. The inputs and outputs to the model during training would be:\n",
    "\n",
    "Input1: ['1', '2', '3']\n",
    "\n",
    "Input2: ['_', '4', '5']\n",
    "\n",
    "Output: ['4', '5', '6']\n",
    "\n",
    "The model is intended to be called recursively when generating target sequences for new source sequences.\n",
    "\n",
    "The source sequence is encoded and the target sequence is generated one element at a time, using a “start of sequence” character such as ‘_’ to start the process. Therefore, in the above case, the following input-output pairs would occur during training:\n",
    "\n",
    "|t| \tInput1      |Input2|Output|\n",
    "|-|-----------------|------|------|\n",
    "|1| ['1', '2', '3'] |'_'.  |'4'   |\n",
    "|2| ['1', '2', '3']\t|'4'.  |'5'   |\n",
    "|3| ['1', '2', '3']\t|'5'.  |'6'   |\n",
    "\n",
    "Here you can see how the recursive use of the model can be used to build up output sequences.\n",
    "\n",
    "During prediction, the **inference_encoder** model is used to encode the input sequence once which returns states that are used to initialize the **inference_decoder** model. From that point, the **inference_decoder** model is used to generate predictions step by step.\n",
    "\n",
    "The function named **predict_sequence** can be used after the model is trained to generate a target sequence given a source sequence.\n",
    "\n",
    "This function takes 5 arguments as follows:\n",
    "\n",
    "- **infenc**: Encoder model used when making a prediction for a new source sequence.\n",
    "- **infdec**: Decoder model use when making a prediction for a new source sequence.\n",
    "- **source**:Encoded source sequence.\n",
    "- **n_steps**: Number of time steps in the target sequence.\n",
    "- **cardinality**: The cardinality of the output sequence, e.g. the number of features, words, or characters for each time step.\n",
    "\n",
    "The function then returns a list containing the target sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalable Sequence-to-Sequence Problem\n",
    "\n",
    "In this section, we define a contrived and scalable sequence-to-sequence prediction problem.\n",
    "\n",
    "The source sequence is a series of randomly generated integer values, such as [20, 36, 40, 10, 34, 28], and the target sequence is a reversed pre-defined subset of the input sequence, such as the first 3 elements in reverse order [40, 36, 20].\n",
    "\n",
    "The length of the source sequence is configurable; so is the cardinality of the input and output sequence and the length of the target sequence.\n",
    "\n",
    "We will use source sequences of 6 elements, a cardinality of 50, and target sequences of 3 elements.\n",
    "\n",
    "Below are some more examples to make this concrete.\n",
    "\n",
    "|Source \t\t\t\t    |Target      |\n",
    "|---------------------------|------------|\n",
    "|[13, 28, 18, 7, 9, 5]\t\t|[18, 28, 13]|\n",
    "|[29, 44, 38, 15, 26, 22]\t|[38, 44, 29]|\n",
    "|[27, 40, 31, 29, 32, 1]    |[31, 40, 27]|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start off by defining a function to generate a sequence of random integers.\n",
    "\n",
    "We will use the value of 0 as the padding or start of sequence character, therefore it is reserved and we cannot use it in our source sequences. To achieve this, we will add 1 to our configured cardinality to ensure the one-hot encoding is large enough (e.g. a value of 1 maps to a ‘1’ value in index 1).\n",
    "\n",
    "Next, we need to create the corresponding output sequence given the source sequence.\n",
    "\n",
    "To keep thing simple, we will select the first n elements of the source sequence as the target sequence and reverse them.\n",
    "\n",
    "We also need a version of the output sequence shifted forward by one time step that we can use as the mock target generated so far, including the start of sequence value in the first time step. We can create this from the target sequence directly.\n",
    "\n",
    "Now that all of the sequences have been defined, we can one-hot encode them, i.e. transform them into sequences of binary vectors. We can use the Keras built in to_categorical() function to achieve this.\n",
    "\n",
    "Finally, we need to be able to decode a one-hot encoded sequence to make it readable again.\n",
    "\n",
    "This is needed for both printing the generated target sequences but also for easily comparing whether the full predicted target sequence matches the expected target sequence. The one_hot_decode() function will decode an encoded sequence.\n",
    "\n",
    "A complete worked example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 6, 51) (1, 1, 3, 51) (1, 1, 3, 51)\n",
      "X1=[1], X2=[0], y=[45]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\tfor _ in range(n_samples):\n",
    "\t\t# generate source sequence\n",
    "\t\tsource = generate_sequence(n_in, cardinality)\n",
    "\t\t# define target sequence\n",
    "\t\ttarget = source[:n_out]\n",
    "\t\ttarget.reverse()\n",
    "\t\t# create padded input target sequence\n",
    "\t\ttarget_in = [0] + target[:-1]\n",
    "\t\t# encode\n",
    "\t\tsrc_encoded = to_categorical([source], num_classes=cardinality)\n",
    "\t\ttar_encoded = to_categorical([target], num_classes=cardinality)\n",
    "\t\ttar2_encoded = to_categorical([target_in], num_classes=cardinality)\n",
    "\t\t# store\n",
    "\t\tX1.append(src_encoded)\n",
    "\t\tX2.append(tar2_encoded)\n",
    "\t\ty.append(tar_encoded)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "# configure problem\n",
    "n_features = 50 + 1\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "# generate a single source and target sequence\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "print(X1.shape, X2.shape, y.shape)\n",
    "print('X1=%s, X2=%s, y=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to develop a model for this sequence-to-sequence prediction problem.\n",
    "\n",
    "## Encoder-Decoder LSTM for Sequence Prediction\n",
    "\n",
    "In this section, we will apply the encoder-decoder LSTM model developed in the first section to the sequence-to-sequence prediction problem developed in the second section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6, 51) (100000, 3, 51) (100000, 3, 51)\n",
      "3125/3125 [==============================] - 76s 23ms/step - loss: 0.6460 - accuracy: 0.7954\n",
      "Accuracy: 98.00%\n",
      "X=[33, 30, 31, 35, 26, 47] y=[31, 30, 33], yhat=[30, 30, 33]\n",
      "X=[13, 16, 32, 7, 18, 39] y=[32, 16, 13], yhat=[32, 16, 13]\n",
      "X=[44, 15, 31, 40, 48, 41] y=[31, 15, 44], yhat=[31, 15, 44]\n",
      "X=[45, 26, 48, 10, 7, 11] y=[48, 26, 45], yhat=[48, 26, 45]\n",
      "X=[47, 17, 29, 25, 25, 10] y=[29, 17, 47], yhat=[29, 17, 47]\n",
      "X=[10, 29, 50, 13, 36, 8] y=[50, 29, 10], yhat=[50, 29, 10]\n",
      "X=[48, 36, 12, 44, 25, 41] y=[12, 36, 48], yhat=[12, 36, 48]\n",
      "X=[14, 37, 44, 7, 36, 3] y=[44, 37, 14], yhat=[44, 37, 14]\n",
      "X=[33, 3, 32, 1, 13, 8] y=[32, 3, 33], yhat=[32, 3, 33]\n",
      "X=[46, 39, 37, 14, 14, 4] y=[37, 39, 46], yhat=[37, 39, 46]\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# prepare data for the LSTM\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\tfor _ in range(n_samples):\n",
    "\t\t# generate source sequence\n",
    "\t\tsource = generate_sequence(n_in, cardinality)\n",
    "\t\t# define padded target sequence\n",
    "\t\ttarget = source[:n_out]\n",
    "\t\ttarget.reverse()\n",
    "\t\t# create padded input target sequence\n",
    "\t\ttarget_in = [0] + target[:-1]\n",
    "\t\t# encode\n",
    "\t\tsrc_encoded = to_categorical(source, num_classes=cardinality)\n",
    "\t\ttar_encoded = to_categorical(target, num_classes=cardinality)\n",
    "\t\ttar2_encoded = to_categorical(target_in, num_classes=cardinality)\n",
    "\t\t# store\n",
    "\t\tX1.append(src_encoded)\n",
    "\t\tX2.append(tar2_encoded)\n",
    "\t\ty.append(tar_encoded)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "\t# define training encoder\n",
    "\tencoder_inputs = Input(shape=(None, n_input))\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "\t# define training decoder\n",
    "\tdecoder_inputs = Input(shape=(None, n_output))\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdecoder_dense = Dense(n_output, activation='softmax')\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "\t# define inference decoder\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\t# return all models\n",
    "\treturn model, encoder_model, decoder_model\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "\t# encode\n",
    "\tstate = infenc.predict(source)\n",
    "\t# start of sequence input\n",
    "\ttarget_seq = array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "\t# collect predictions\n",
    "\toutput = list()\n",
    "\tfor t in range(n_steps):\n",
    "\t\t# predict next char\n",
    "\t\tyhat, h, c = infdec.predict([target_seq] + state)\n",
    "\t\t# store prediction\n",
    "\t\toutput.append(yhat[0,0,:])\n",
    "\t\t# update state\n",
    "\t\tstate = [h, c]\n",
    "\t\t# update target sequence\n",
    "\t\ttarget_seq = yhat\n",
    "\treturn array(output)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [argmax(vector) for vector in encoded_seq]\n",
    "# configure problem\n",
    "n_features = 50 + 1\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "# define model\n",
    "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# generate training dataset\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "print(X1.shape,X2.shape,y.shape)\n",
    "# train model\n",
    "train.fit([X1, X2], y, epochs=1)\n",
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tif array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))\n",
    "# spot check some examples\n",
    "for _ in range(10):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tprint('X=%s y=%s, yhat=%s' % (one_hot_decode(X1[0]), one_hot_decode(y[0]), one_hot_decode(target)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, 10 new examples are generated and target sequences are predicted. Again, we can see that the model correctly predicts the output sequence in each case and the expected value matches the reversed first 3 elements of the source sequences.\n",
    "\n",
    "You now have a template for an encoder-decoder LSTM model that you can apply to your own sequence-to-sequence prediction problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
