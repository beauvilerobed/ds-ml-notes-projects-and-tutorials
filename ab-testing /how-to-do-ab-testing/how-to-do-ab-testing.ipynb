{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Do A/B Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is A/B testing?\n",
    "\n",
    "A/B testing, also known as split testing, is a experiment wherein you split your audience to test variations to determine which performs better. It can be valuable because different audiences behave, well, differently. Something that works for one company may not necessarily work for another."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does A/B testing work?\n",
    "\n",
    "To run A/B test, you need to create two different versions of one piece of content, with changes to a single **variable**. Then you'll show these two versions to two similarly sized audiences and analyze which one performed better over a specific period (long enough to make accurate conclusions about your results).\n",
    "\n",
    "<img src=img/a-b-testing-explanation.webp>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \n",
    "\n",
    "Here are two types of A/B tests you might conduct to increase your website's conversion rate.\n",
    "\n",
    "### Example 1: User Experience Test\n",
    "Perhaps you want to see if moving a certain call-to-action (CTA) button to the top of your homepage instead of keeping it in the sidebar will improve its click-through rate.\n",
    "\n",
    "To A/B test this theory, you'd create another, alternative web page that uses the new CTA placement.\n",
    "\n",
    "The existing design with the sidebar CTA — or the \"**control**\" — is version A. Version B with the CTA at the top is the \"**challenger.**\" Then, you'd test these two versions by showing each to a predetermined percentage of site visitors.\n",
    "\n",
    "Ideally, the percentage of visitors seeing either version is the same.\n",
    "\n",
    "### Example 2: Design Test\n",
    "Perhaps you want to find out if changing the color of your CTA button can increase its click-through rate.\n",
    "To A/B test this theory, you'd design an alternative CTA button with a different button color that leads to the same landing page as the control.\n",
    "If you usually use a red CTA button in your marketing content, and the green variation receives more clicks after your A/B test, this could merit changing the default color of your CTA buttons to green from now on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing in Marketing\n",
    "\n",
    "There is a limitless list of items you can test to determine the overall impact on your bottom line\n",
    "Here are some elements you might decide to test in your campaigns:\n",
    "\n",
    "- Subject lines\n",
    "- CTAs\n",
    "- Headers\n",
    "- Titles\n",
    "- Fonts and colors\n",
    "- Product images\n",
    "- Blog graphics\n",
    "- Body copy\n",
    "- Navigation\n",
    "- Opt-in forms\n",
    "\n",
    "These tests are valuable to a business because they're low in cost but high in reward. Let's say you employ a content creator with a &#36;50,000/year salary.\n",
    "\n",
    "This content creator publishes five articles weekly for the company blog, totaling 260 articles per year.If the average post on the company's blog generates 10 leads, you could say it costs just over &#36;192 to generate 10 leads for the business (50,000 dollar salary ÷ 260 articles = 192 dollars per article). \n",
    "\n",
    "That's a solid chunk of change. Now, if you ask this content creator to spend two days developing an A/B test on one article, instead of writing two posts in that time, you might burn &#36;192, as you're publishing fewer articles. \n",
    "\n",
    "But if that A/B test finds you can increase conversion rates from 10 to 20 leads, you just spent &#36;192 to potentially double the number of customers your business gets from your blog. \n",
    "\n",
    "If the test fails, of course, you lost &#36;192 — but now you can make your next A/B test even more educated. \n",
    "\n",
    "If that second test succeeds, you ultimately spent &#36;384 to double your company's revenue. No matter how many times your A/B test fails, its eventual success will almost always outweigh the cost of conducting it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing Goals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common goals marketers have for their business when A/B testing.\n",
    "\n",
    "- **Higher Conversion Rate**: Testing different locations, colors, or even anchor text on your CTAs can change the number of people who click these CTAs to get to a landing page. This can increase the number of people who fill out forms on your website, submit their contact info to you, and \"convert\" into a lead. \n",
    "\n",
    "- **Increased Website Traffic**: Testing different blog or web page titles can change the number of people who click on that hyperlinked title to get to your website. This can increase website traffic.An increase in web traffic is a good thing! More traffic usually means more sales.\n",
    "\n",
    "- **Lower Bounce Rate**: If your website visitors leave (or \"bounce\") quickly after visiting your website, testing different blog post introductions, fonts, or featured images can retain visitors.\n",
    "\n",
    "- **Perfect Product Images**: Use A/B testing to determine which product image best catches the attention of your intended audience. Compare the images against each other and pick the one with the highest sales rate.\n",
    "\n",
    "- **Lower Cart Abandonment**: Testing different product photos, check-out page designs, and even where shipping costs are displayed can lower this abandonment rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Design an A/B Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to designing a successful A/B test is to determine which elements of your blog, website, or ad campaign that can be compared and contrasted against a new or different version.\n",
    "\n",
    "Before you jump into testing all the elements of your marketing campaign, check out these A/B testing best practices.\n",
    "\n",
    "- **Test appropriate items**: Choose appropriate test items by listing elements that affect your overall sales or lead conversion, and then prioritize them.\n",
    "\n",
    "- **Determine the correct sample size**: Make sure your sample size is large enough to yield accurate results. Use tools like **a sample size calculator** to help you figure out the correct number of interactions or visitors you need to your website or campaign to obtain the best result.\n",
    "\n",
    "- **Check your data**: Ensure your data is statistically significant and reliable by using tools like A/B test significance calculators.\n",
    "\n",
    "- **Schedule your tests**: Choose a timeframe when you can expect similar traffic to both portions of your split test.\n",
    "\n",
    "- **Test only one element**: Don’t try to test multiple elements at once. A good A/B test will be designed to test only one element at a time.\n",
    "\n",
    "- **Analyze the data**: Accurate and reliable data may tell a different story than you first imagined. Use the data to help plan or make changes to your campaigns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before the A/B Test\n",
    "\n",
    "1. **Pick one variable to test**: You'll want to isolate one **independent variable** and measure its performance.\n",
    "2. **Identify your goal**: Choose a primary metric to focus on before you run the test. This is your **dependent variable**, which changes based on how you manipulate the independent variable.\n",
    "3. **Create a 'control' and a 'challenger**: You now have your independent variable, your dependent variable, and your desired outcome. Use this information to set up the unaltered version of whatever you're testing as your control scenario.\n",
    "4. **Split your sample groups equally and randomly**\n",
    "5. **Determine your sample size (if applicable)**\n",
    "6. **Decide how significant your results need to be**: A takeaway here is that the more radical the change, the less scientific we need to be process-wise. The more specific the change (button color, microcopy, etc.), the more scientific we should be because the change is less likely to have a large and noticeable impact on conversion rate.\n",
    "7. **Make sure you're only running one test at a time on any campaign**: If you A/B test an email campaign that directs to a landing page while you’re A/B testing that landing page, how can you know which change caused the increase in leads?\n",
    "\n",
    "# During the A/B Test\n",
    "8. **Use an A/B testing tool**\n",
    "9. **Test both variations simultaneously**: If you were to run version A during one month and version B a month later, how would you know whether the performance change was caused by the different design or the different month?\n",
    "10. **Give the A/B test enough time to produce useful data**: A big part of how long it takes to get statistically significant results is how much traffic you get — so if your business doesn't get a lot of traffic to your website, it'll take much longer to run an A/B test.\n",
    "11. **Ask for feedback from real users**: A/B testing has a lot to do with quantitative data ... but that won't necessarily help you understand why people take certain actions over others. While you're running your A/B test, why not collect qualitative feedback from real users?\n",
    "\n",
    "# After the A/B Test\n",
    "12. **Focus on your goal metric**: For example, if you tested two variations of an email and chose leads as your primary metric, don’t get caught up on click-through rates. You might see a high click-through rate and poor conversions, in which case you might choose the variation that had a lower click-through rate in the end.\n",
    "13. **Measure the significance of your results using our A/B testing calculator**\n",
    "14. **Take action based on your results**:While A/B tests help you impact results on a case-by-case basis, you can also apply the lessons you learn from each test to future efforts. For example, suppose you've conducted A/B tests in your email marketing and have repeatedly found that using numbers in email subject lines generates better clickthrough rates. In that case, consider using that tactic in more of your emails.\n",
    "15. **Plan your next A/B test**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Read A/B Testing Results\n",
    "\n",
    "1. **Check your goal metric**\n",
    "2. **Compare your conversion rates**: The true test of success is whether your results are statistically significant. For example, variation A had a 16.04% conversion rate. Variation B had a 16.02% conversion rate, and your confidence interval of statistical significance is 95%. Variation A has a higher conversion rate, but the results are not statistically significant, meaning that variation A won’t significantly improve your overall conversion rate.\n",
    "3. **Segment your audiences for further insights**: Common variables for segmenting audiences are:\n",
    "    - Visitor type, or which version performed best for new visitors versus repeat visitors.\n",
    "    - Device type, or which version performed best on mobile versus desktop.\n",
    "    - Traffic source, or which version performed best based on where traffic to your two variations originated."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
