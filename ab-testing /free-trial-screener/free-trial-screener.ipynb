{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Tests with Python \"free trial\" Screener\n",
    "**Experiment Name**: \"Free Trial\" Screener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Setup\n",
    "\n",
    "- Udacity course pages currently show two options: **“Start free trial”** and **“Access course materials.”**\n",
    "  - **Start free trial:** Students enter credit card info and get a 14-day trial of the paid version. They’re charged automatically after 14 days unless they cancel.  \n",
    "  - **Access course materials:** Students can view videos and take quizzes for free, but don’t get coaching, feedback, or a certificate.\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "- Udacity tested adding a **time commitment question** after clicking “Start free trial.”  \n",
    "  - If a student said they could spend **5+ hours/week**, they continued to checkout as usual.  \n",
    "  - If they said **less than 5 hours/week**, they saw a message explaining that Udacity courses usually need more time and suggesting they might prefer the free materials option.  \n",
    "  - Students could then **either continue with the free trial** or **choose the free materials**.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "The change would help set realistic expectations and reduce cancellations from students who didn’t have enough time—without significantly lowering the number of paying or completing students.  \n",
    "This could improve student satisfaction and let coaches focus on learners likely to finish.\n",
    "\n",
    "## Experiment Details\n",
    "\n",
    "- **Unit of diversion:** cookie  \n",
    "- If a student enrolls in a free trial, tracking switches to **user ID** (a user can’t start multiple free trials).  \n",
    "- Users who don’t enroll aren’t tracked by user ID, even if they were signed in.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Choice\n",
    "\n",
    "A successful experiment needs two types of metrics: **Invariant** and **Evaluation** metrics.\n",
    "\n",
    "- **Invariant metrics**: Should *not* be affected by the experiment. They help verify that randomization worked and groups are comparable.\n",
    "- **Evaluation metrics**: Measure the impact of the experiment and relate directly to business goals.\n",
    "\n",
    "Each metric includes a **$D_{min}$** — the minimum meaningful change for the business.  \n",
    "(Example: a retention increase below 2% may be statistically significant but not practically useful.)\n",
    "\n",
    "### Invariant Metrics – Sanity Checks\n",
    "\n",
    "| Metric Name | Formula | $D_{min}$ | Notation |\n",
    "|--------------|----------|------------|-----------|\n",
    "| Cookies on course overview page | # unique daily cookies on page | 3000 cookies | $C_k$ |\n",
    "| Clicks on Free Trial button | # unique daily cookies who clicked | 240 clicks | $C_l$ |\n",
    "| Free Trial Click-Through Probability | $\\frac{C_l}{C_k}$ | 0.01 | $CTP$ |\n",
    "\n",
    "### Evaluation Metrics – Performance Indicators\n",
    "\n",
    "| Metric Name | Formula | $D_{min}$ | Notation |\n",
    "|--------------|----------|------------|-----------|\n",
    "| **Gross Conversion** | $\\frac{enrolled}{C_l}$ | 0.01 | $Conversion_{Gross}$ |\n",
    "| **Retention** | $\\frac{paid}{enrolled}$ | 0.01 | $Retention$ |\n",
    "| **Net Conversion** | $\\frac{paid}{C_l}$ | 0.0075 | $Conversion_{Net}$ |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the baseline values of metrics \n",
    "\n",
    "Before we start our experiment we should know how these metrics behave before the change - that is, what are their baseline values.\n",
    "\n",
    "### Collecting estimators data \n",
    "|Item|Description|Estimator|\n",
    "|--|--|--|\n",
    "|Number of cookies|\tDaily unique cookies to view course overview page|\t40,000|\n",
    "|Number of clicks|\tDaily unique cookies to click Free Trial button\t|3,200|\n",
    "|Number of enrollments\t|Free Trial enrollments per day|\t660|\n",
    "|CTP\t|CTP on Free Trial button\t|0.08|\n",
    "|Gross Conversion\t|Probability of enrolling, given a click\t|0.20625|\n",
    "|Retention\t|Probability of payment, given enrollment\t|0.53|\n",
    "|Net Conversion\t|Probability of payment, given click\t|0.109313|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's place this estimators into a dictionary for ease of use later\n",
    "baseline = {\"Cookies\": 40000,\n",
    "            \"Clicks\": 3200,\n",
    "            \"Enrollments\": 660,\n",
    "            \"CTP\": 0.08,\n",
    "            \"GConversion\": 0.20625,\n",
    "            \"Retention\": 0.53,\n",
    "            \"NConversion\": 0.109313}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Standard Deviation\n",
    "\n",
    "After collecting metric estimates, we calculate the **standard deviation** to use in **sample size** and **confidence interval** calculations.  \n",
    "A higher variance means it’s harder to detect a statistically significant effect.\n",
    "\n",
    "Assuming **5,000 cookies** visit the course overview page per day (as stated in the project instructions), we’ll estimate the standard deviation for the **evaluation metrics only**.  \n",
    "This sample size is smaller than the total population but large enough to form two comparison groups.\n",
    "\n",
    "### Scaling Collected Data\n",
    "\n",
    "Before calculating variance, we need to **scale** our collected metric counts to match the sample size used for variance estimation.  \n",
    "In this case, we scale from **40,000 unique cookies per day** (original data) down to **5,000 cookies per day**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale The counts estimates\n",
    "baseline[\"Cookies\"] = 5000\n",
    "baseline[\"Clicks\"]=baseline[\"Clicks\"]*(5000/40000)\n",
    "baseline[\"Enrollments\"]=baseline[\"Enrollments\"]*(5000/40000)\n",
    "baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Analytically\n",
    "\n",
    "To estimate variance analytically, we assume metrics that represent probabilities $(\\hat{p})$ follow a **binomial distribution**.  \n",
    "The standard deviation can then be calculated using:\n",
    "\n",
    "$$\n",
    "SD = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
    "$$\n",
    "\n",
    "This assumption holds only when the **unit of diversion** (how users are split) matches the **unit of analysis** (the denominator in the metric formula).  \n",
    "\n",
    "### What Happens if Units Differ?\n",
    "\n",
    "Suppose the **unit of diversion** is a user ID, but a single user may click multiple times:\n",
    "\n",
    "- User A clicks 3 times → potentially 3 “trials” in the denominator.  \n",
    "- User B clicks once → 1 trial.  \n",
    "\n",
    "Outcomes from the same user are **correlated** (if User A doesn’t enroll once, they probably won’t enroll on other clicks).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "- **Independent Bernoulli trials:**  \n",
    "$\n",
    "Var(X) = n p (1-p)\n",
    "$\n",
    "\n",
    "- **Correlated trials:**  \n",
    "$\n",
    "Var(X) > n p (1-p)\n",
    "$ (assuming positive correlation between all users)\n",
    "\n",
    "**Why?** Because repeated measures from the same user aren’t adding as much independent information. The effective sample size is smaller than the count of trials. The actual variance may vary, and it’s better to estimate it **empirically** using collected data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gross Conversion** – The baseline represents the probability of enrollment given a click.\n",
    "\n",
    "> In this case, the **unit of diversion** (cookies) — how users are split between control and experiment — is the same as the **unit of analysis** (cookies who click), which is the denominator in the Gross Conversion formula.  \n",
    "\n",
    "Because these units match, the **analytical estimate of variance** is valid and sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the p and n we need for Gross Conversion (GC)\n",
    "# and compute the Stansard Deviation(sd) rounded to 4 decimal digits.\n",
    "GC={}\n",
    "GC[\"d_min\"]=0.01\n",
    "GC[\"p\"]=baseline[\"GConversion\"]\n",
    "#p is given in this case - or we could calculate it from enrollments/clicks\n",
    "GC[\"n\"]=baseline[\"Clicks\"]\n",
    "GC[\"sd\"]=round(mt.sqrt((GC[\"p\"]*(1-GC[\"p\"]))/GC[\"n\"]),4)\n",
    "GC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retention** - The baseline is the probability of payment, given enrollment. The sample size is the number of enrolled users. \n",
    "\n",
    ">In this case, unit of diversion is not equal to unit of analysis (users who enrolled) so an analytical estimation is not enough \n",
    "\n",
    "If we had the data for these estimates, we would want to estimate this variance empirically as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the p and n we need for Retention(R)\n",
    "# and compute the Stansard Deviation(sd) rounded to 4 decimal digits.\n",
    "R={}\n",
    "R[\"d_min\"]=0.01\n",
    "R[\"p\"]=baseline[\"Retention\"]\n",
    "R[\"n\"]=baseline[\"Enrollments\"]\n",
    "R[\"sd\"]=round(mt.sqrt((R[\"p\"]*(1-R[\"p\"]))/R[\"n\"]),4)\n",
    "R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Net Conversion** - The baseline is the probability of payment, given a click. The sample size is the number of cookies that clicked. \n",
    "> In this case, the unit of analysis and diversion are equal so we expect a good enough estimation analytically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the p and n we need for Net Conversion (NC)\n",
    "# and compute the Standard Deviation (sd) rounded to 4 decimal digits.\n",
    "NC={}\n",
    "NC[\"d_min\"]=0.0075\n",
    "NC[\"p\"]=baseline[\"NConversion\"]\n",
    "NC[\"n\"]=baseline[\"Clicks\"]\n",
    "NC[\"sd\"]=round(mt.sqrt((NC[\"p\"]*(1-NC[\"p\"]))/NC[\"n\"]),4)\n",
    "NC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Sizing\n",
    "\n",
    "Given  $\\alpha = 0.05$ (significance level) and  $\\beta = 0.2$ (power), we want to estimate how many total pageviews (cookies who viewed the course overview page) are needed in the experiment. This total will be divided into the two groups: control and experiment.\n",
    "\n",
    "The minimum sample size for control and experiment groups, which provides a probability of **Type I Error** $\\alpha$, **Power** $1−\\beta$, **detectable effect** $d$, and **baseline conversion rate** $p$ (simple hypothesis)  \n",
    "\n",
    "$$\n",
    "H_0: P_{cont} - P_{exp} = 0\n",
    "$$\n",
    "\n",
    "against the simple alternative  \n",
    "\n",
    "$$\n",
    "H_A: P_{cont} - P_{exp} = d\n",
    "$$\n",
    "\n",
    "is:\n",
    "\n",
    "$$\n",
    "n = \\frac{\\left(Z_{1-\\frac{\\alpha}{2}} sd_1 + Z_{1-\\beta} sd_2 \\right)^2}{d^2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "sd_1 = \\sqrt{2 p (1-p)}, \\quad sd_2 = \\sqrt{p (1-p) + (p+d)(1-(p+d))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Size Formula for Two-Sample Proportions\n",
    "\n",
    "Suppose we want to compare two proportions:\n",
    "\n",
    "- Control group proportion: $p_1 = p$  \n",
    "- Experiment group proportion: $p_2 = p + d$  \n",
    "- Detectable difference: $d = p_2 - p_1$  \n",
    "\n",
    "We aim for:\n",
    "\n",
    "- Significance level: $\\alpha$ (Type I error)  \n",
    "- Power: $1 - \\beta$ (probability of detecting a true effect)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Test Statistic\n",
    "\n",
    "For a two-sided z-test comparing proportions, the test statistic is:\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\frac{p_1 (1-p_1)}{n} + \\frac{p_2 (1-p_2)}{n}}}\n",
    "$$\n",
    "\n",
    "where $n$ is the sample size per group.  \n",
    "\n",
    "- **Under $H_0$**: $p_1 = p_2 = p$\n",
    "\n",
    "$$\n",
    "SD_0 = \\sqrt{\\frac{p(1-p)}{n} + \\frac{p(1-p)}{n}} = \\sqrt{\\frac{2 p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "- **Under $H_A$**: $p_1 = p, p_2 = p+d$\n",
    "\n",
    "$$\n",
    "SD_A = \\sqrt{\\frac{p(1-p) + (p+d)(1-(p+d))}{n}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Critical Values for Significance and Power\n",
    "\n",
    "- Two-sided test significance $\\alpha$:\n",
    "\n",
    "$$\n",
    "Z_{\\text{crit}} = Z_{1-\\frac{\\alpha}{2}}\n",
    "$$\n",
    "\n",
    "- To achieve power $1-\\beta$, we require:\n",
    "\n",
    "$$\n",
    "P(\\text{Reject } H_0 \\mid H_A) = 1 - \\beta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relating Detectable Difference to z-Scores\n",
    "\n",
    "Let $X = \\hat{p}_1 - \\hat{p}_2$. Under $H_A$, \n",
    "\n",
    "$$\n",
    "X \\sim N(d, SD_A^2)\n",
    "$$\n",
    "\n",
    "Standardizing:\n",
    "\n",
    "$$\n",
    "P\\left( \\frac{X - d}{SD_A} > \\frac{Z_{\\text{crit}} SD_0 - d}{SD_A} \\right) = 1 - \\beta\n",
    "$$\n",
    "\n",
    "By the definition of the standard normal quantile:\n",
    "\n",
    "$$\n",
    "\\frac{Z_{\\text{crit}} SD_0 - d}{SD_A} = -Z_{1-\\beta}\n",
    "$$\n",
    "\n",
    "Rearranging gives:\n",
    "\n",
    "$$\n",
    "d = Z_{1-\\frac{\\alpha}{2}} SD_0 + Z_{1-\\beta} SD_A\n",
    "$$\n",
    "\n",
    "Substitute $SD_0$ and $SD_A$:\n",
    "\n",
    "$$\n",
    "d = Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{2 p(1-p)}{n}} + Z_{1-\\beta} \\sqrt{\\frac{p(1-p) + (p+d)(1-(p+d))}{n}}\n",
    "$$\n",
    "\n",
    "Factor out $1/\\sqrt{n}$:\n",
    "\n",
    "$$\n",
    "d = \\frac{1}{\\sqrt{n}} \\Bigg( Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))} \\Bigg)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Solve for Sample Size $n$\n",
    "\n",
    "$$\n",
    "\\sqrt{n} = \\frac{Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))}}{d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "n = \\frac{\\Big( Z_{1-\\frac{\\alpha}{2}} \\sqrt{2 p(1-p)} + Z_{1-\\beta} \\sqrt{p(1-p) + (p+d)(1-(p+d))} \\Big)^2}{d^2}\n",
    "}\n",
    "$$\n",
    "\n",
    "This gives the **required sample size per group** to detect a difference $d$ with significance level $\\alpha$ and power $1-\\beta$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding inputs, we have all the data we need: Type 1 error $(\\alpha)$, power $(1−\\beta)$, detectable change $(d=Dmin)$ and baseline conversion rate, our $\\hat{p}$. What we need to calculate:\n",
    "\n",
    "- Get Z score for $1−\\frac{α}{2}$ and for $1−\\beta$\n",
    " \n",
    "- Get standard deviations 1 & 2, that is for both the baseline and for expected changed rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: required alpha value (alpha should already fit the required test)\n",
    "#Returns: z-score for given alpha\n",
    "def get_z_score(alpha):\n",
    "    return norm.ppf(alpha)\n",
    "\n",
    "# Inputs p-baseline conversion rate which is our estimated p and d-minimum detectable change\n",
    "# Returns\n",
    "def get_sds(p,d):\n",
    "    sd1=mt.sqrt(2*p*(1-p))\n",
    "    sd2=mt.sqrt(p*(1-p)+(p+d)*(1-(p+d)))\n",
    "    sds=[sd1,sd2]\n",
    "    return sds\n",
    "\n",
    "# Inputs:sd1-sd for the baseline,sd2-sd for the expected change,alpha,beta,d-d_min,p-baseline estimate p\n",
    "# Returns: the minimum sample size required per group according to metric denominator\n",
    "def get_sampSize(sds,alpha,beta,d):\n",
    "    n=pow((get_z_score(1-alpha/2)*sds[0]+get_z_score(1-beta)*sds[1]),2)/pow(d,2)\n",
    "    return n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Sample Size per Metric\n",
    "\n",
    "### Gross Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC[\"d\"]=0.01\n",
    "R[\"d\"]=0.01\n",
    "NC[\"d\"]=0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an integer value for simplicity\n",
    "GC[\"SampSize\"]=round(get_sampSize(get_sds(GC[\"p\"],GC[\"d\"]),0.05,0.2,GC[\"d\"]))\n",
    "GC[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need at least 25,835 cookies who click the Free Trial button - per group! That means that if we got 400 clicks out of 5000 pageviews (400/5000 = 0.08) -> So, we are going to need GC[\"SampSize\"]/0.08 = 322,938 pageviews, again ; per group! Finally, the total amount of samples per the Gross Conversion metric is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC[\"SampSize\"]=round(GC[\"SampSize\"]/0.08*2)\n",
    "GC[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a nice integer value\n",
    "R[\"SampSize\"]=round(get_sampSize(get_sds(R[\"p\"],R[\"d\"]),0.05,0.2,R[\"d\"]))\n",
    "R[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we need 39,087 users who enrolled per group! We have to first convert this to cookies who clicked, and then to cookies who viewed the page, then finally to multipky by two for both groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R[\"SampSize\"]=round(R[\"SampSize\"]/0.08/0.20625*2)\n",
    "R[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes us as high as over 4 million page views total, this is practically impossible because we know we get about 40,000 a day, this would take well over 100 days. This means we have to drop this metric and not continue to work with it because results from our experiment (which is much smaller) will be biased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a nice integer value\n",
    "NC[\"SampSize\"]=round(get_sampSize(get_sds(NC[\"p\"],NC[\"d\"]),0.05,0.2,NC[\"d\"]))\n",
    "NC[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, needing 27,413 cookies who click per group takes us all the way up to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC[\"SampSize\"]=NC[\"SampSize\"]/0.08*2\n",
    "NC[\"SampSize\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all the way up to 685,325 cookies who view the page. This is more than what was needed for Gross Conversion, so this will be our number. Assuming we take 80% of each days pageviews, the data collection period for this experiment (the period in which the experiment is revealed) will be about 3 weeks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Collected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pandas to load datasets\n",
    "control=pd.read_csv(\"control_data.csv\")\n",
    "experiment=pd.read_csv(\"experiment_data.csv\")\n",
    "control.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "We have 3 Invariant metrics::\n",
    "\n",
    "- Number of Cookies in Course Overview Page\n",
    "- Number of Clicks on Free Trial Button\n",
    "- Free Trial button Click-Through-Probability (CTP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A significant difference will imply a biased experiment \n",
    "# that we should not rely on it's results.\n",
    "\n",
    "pageviews_cont=control['Pageviews'].sum()\n",
    "pageviews_exp=experiment['Pageviews'].sum()\n",
    "pageviews_total=pageviews_cont+pageviews_exp\n",
    "print (\"number of pageviews in control:\", pageviews_cont)\n",
    "print (\"number of Pageviewsin experiment:\" ,pageviews_exp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so these look like pretty close numbers. Now, let's make sure this difference in amounts is not significant and is random and even like we expected. We can model this diversion in the following way:\n",
    "We expect the amount of pageviews in the control group to be about a half (50%) of the total pageviews in both groups, so we can define a random variable with an easy to use distribution.\n",
    "\n",
    "A binomial random variable will be the number of successes we can expect to get out of N experiments, given the probability of a single success. So, if we consider being assigned to a group (control, for example) a success with probability 0.5 (random!), the number of samples which get assigned to the group is the value of our random binomial variable!\n",
    "\n",
    "This get's easier thanks to the central limit theorem which let's us approximate the binomial distribution to a normal distribution (when n is large enough) with a mean of  $p$ and a standard deviation $\\sqrt{\\frac{p(1-p)}{N}}$\n",
    "\n",
    "$$\n",
    "X \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{N}}\\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to test is whether our observed $\\hat{p}$ (number of samples in control divided by total number of damples in both groups) is not significantly different than $p=0.5$. \n",
    "\n",
    "In order to do that we can calculate the margin of error acceptable at a 95% confidence level:\n",
    "\n",
    "$$\n",
    "ME = Z_{1-\\frac{\\alpha}{2}}SD\n",
    "$$\n",
    "with confidence interval\n",
    "$$\n",
    "CI = [\\hat{p}-ME,\\hat{p}+ME]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.5\n",
    "alpha=0.05\n",
    "p_hat=round(pageviews_cont/(pageviews_total),4)\n",
    "sd=mt.sqrt(p*(1-p)/(pageviews_total))\n",
    "ME=round(get_z_score(1-(alpha/2))*sd,4)\n",
    "print (\"The confidence interval is between\",p-ME,\"and\",p+ME,\"; Is\",p_hat,\"inside this range?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our observed  $\\hat{p}$ is inside this range which means the difference in number of samples between groups is expected. So far so good, since this invariant metric sanity test passes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cookies who clicked the Free Trial Button \n",
    "\n",
    "clicks_cont=control['Clicks'].sum()\n",
    "clicks_exp=experiment['Clicks'].sum()\n",
    "clicks_total=clicks_cont+clicks_exp\n",
    "\n",
    "p_hat=round(clicks_cont/clicks_total,4)\n",
    "sd=mt.sqrt(p*(1-p)/clicks_total)\n",
    "ME=round(get_z_score(1-(alpha/2))*sd,4)\n",
    "print (\"The confidence interval is between\",p-ME,\"and\",p+ME,\"; Is\",p_hat,\"inside this range?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have another pass! Great, so far it still seems all is well with our experiment results. Now, for the final metric which is a probability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks for differences between probabilities \n",
    "\n",
    "**Click-through-probability of the Free Trial Button** In this case, we want to make sure the proportion of clicks given a pageview (our observed CTP) is about the same in both groups. In order to check this out we will calculate the CTP in each group and calculate a confidence interval for the expected difference between them.\n",
    "\n",
    "I.e. we expect to see no difference $(CTP_{exp}−CTP_{cont}=0)$ with an acceptable margin of error. The changes we should notice are for the calculation of the standard error - which in this case is a pooled standard error.\n",
    "\n",
    "$$\n",
    "SD_{pool} = \\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{N_{cont}}+\\frac{1}{N_{exp}}\\right)}\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\hat{p}_{pool} = \\frac{x_{cont}+x_{exp}}{N_{cont}+N_{exp}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctp_cont=clicks_cont/pageviews_cont\n",
    "ctp_exp=clicks_exp/pageviews_exp\n",
    "d_hat=round(ctp_exp-ctp_cont,4)\n",
    "p_pooled=clicks_total/pageviews_total\n",
    "sd_pooled=mt.sqrt(p_pooled*(1-p_pooled)*(1/pageviews_cont+1/pageviews_exp))\n",
    "ME=round(get_z_score(1-(alpha/2))*sd_pooled,4)\n",
    "print (\"The confidence interval is between\",0-ME,\"and\",0+ME,\"; Is\",d_hat,\"within this range?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful. It seems this test has passed with flying colors as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining effect size\n",
    "\n",
    "The next step is looking at the changes between the control and experiment groups with regard to our evaluation metrics to make sure the difference is there, that it is statistically significant and most importantly practically significant (the difference is \"big\" enough to make the experimented change beneficial to the company)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note:  A metric is statistically significant if the confidence interval does not include 0 (that is, you can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, you can be confident there is a change that matters to the business.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gross Conversion \n",
    "\n",
    "# Count the total clicks from complete records only\n",
    "clicks_cont=control[\"Clicks\"].loc[control[\"Enrollments\"].notnull()].sum()\n",
    "clicks_exp=experiment[\"Clicks\"].loc[experiment[\"Enrollments\"].notnull()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gross Conversion - number of enrollments divided by number of clicks\n",
    "enrollments_cont=control[\"Enrollments\"].sum()\n",
    "enrollments_exp=experiment[\"Enrollments\"].sum()\n",
    "\n",
    "GC_cont=enrollments_cont/clicks_cont\n",
    "GC_exp=enrollments_exp/clicks_exp\n",
    "GC_pooled=(enrollments_cont+enrollments_exp)/(clicks_cont+clicks_exp)\n",
    "GC_sd_pooled=mt.sqrt(GC_pooled*(1-GC_pooled)*(1/clicks_cont+1/clicks_exp))\n",
    "GC_ME=round(get_z_score(1-alpha/2)*GC_sd_pooled,4)\n",
    "GC_diff=round(GC_exp-GC_cont,4)\n",
    "print(\"The change due to the experiment is\",GC_diff*100,\"%\")\n",
    "print(\"Confidence Interval: [\",GC_diff-GC_ME,\",\",GC_diff+GC_ME,\"]\")\n",
    "print (\"The change is statistically significant if the CI doesn't include 0.\")\n",
    "print (\"In that case, it is practically significant if\",-GC[\"d_min\"],\"is not in the CI as well.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this result there was a change due to the experiment, that change was both statistically and practically significant. We have a negative change of 2.06%, when we were willing to accept any change greater than 1%. This means the Gross Conversion rate of the experiment group (the one exposed to the change, i.e. asked how many hours they can devote to studying) has decreased as expected by 2% and this change was significant. This means less people enrolled in the Free Trial after due to the pop-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net Conversion\n",
    "\n",
    "#Net Conversion - number of payments divided by number of clicks\n",
    "payments_cont=control[\"Payments\"].sum()\n",
    "payments_exp=experiment[\"Payments\"].sum()\n",
    "\n",
    "NC_cont=payments_cont/clicks_cont\n",
    "NC_exp=payments_exp/clicks_exp\n",
    "NC_pooled=(payments_cont+payments_exp)/(clicks_cont+clicks_exp)\n",
    "NC_sd_pooled=mt.sqrt(NC_pooled*(1-NC_pooled)*(1/clicks_cont+1/clicks_exp))\n",
    "NC_ME=round(get_z_score(1-alpha/2)*NC_sd_pooled,4)\n",
    "NC_diff=round(NC_exp-NC_cont,4)\n",
    "print(\"The change due to the experiment is\",NC_diff*100,\"%\")\n",
    "print(\"Confidence Interval: [\",NC_diff-NC_ME,\",\",NC_diff+NC_ME,\"]\")\n",
    "print (\"The change is statistically significant if the CI doesn't include 0.\")\n",
    "print (\"In that case, it is practically significant if\",NC[\"d_min\"],\"is not in the CI as well.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we got a change size of less than a 0.5%, a very small decrease which is not statistically significant, and as such not practically significant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check with Sign Tests \n",
    "\n",
    "In a sign test we get another angle at analyzing the results we got - we check if the trend of change we observed (increase or decrease) was evident in the daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first create the dataset we need for this:\n",
    "# start by merging the two datasets\n",
    "\n",
    "full=control.join(other=experiment,how=\"inner\",lsuffix=\"_cont\",rsuffix=\"_exp\")\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we only need the complete data records\n",
    "full=full.loc[full[\"Enrollments_cont\"].notnull()]\n",
    "full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect! Now, derive a new column for each metric, so we have it's daily values\n",
    "\n",
    "# We need a 1 if the experiment value is greater than the control value=\n",
    "x=full['Enrollments_cont']/full['Clicks_cont']\n",
    "y=full['Enrollments_exp']/full['Clicks_exp']\n",
    "full['GC'] = np.where(x<y,1,0)\n",
    "\n",
    "# The same now for net conversion\n",
    "z=full['Payments_cont']/full['Clicks_cont']\n",
    "w=full['Payments_exp']/full['Clicks_exp']\n",
    "full['NC'] = np.where(z<w,1,0)\n",
    "\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_x=full.GC[full[\"GC\"]==1].count()\n",
    "NC_x=full.NC[full[\"NC\"]==1].count()\n",
    "n=full.NC.count()\n",
    "print(f\"\"\"\n",
    "No. of cases for GC: {GC_x}\n",
    "No. of cases for NC: {NC_x}\n",
    "No. of total cases {n}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Sign Test\n",
    "What we want to do after we count the amount of days in which the experiment group had a higher metric value than that of the control group, is to see if that number is likely to be seen again in a new experiment (significance). \n",
    "\n",
    "We assume the chance of a day like this is random (50% chance to happen) and then use the binomial distribution with  $p=0.5$ and the number of experiments (days) to tell us the probability of this happening according to a random chance.\n",
    "\n",
    "So, according to the binomial distribution with $p=0.5$ and $n=$ total number of days; we want to know the probability of x days being a success (higher metric value in experiment). Because we are doing a two-tailed test we want to double this probability and once we have we can call it the p−value and compare it to our $\\alpha$. If the p−value is greater than the $\\alpha$ the result is not significant and vice-versa.\n",
    "\n",
    "$$\n",
    "p(successes) = \\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\n",
    "$$\n",
    "\n",
    "**Recall** that a p-value is the probability of observing a test statistic as or more extreme than that observed. If we observe 2 days like that, the p-value for the test is: $p = P(x\\leq 2)$. Thus\n",
    "\n",
    "$$\n",
    "p(x\\leq 2) = p(0)+p(1)+p(2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first a function for calculating probability of x=number of successes\n",
    "def get_prob(x,n):\n",
    "    p=round(mt.factorial(n)/(mt.factorial(x)*mt.factorial(n-x))*0.5**x*0.5**(n-x),4)\n",
    "    return p\n",
    "\n",
    "#next a function to compute the pvalue from probabilities of maximum x\n",
    "def get_2side_pvalue(x,n):\n",
    "    p=0\n",
    "    for i in range(0,x+1):\n",
    "        p=p+get_prob(i,n)\n",
    "    return 2*p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to conduct the sign test itself: we will calculate the p-value for each metric, using the counts GC_x, NC_x and n and the function get_2side_pvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"GC Change is significant if\",get_2side_pvalue(GC_x,n),\"is smaller than 0.05\")\n",
    "print (\"NC Change is significant if\",get_2side_pvalue(NC_x,n),\"is smaller than 0.05\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same conclusions as we got from our effect size calculation: the change in Gross conversion was indeed significant, while the change in Net conversion was not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions & Recommendations \n",
    "\n",
    "At this point, once we have seen that the actual underlying goal we had was not reached (increase fraction of paying users by asking them in advance if they have the time to invest in the course), we can only recommend to not continue with change. It may have caused a change in Gross conversion, but it didn't for net conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
