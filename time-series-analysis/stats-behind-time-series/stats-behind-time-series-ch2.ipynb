{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Stationary Processes\n",
    "\n",
    "## 2.1 Measure of Dependence\n",
    "\n",
    "Denote the mean function of $\\{X_t\\}$ as  \n",
    "$$\n",
    "\\mu_X(t) = \\mathbb{E}(X_t),\n",
    "$$  \n",
    "provided it exists. And the autocovariance function of $\\{X_t\\}$ is  \n",
    "$$\n",
    "\\gamma_X(s, t) = \\text{Cov}(X_s, X_t) = \\mathbb{E} \\big[ (X_s - \\mu_X(s)) (X_t - \\mu_X(t)) \\big].\n",
    "$$\n",
    "\n",
    "Preliminary results of covariance and correlation: for any random variables $X, Y$, and $Z$,  \n",
    "$$\n",
    "\\text{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y), \\quad \\text{and} \\quad \\text{Corr}(X, Y) = \\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}}.\n",
    "$$\n",
    "\n",
    "1. $-1 \\leq \\rho_{XY} \\leq 1$ for any $X$ and $Y$.  \n",
    "2. $\\text{Cov}(X, X) = \\text{Var}(X)$.  \n",
    "3. $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$.  \n",
    "4. $\\text{Cov}(aX, Y) = a \\, \\text{Cov}(X, Y)$.  \n",
    "5. $\\text{Cov}(a + X, Y) = \\text{Cov}(X, Y)$.  \n",
    "6. If $X$ and $Y$ are independent, then $\\text{Cov}(X, Y) = 0$.  \n",
    "7. $\\text{Cov}(X, Y) = 0$ does **not** imply $X$ and $Y$ are independent.  \n",
    "8. $\\text{Cov}(X + Y, Z) = \\text{Cov}(X, Z) + \\text{Cov}(Y, Z)$.  \n",
    "9. \n",
    "$$\n",
    "\\text{Cov}\\left(\\sum_{i=1}^n a_i X_i, \\sum_{j=1}^m b_j Y_j\\right) = \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j \\, \\text{Cov}(X_i, Y_j).\n",
    "$$\n",
    "\n",
    "Verify 1–9 as a homework problem.\n",
    "\n",
    "The time series $\\{X_t\\}$ is (weakly) stationary if  \n",
    "1. $\\mu_X(t)$ is independent of $t$;  \n",
    "2. $\\gamma_X(t + h, t)$ is independent of $t$ for each $h$.\n",
    "\n",
    "We say $\\{X_t\\}$ is strictly (or strongly) stationary if  \n",
    "$$\n",
    "(X_{t_1}, \\ldots, X_{t_k}) \\quad \\text{and} \\quad (X_{t_1 + h}, \\ldots, X_{t_k + h})\n",
    "$$  \n",
    "have the same joint distributions for all $k = 1, 2, \\ldots$, $h = 0, \\pm 1, \\pm 2, \\ldots$, and time points $t_1, \\ldots, t_k$. This is a very strong condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.1.** Basic properties of a strictly stationary time series $\\{X_t\\}$:  \n",
    "1. $X_t$'s are from the same distribution.  \n",
    "2. $(X_t, X_{t+h}) \\stackrel{d}{=} (X_1, X_{1+h})$ for all integers $t$ and $h$.  \n",
    "3. $\\{X_t\\}$ is weakly stationary if $\\mathbb{E}(X_t^2) < \\infty$ for all $t$.  \n",
    "4. Weak stationarity does **not** imply strict stationarity.  \n",
    "5. An iid sequence is strictly stationary.\n",
    "\n",
    "*Proof.* The proof is quite straightforward and thus left as a homework problem.\n",
    "\n",
    "\n",
    "**Example 2.1 (q-dependent strictly stationary time series):**  \n",
    "One of the simplest ways to construct a time series $\\{X_t\\}$ that is strictly stationary is to “filter” an iid sequence. Let $\\{Z_t\\} \\sim \\text{IID}(0, \\sigma^2)$, define  \n",
    "$$\n",
    "X_t = g(Z_t, Z_{t-1}, \\ldots, Z_{t-q})\n",
    "$$  \n",
    "for some real-valued function $g$. Then $\\{X_t\\}$ is strictly stationary and also $q$-dependent; i.e., $X_s$ and $X_t$ are independent whenever $|t - s| > q$.\n",
    "\n",
    "\n",
    "A process $\\{X_t\\}$ is said to be a **Gaussian process** if the $n$-dimensional vector $\\mathbf{X} = (X_{t_1}, \\ldots, X_{t_n})$, for every collection of time points $t_1, \\ldots, t_n$, and every positive integer $n$, has a multivariate normal distribution.\n",
    "\n",
    "\n",
    "**Lemma 2.1.** For Gaussian processes, weak stationarity is equivalent to strict stationarity.\n",
    "\n",
    "*Proof.* It suffices to show that every weakly stationary Gaussian process $\\{X_t\\}$ is strictly stationary. Suppose it is not, then there must exist $(t_1, t_2)^\\top$ and $(t_1 + h, t_2 + h)^\\top$ such that $(X_{t_1}, X_{t_2})^\\top$ and $(X_{t_1+h}, X_{t_2+h})^\\top$ have different distributions, which contradicts the assumption of weak stationarity.\n",
    "\n",
    "\n",
    "In the following, unless indicated specifically, **stationary** always refers to weak stationarity.\n",
    "\n",
    "Note, when $\\{X_t\\}$ is stationary, $r_X(t + h, h)$ can be written as $\\gamma_X(h)$ for simplicity since $\\gamma_X(t + h, h)$ does not depend on $t$ for any given $h$.\n",
    "\n",
    "Let $\\{X_t\\}$ be a stationary time series. Its mean is $\\mu_X = \\mu_X(t)$. Its autocovariance function (ACVF) of $\\{X_t\\}$ at lag $h$ is  \n",
    "$$\n",
    "\\gamma_X(h) = \\text{Cov}(X_{t+h}, X_t).\n",
    "$$  \n",
    "Its autocorrelation function (ACF) of $\\{X_t\\}$ at lag $h$ is  \n",
    "$$\n",
    "\\rho_X(h) = \\frac{\\gamma_X(h)}{\\gamma_X(0)} = \\text{Corr}(X_{t+h}, X_t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.2.** Basic properties of $\\gamma_X(\\cdot)$:  \n",
    "1. $\\gamma_X(0) \\geq 0$;  \n",
    "2. $|\\gamma_X(h)| \\leq \\gamma_X(0)$ for all $h$;  \n",
    "3. $\\gamma_X(h) = \\gamma_X(-h)$ for all $h$;  \n",
    "4. $\\gamma_X$ is nonnegative definite; i.e., a real valued function $K$ defined on the integers is nonnegative definite if and only if  \n",
    "$$\n",
    "\\sum_{i,j=1}^n a_i K(i-j) a_j \\geq 0\n",
    "$$  \n",
    "for all positive integers $n$ and real vectors $a = (a_1, \\ldots, a_n)^\\top \\in \\mathbb{R}^n$.\n",
    "\n",
    "*Proof.* The first one is trivial since  \n",
    "$$\n",
    "\\gamma_X(0) = \\mathrm{Cov}(X_t, X_t) = \\mathrm{Var}(X_t) \\geq 0 \\quad \\text{for all } t.\n",
    "$$  \n",
    "The second is based on the Cauchy–Schwarz inequality:  \n",
    "$$\n",
    "|\\gamma_X(h)| = |\\mathrm{Cov}(X_{t+h}, X_t)| \\leq \\sqrt{\\mathrm{Var}(X_{t+h})} \\sqrt{\\mathrm{Var}(X_t)} = \\gamma_X(0).\n",
    "$$  \n",
    "The third one is established by observing that  \n",
    "$$\n",
    "\\gamma_X(h) = \\mathrm{Cov}(X_{t+h}, X_t) = \\mathrm{Cov}(X_t, X_{t+h}) = \\gamma_X(-h).\n",
    "$$  \n",
    "The last statement can be verified by  \n",
    "$$\n",
    "0 \\leq \\mathrm{Var}\\big(a^\\top X_n\\big) = a^\\top \\Gamma_n a = \\sum_{i,j=1}^n a_i \\gamma_X(i-j) a_j\n",
    "$$  \n",
    "where  \n",
    "$$\n",
    "X_n = (X_n, \\ldots, X_1)^\\top\n",
    "$$  \n",
    "and  \n",
    "$$\n",
    "\\Gamma_n = \\mathrm{Var}(X_n) = \n",
    "\\begin{pmatrix}\n",
    "\\mathrm{Cov}(X_n, X_n) & \\mathrm{Cov}(X_n, X_{n-1}) & \\cdots & \\mathrm{Cov}(X_n, X_2) & \\mathrm{Cov}(X_n, X_1) \\\\\n",
    "\\mathrm{Cov}(X_{n-1}, X_n) & \\mathrm{Cov}(X_{n-1}, X_{n-1}) & \\cdots & \\mathrm{Cov}(X_{n-1}, X_2) & \\mathrm{Cov}(X_{n-1}, X_1) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\mathrm{Cov}(X_2, X_n) & \\mathrm{Cov}(X_2, X_{n-1}) & \\cdots & \\mathrm{Cov}(X_2, X_2) & \\mathrm{Cov}(X_2, X_1) \\\\\n",
    "\\mathrm{Cov}(X_1, X_n) & \\mathrm{Cov}(X_1, X_{n-1}) & \\cdots & \\mathrm{Cov}(X_1, X_2) & \\mathrm{Cov}(X_1, X_1)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\gamma_X(0) & \\gamma_X(1) & \\cdots & \\gamma_X(n-2) & \\gamma_X(n-1) \\\\\n",
    "\\gamma_X(1) & \\gamma_X(0) & \\cdots & \\gamma_X(n-3) & \\gamma_X(n-2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\gamma_X(n-2) & \\gamma_X(n-3) & \\cdots & \\gamma_X(0) & \\gamma_X(1) \\\\\n",
    "\\gamma_X(n-1) & \\gamma_X(n-2) & \\cdots & \\gamma_X(1) & \\gamma_X(0)\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "**Remark 2.1.** An autocorrelation function $\\rho(\\cdot)$ has all the properties of an autocovariance function and satisfies the additional condition $\\rho(0) = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.3.** A real-valued function defined on the integers is the autocovariance function of a stationary time series if and only if it is even and non-negative definite.\n",
    "\n",
    "*Proof.* We only need prove that for any even and non-negative definite $K(\\cdot)$, we can find a stationary process $\\{X_t\\}$ such that  \n",
    "$$\n",
    "\\gamma_X(h) = K(h)\n",
    "$$  \n",
    "for any integer $h$. It is quite trivial to choose $\\{X_t\\}$ to be a Gaussian process such that  \n",
    "$$\n",
    "\\mathrm{Cov}(X_i, X_j) = K(i - j)\n",
    "$$  \n",
    "for any $i$ and $j$.\n",
    "\n",
    "\n",
    "### 2.1.1 Examples\n",
    "\n",
    "**Example 2.2.** Consider  \n",
    "$$\n",
    "X_t = A \\cos(\\theta t) + B \\sin(\\theta t)\n",
    "$$  \n",
    "where $A$ and $B$ are two uncorrelated random variables with zero means and unit variances with $\\theta \\in [-\\pi, \\pi]$. Then  \n",
    "$$\n",
    "\\mu_X(t) = 0\n",
    "$$  \n",
    "and  \n",
    "\n",
    "\\begin{aligned}\n",
    "\\gamma_X(t + h, t) &= \\mathbb{E}(X_{t+h} X_t) \\\\\n",
    "&= \\mathbb{E}\\big[ (A \\cos(\\theta t + \\theta h) + B \\sin(\\theta t + \\theta h)) (A \\cos(\\theta t) + B \\sin(\\theta t)) \\big] \\\\\n",
    "&= \\cos(\\theta t + \\theta h) \\cos(\\theta t) + \\sin(\\theta t + \\theta h) \\sin(\\theta t) \\\\\n",
    "&= \\cos(\\theta t + \\theta h - \\theta t) = \\cos(\\theta h),\n",
    "\\end{aligned}\n",
    "which is free of $t$. Thus $\\{X_t\\}$ is a stationary process. Further,  \n",
    "$$\n",
    "\\rho_X(h) = \\cos(\\theta h).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2.3.** For white noise $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$, we have  \n",
    "$\\mu_W = 0$,  \n",
    "$\\gamma_W(h) = \\begin{cases}\n",
    "\\sigma^2 & \\text{if } h = 0; \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{cases}$  \n",
    "\n",
    "$\\rho_W(h) = \\begin{cases}\n",
    "1 & \\text{if } h = 0; \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the autocorrelation function\n",
    "rho <- function(h) {\n",
    "  I(h == 0) * 1  # Indicator function: 1 when h == 0, else 0\n",
    "}\n",
    "\n",
    "# Create a sequence of lag values\n",
    "h <- seq(-5, 5, 1)\n",
    "\n",
    "# Sequence index for each h\n",
    "s <- seq_along(h)\n",
    "\n",
    "# Evaluate the rho function for each h\n",
    "y <- rho(h)\n",
    "\n",
    "# Set plot background color\n",
    "par(bg = \"white\")\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot(h, y, type = \"h\", col = \"blue\", \n",
    "     xlab = \"h\", ylab = expression(rho[X](h)), \n",
    "     main = \"Autocorrelation Function\")\n",
    "\n",
    "# Add vertical segments from x-axis to the values\n",
    "segments(h[s], y[s], h[s], 0, col = \"blue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2.4. (Mean Function of a Three-Point Moving Average Smoother).**  \n",
    "See Example 1.10. We have  \n",
    "$X_t = \\frac{1}{3}(W_{t-1} + W_t + W_{t+1})$,  \n",
    "where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$. Then\n",
    "\n",
    "$\\mu_X(t) = \\mathbb{E}(X_t) = \\frac{1}{3}[\\mathbb{E}(W_{t-1}) + \\mathbb{E}(W_t) + \\mathbb{E}(W_{t+1})] = 0$,\n",
    "\n",
    "$\\gamma_X(t + h, t) = \\frac{3}{9} \\sigma^2 \\, I(h = 0) + \\frac{2}{9} \\sigma^2 \\, I(|h| = 1) + \\frac{1}{9} \\sigma^2 \\, I(|h| = 2)$\n",
    "\n",
    "does not depend on $t$ for any $h$. Thus, $\\{X_t\\}$ is stationary. Further,\n",
    "\n",
    "$\\rho_X(h) = I(h = 0) + \\frac{2}{3} I(|h| = 1) + \\frac{1}{3} I(|h| = 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the autocorrelation function with finite support\n",
    "rho <- function(h, theta) {\n",
    "  I(h == 0) + \n",
    "    (2 / 3) * I(abs(h) == 1) + \n",
    "    (1 / 3) * I(abs(h) == 2)\n",
    "}\n",
    "\n",
    "# Sequence of lag values from -5 to 5\n",
    "h <- seq(-5, 5, 1)\n",
    "\n",
    "# Index for each lag value\n",
    "s <- seq_along(h)\n",
    "\n",
    "# Evaluate the autocorrelation function at each h (theta unused here)\n",
    "y <- rho(h, 0.6)\n",
    "\n",
    "# Set plot background to white\n",
    "par(bg = \"white\")\n",
    "\n",
    "# Plot autocorrelation function\n",
    "plot(h, y, type = \"h\", col = \"blue\",\n",
    "     xlab = \"h\", ylab = expression(rho[X](h)),\n",
    "     main = \"Autocorrelation Function with Finite Support\")\n",
    "\n",
    "# Optional: Reinforce vertical segments from the x-axis to each point\n",
    "segments(h[s], y[s], h[s], 0, col = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2.5. MA(1) Process (First-Order Moving Average):**  \n",
    "$X_t = W_t + \\theta W_{t-1}, \\quad t = 0, \\pm1, \\pm2, \\ldots$,  \n",
    "where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$ and $\\theta$ is a constant. Then\n",
    "\n",
    "$\\mu_X(t) = 0$\n",
    "\n",
    "$\\gamma_X(h) = \\sigma^2(1 + \\theta^2) \\, I(h = 0) + \\theta \\sigma^2 \\, I(|h| = 1)$\n",
    "\n",
    "$\\rho_X(h) = I(h = 0) + \\frac{\\theta}{1 + \\theta^2} \\, I(|h| = 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the autocorrelation function with parameter theta\n",
    "rho <- function(h, theta) {\n",
    "  I(h == 0) + \n",
    "    (theta / (1 + theta^2)) * I(abs(h) == 1)\n",
    "}\n",
    "\n",
    "# Sequence of lag values from -5 to 5\n",
    "h <- seq(-5, 5, 1)\n",
    "\n",
    "# Index for each h (used for plotting segments)\n",
    "s <- seq_along(h)\n",
    "\n",
    "# Evaluate the autocorrelation function for theta = 0.6\n",
    "y <- rho(h, 0.6)\n",
    "\n",
    "# Set plot background to white\n",
    "par(bg = \"white\")\n",
    "\n",
    "# Plot autocorrelation values\n",
    "plot(h, y, type = \"h\", col = \"blue\",\n",
    "     xlab = \"h\", ylab = expression(rho[X](h)),\n",
    "     main = expression(paste(\"Autocorrelation for \", theta == 0.6)))\n",
    "\n",
    "# Draw vertical segments from x-axis to each point\n",
    "segments(h[s], y[s], h[s], 0, col = \"blue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2.6. AR(1) Model (Autoregression of Order 1):**  \n",
    "Consider the following model:  \n",
    "$X_t = \\phi X_{t-1} + W_t, \\quad t = 0, \\pm1, \\pm2, \\ldots$,  \n",
    "where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$ and $W_t$ is uncorrelated with $X_s$ for $s < t$.  \n",
    "Assume that $\\{X_t\\}$ is stationary and $0 < |\\phi| < 1$. Then,\n",
    "\n",
    "$\\mu_X = \\phi \\mu_X \\Rightarrow \\mu_X = 0$\n",
    "\n",
    "Further, for $h > 0$,\n",
    "\n",
    "$\\gamma_X(h) = \\mathbb{E}(X_t X_{t-h}) = \\mathbb{E}(\\phi X_{t-1} X_{t-h} + W_t X_{t-h})$  \n",
    "$= \\phi \\mathbb{E}(X_{t-1} X_{t-h}) + 0 = \\phi \\, \\text{Cov}(X_{t-1}, X_{t-h})$  \n",
    "$= \\phi \\gamma_X(h - 1) = \\cdots = \\phi^h \\gamma_X(0)$\n",
    "\n",
    "And,\n",
    "\n",
    "$\\gamma_X(0) = \\text{Cov}(\\phi X_{t-1} + W_t, \\phi X_{t-1} + W_t)$  \n",
    "$= \\phi^2 \\gamma_X(0) + \\sigma^2 \\Rightarrow \\gamma_X(0) = \\frac{\\sigma^2}{1 - \\phi^2}$\n",
    "\n",
    "Further, we have $\\gamma_X(h) = \\gamma_X(-h)$, and\n",
    "\n",
    "$\\rho_X(h) = \\phi^{|h|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define the autocorrelation function for an AR(1) process\n",
    "rho <- function(h, phi) {\n",
    "  phi^(abs(h))  # Exponential decay based on absolute lag\n",
    "}\n",
    "\n",
    "# Sequence of lag values from -5 to 5\n",
    "h <- seq(-5, 5, 1)\n",
    "\n",
    "# Index for each h (used for plotting segments)\n",
    "s <- seq_along(h)\n",
    "\n",
    "# Evaluate autocorrelation for phi = 0.6\n",
    "y <- rho(h, 0.6)\n",
    "\n",
    "# Set the background color of the plot\n",
    "par(bg = \"white\")\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot(h, y, type = \"h\", col = \"blue\",\n",
    "     xlab = \"h\", ylab = expression(rho[X](h)),\n",
    "     main = expression(paste(\"Autocorrelation for \", phi == 0.6)))\n",
    "\n",
    "# Draw vertical segments from x-axis to each autocorrelation point\n",
    "segments(h[s], y[s], h[s], 0, col = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2.7. (Mean Function of a Random Walk with Drift):**  \n",
    "See Example 1.12. We have:  \n",
    "$X_t = \\delta t + \\sum_{j=1}^t W_j, \\quad t = 1, 2, \\ldots$,  \n",
    "where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$.\n",
    "\n",
    "Then,  \n",
    "$\\mu_X(t) = \\mathbb{E}(X_t) = \\delta t$.\n",
    "\n",
    "Obviously, when $\\delta \\ne 0$, $\\{X_t\\}$ is **not stationary**, since its mean is not constant.  \n",
    "Further, if $\\delta = 0$,\n",
    "\n",
    "$\\gamma_X(t + h, t) = \\text{Cov}\\left(\\sum_{j=1}^{t+h} W_j, \\sum_{j=1}^t W_j\\right) = \\min\\{t + h, t\\} \\cdot \\sigma^2$\n",
    "\n",
    "This is, again, not free of $t$. Thus, $\\{X_t\\}$ is **not stationary for any** $\\delta$.\n",
    "\n",
    "**Example 2.8. The MA(q) Process:**  \n",
    "$\\{X_t\\}$ is a moving-average process of order $q$ if\n",
    "\n",
    "$X_t = W_t + \\theta_1 W_{t-1} + \\cdots + \\theta_q W_{t-q}$,\n",
    "\n",
    "where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$ and $\\theta_1, \\ldots, \\theta_q$ are constants.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\mu_X(t) = 0$\n",
    "\n",
    "$\\gamma_X(h) = \\sigma^2 \\sum_{j=0}^{q - |h|} \\theta_j \\theta_{j + |h|} \\cdot I(|h| \\le q)$\n",
    "\n",
    "**Proposition 2.1.**  \n",
    "If $\\{X_t\\}$ is a stationary $q$-correlated time series (i.e., $\\text{Cov}(X_s, X_t) = 0$ whenever $|s - t| > q$) with mean $0$, then it can be represented as an MA($q$) process.\n",
    "\n",
    "**Proof.** See Proposition 3.2.1 on page 89 of *Brockwell and Davis* (2009, *Time Series: Theory and Methods*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Identify Non-Stationary Time Series\n",
    "\n",
    "After learning about stationary time series, one natural question is: **What kind of time series is not stationary?**  \n",
    "Plotting the data is always helpful for identifying whether a time series is stationary.\n",
    "\n",
    "- Any time series with a **non-constant trend** is **not stationary**.  \n",
    "  For example, if $X_t = m_t + Y_t$ with trend $m_t$ and zero-mean error $Y_t$, then $\\mu_X(t) = m_t$ is **not constant**.\n",
    "\n",
    "As an illustration, consider the time series:  \n",
    "$X_t = 1 + 0.5t + Y_t$,  \n",
    "where $\\{Y_t\\} \\sim \\text{i.i.d. } N(0, 1)$.\n",
    "\n",
    "The following figure plots a realization of this process, clearly demonstrating the presence of a linear trend, and hence non-stationarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "par(mar = c(4, 4, 2, .5))\n",
    "t <- seq(1, 100, 1)\n",
    "tt <- 1 + .05 * t\n",
    "xt <- tt + rnorm(length(t), 0, 2)\n",
    "par(bg = \"white\")\n",
    "plot(t, xt, xlab = \"t\", ylab = expression(X[t]))\n",
    "lines(t, tt, col = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any time series with a **seasonal trend** is also **not stationary**.  \n",
    "  For example, if $X_t = s_t + Y_t$ with seasonal trend $s_t$ and zero-mean error $Y_t$, then $\\mu_X(t) = s_t$ is **not constant**.\n",
    "\n",
    "As an example, consider the time series:  \n",
    "$X_t = 1 + 0.5t + 2 \\cos\\left(\\frac{\\pi t}{5}\\right) + 3 \\sin\\left(\\frac{\\pi t}{3}\\right) + W_t$,  \n",
    "where $\\{Y_t\\} \\sim \\text{i.i.d. } N(0, 1)$.\n",
    "\n",
    "This time series includes both a linear trend and seasonal components (sine and cosine terms), making it **non-stationary**. A plot of this realization would clearly illustrate periodic behavior and a changing mean over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "par(mar = c(4, 4, 2, .5))\n",
    "t <- seq(1, 100, 1)\n",
    "tt <- 1 + .05 * t\n",
    "st <- 2 * cos(pi * t / 5) + 3 * sin(pi * t / 3)\n",
    "xt <- tt + st + rnorm(length(t), 0, 2)\n",
    "par(bg = \"white\")\n",
    "plot(t, xt, xlab = \"t\", ylab = expression(X[t]))\n",
    "lines(t, tt + st, col = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any time series with **non-constant variance** is **not stationary**.  \n",
    "  For example, consider a **random walk** defined as:\n",
    "\n",
    "  $\\{S_t = \\sum_{j=1}^t X_j\\}$,  \n",
    "  where $X_t \\sim \\text{i.i.d. } N(0, 1)$.\n",
    "\n",
    "In this case, although $\\mathbb{E}(S_t) = 0$, the **variance** of $S_t$ is $\\text{Var}(S_t) = t$, which **increases with time**.  \n",
    "Therefore, the variance is **not constant**, and $\\{S_t\\}$ is **not stationary**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(150)\n",
    "par(mar = c(4, 4, 2, .5))\n",
    "t <- seq(1, 200, by = 1)\n",
    "xt1 <- rnorm(length(t), 0, 1)\n",
    "par(bg = \"white\")\n",
    "plot(c(0, t), c(0, cumsum(xt1)), type = \"o\", col = \"blue\", xlab = \"t\", ylab = expression(S[t]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way you may have already figured out to identify stationarity is by examining the **shape of the autocorrelation function (ACF)**. However, in practice, you can **never know the true ACF**. Therefore, a **sample version** of the ACF is often used as an approximation. In the following, we will present the estimators of the mean $\\mu_X$, the autocovariance function (ACVF), and the autocorrelation function (ACF). Later, we will introduce the **asymptotic properties** of these estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For observations $x_1, \\ldots, x_n$ of a time series, the **sample mean** is\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{t=1}^n x_t.\n",
    "$$\n",
    "\n",
    "The **sample autocovariance function** is\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_X(h) = \\frac{1}{n} \\sum_{t=1}^{n - |h|} (x_{t+|h|} - \\bar{x})(x_t - \\bar{x}), \\quad \\text{for } -n < h < n.\n",
    "$$\n",
    "\n",
    "This is like the sample covariance of $(x_1, x_{h+1}), \\ldots, (x_{n - h}, x_n)$, except that\n",
    "\n",
    "- we normalize it by $n$ instead of $n - h$,\n",
    "- we subtract the **full sample mean** $\\bar{x}$.\n",
    "\n",
    "This setting ensures that the sample covariance matrix  \n",
    "$$\n",
    "\\hat{\\Gamma}_n = [\\hat{\\gamma}_X(i - j)]_{i,j=1}^n\n",
    "$$  \n",
    "is **nonnegative definite**.\n",
    "\n",
    "The **sample autocorrelation function (sample ACF)** is\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_X(h) = \\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}, \\quad \\text{for } -n < h < n.\n",
    "$$\n",
    "\n",
    "The sample ACF can help us recognize many **non-white** (even **non-stationary**) time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some guidelines for the Sample ACF of different time series types:**\n",
    "\n",
    "| Time Series Type | Sample ACF Behavior              |\n",
    "|------------------|--------------------------------|\n",
    "| White noise      | Zero for $\\|h\\| > 0$             |\n",
    "| Trend            | Slow decay                     |\n",
    "| Periodic         | Periodic                       |\n",
    "| MA($q$)          | Zero for $\\|h\\| > q$             |\n",
    "| AR(1)            | Decays to zero exponentially   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "par(mfrow = c(5, 2))\n",
    "par(mar = c(4, 4, 2, 1))\n",
    "par(bg = \"white\")\n",
    "par(cex.main = 2,   # Title size\n",
    "    cex.lab = 1.4,    # Axis labels size\n",
    "    cex.axis = 2)   # Tick labels size\n",
    "\n",
    "\n",
    "options(repr.plot.width = 20, repr.plot.height = 20)  # Works in Jupyter notebooks or RStudio\n",
    "\n",
    "# White Noise\n",
    "wn <- rnorm(100, 0, 1)\n",
    "n <- length(wn)\n",
    "plot(1:n, wn, type = \"o\", col = \"blue\", main = \"White Noise\", ylab = expression(X[t]), xlab = \"t\")\n",
    "acf(wn)\n",
    "\n",
    "# Trend\n",
    "t <- seq(1, 100, 1)\n",
    "tt <- 1 + .1 * t\n",
    "xt <- tt + rnorm(length(t), 0, 4)\n",
    "plot(t, xt, xlab = \"t\", ylab = expression(X[t]), main = \"Trend\")\n",
    "lines(t, tt, col = \"blue\")\n",
    "acf(xt)\n",
    "\n",
    "# Periodic\n",
    "t <- seq(1, 150, 1)\n",
    "st <- 2 * cos(pi * t / 5) + 3 * sin(pi * t / 3)\n",
    "xt <- st + rnorm(length(t), 0, 2)\n",
    "plot(t, xt, xlab = \"t\", ylab = expression(X[t]), main = \"Periodic\")\n",
    "lines(t, st, col = \"blue\")\n",
    "acf(xt)\n",
    "\n",
    "# MA(1)\n",
    "w <- rnorm(550, 0, 1)\n",
    "v <- filter(w, sides = 1, c(1, .6))[-(1:50)]\n",
    "plot.ts(v, main = \"MA(1)\", col = \"blue\", ylab = expression(X[t]), xlab = \"t\")\n",
    "acf(v)\n",
    "\n",
    "# AR(1)\n",
    "w <- rnorm(550, 0, 1)\n",
    "x <- filter(w, filter = c(.6), method = \"recursive\")[-(1:50)]\n",
    "plot.ts(x, main = \"AR(1)\", col = \"blue\", ylab = expression(X[t]), xlab = \"t\")\n",
    "acf(x)\n",
    "\n",
    "options(repr.plot.width = NULL, repr.plot.height = NULL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical procedure of time series modeling can be described as:\n",
    "\n",
    "1. Plot the time series (look for trends, seasonal components, step changes, outliers).\n",
    "2. Transform data so that residuals are stationary.\n",
    "   - (a) Estimate and subtract $m_t$, $s_t$.\n",
    "   - (b) Differencing.\n",
    "   - (c) Nonlinear transformations (e.g., $\\log$, $\\sqrt{\\cdot}$).\n",
    "3. Fit model to residuals.\n",
    "\n",
    "Now, we introduce the **difference operator** $\\nabla$ and the **backshift operator** $B$.\n",
    "\n",
    "- Define the lag-1 difference operator (think ‘first derivative’):\n",
    "\n",
    "$$\n",
    "\\nabla X_t = X_t - X_{t-1} = (1 - B) X_t,\n",
    "$$\n",
    "\n",
    "where $B$ is the backshift operator defined as $B X_t = X_{t-1}$.\n",
    "\n",
    "- Define the lag-$s$ difference operator:\n",
    "\n",
    "$$\n",
    "\\nabla_s X_t = X_t - X_{t-s} = (1 - B^s) X_t,\n",
    "$$\n",
    "\n",
    "where $B^s$ is the backshift operator applied $s$ times, i.e., $B^s X_t = B(B^{s-1} X_t)$ and $B^1 X_t = B X_t$.\n",
    "\n",
    "**Note that:**\n",
    "\n",
    "- If $X_t = \\beta_0 + \\beta_1 t + Y_t$, then\n",
    "\n",
    "$$\n",
    "\\nabla X_t = \\beta_1 + \\nabla Y_t.\n",
    "$$\n",
    "\n",
    "- If $X_t = \\sum_{i=0}^k \\beta_i t^i + Y_t$, then\n",
    "\n",
    "$$\n",
    "\\nabla^k X_t = k! \\beta_k + \\nabla^k Y_t,\n",
    "$$\n",
    "\n",
    "where $\\nabla^k X_t = \\nabla (\\nabla^{k-1} X_t)$ and $\\nabla^1 X_t = \\nabla X_t$.\n",
    "\n",
    "- If $X_t = m_t + s_t + Y_t$ and $s_t$ has period $s$ (i.e., $s_t = s_{t-s}$ for all $t$), then\n",
    "\n",
    "$$\n",
    "\\nabla_s X_t = m_t - m_{t-s} + \\nabla_s Y_t.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Linear Processes\n",
    "\n",
    "Every, in practice not in general, second-order (or weakly) stationary process is either a linear process or can be transformed into a linear process by subtracting a deterministic component, which will be discussed later.\n",
    "\n",
    "The time series $\\{X_t\\}$ is a **linear process** if it has the representation\n",
    "\n",
    "$$\n",
    "X_t = \\mu + \\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-j},\n",
    "$$\n",
    "\n",
    "for all $t$, where $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$, $\\mu$ is a constant, and $\\{\\psi_j\\}$ is a sequence of constants with\n",
    "\n",
    "$$\n",
    "\\sum_{j=-\\infty}^{\\infty} |\\psi_j| < \\infty.\n",
    "$$\n",
    "\n",
    "If we define the operator\n",
    "\n",
    "$$\n",
    "\\psi(B) = \\sum_{j=-\\infty}^{\\infty} \\psi_j B^j,\n",
    "$$\n",
    "\n",
    "then the linear process can be written as\n",
    "\n",
    "$$\n",
    "X_t = \\mu + \\psi(B) W_t.\n",
    "$$\n",
    "\n",
    "Note that the condition $\\sum_{j=-\\infty}^{\\infty} |\\psi_j| < \\infty$ ensures that $X_t$ is well-defined; i.e., $|X_t| < \\infty$ almost surely. Since $E|W_t| \\leq \\sigma$ for all $t$, we have\n",
    "\n",
    "$$\n",
    "P(|X_t| \\geq \\alpha) \\leq \\frac{1}{\\alpha} E|X_t| \\leq \\frac{1}{\\alpha} \\left(|\\mu| + \\sum_{j=-\\infty}^\\infty |\\psi_j| E|W_{t-j}|\\right) \\leq \\frac{1}{\\alpha} \\left(|\\mu| + \\sigma \\sum_{j=-\\infty}^\\infty |\\psi_j|\\right) \\to 0 \\quad \\text{as } \\alpha \\to \\infty.\n",
    "$$\n",
    "\n",
    "Before proceeding, we provide a brief introduction to several types of convergence in statistics.\n",
    "\n",
    "- We say a sequence of random variables $X_n$ converges **in mean square** to a random variable $X$ (denoted by $X_n \\xrightarrow{L^2} X$) if\n",
    "\n",
    "$$\n",
    "E\\big((X_n - X)^2\\big) \\to 0 \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "- More generally, we have convergence **in $r$-th mean**, denoted by $X_n \\xrightarrow{L^r} X$, if\n",
    "\n",
    "$$\n",
    "E\\big(|X_n - X|^r\\big) \\to 0 \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "- We say $X_n$ converges **in probability** to $X$, denoted by $X_n \\xrightarrow{p} X$, if\n",
    "\n",
    "$$\n",
    "P(|X_n - X| > a) \\to 0, \\quad \\forall a > 0, \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "- $X_n$ converges **in distribution** to $X$, denoted by $X_n \\xrightarrow{d} X$, if\n",
    "\n",
    "$$\n",
    "F_n(x) \\to F(x) \\quad \\text{as } n \\to \\infty,\n",
    "$$\n",
    "\n",
    "at the continuity points of the distribution function $F(\\cdot)$.\n",
    "\n",
    "- The last one is **almost sure convergence**, denoted by $X_n \\xrightarrow{a.s.} X$ (which will not be used).\n",
    "\n",
    "The relationships between these convergences, for $r > 2$, are:\n",
    "\n",
    "$$\n",
    "L^r \\text{ convergence} \\implies L^2 \\text{ convergence} \\implies \\text{convergence in probability} \\implies \\text{convergence in distribution}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on convergence in mean square. One easy way to prove this convergence is through the use of the following theorem:\n",
    "\n",
    "**Theorem 2.4 (Riesz-Fisher Theorem, Cauchy criterion).**  \n",
    "$X_n$ converges in mean square if and only if  \n",
    "$$\n",
    "\\lim_{m,n \\to \\infty} E\\big((X_m - X_n)^2\\big) = 0.\n",
    "$$\n",
    "\n",
    "**Example 2.9.** Consider the linear process  \n",
    "$$\n",
    "X_t = \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j},\n",
    "$$  \n",
    "where if $\\sum_{j=-\\infty}^\\infty |\\psi_j| < \\infty$, then $\\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j}$ converges in mean square.\n",
    "\n",
    "**Proof.** Defining  \n",
    "$$\n",
    "S_n = \\sum_{j=-n}^n \\psi_j W_{t-j},\n",
    "$$  \n",
    "we have  \n",
    "$$\n",
    "E\\big((S_m - S_n)^2\\big) = E\\left( \\sum_{m \\le j \\le n} \\psi_j W_{t-j} \\right)^2 = \\sum_{m \\le j \\le n} \\psi_j^2 \\sigma^2 \\le \\sigma^2 \\left(\\sum_{m \\le j \\le n} |\\psi_j|\\right)^2 \\to 0 \\quad \\text{as } m,n \\to \\infty.\n",
    "$$\n",
    "\n",
    "**Lemma 2.2.** The linear process $\\{X_t\\}$ defined above is stationary with\n",
    "\n",
    "$$\n",
    "\\mu_X = \\mu \\tag{2.1}\n",
    "$$\n",
    "\n",
    "and covariance function\n",
    "\n",
    "$$\n",
    "\\gamma_X(h) = \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_{j+h} \\psi_j. \\tag{2.2}\n",
    "$$\n",
    "\n",
    "**Proof.** Equation (2.1) is trivial. For (2.2), we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_X(h) &= E \\left[ \\left( \\sum_{j=-\\infty}^\\infty \\psi_j W_{t+h-j} \\right) \\left( \\sum_{k=-\\infty}^\\infty \\psi_k W_{t-k} \\right) \\right] \\\\\n",
    "&= \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty \\psi_j \\psi_k E(W_{t+h-j} W_{t-k}) \\\\\n",
    "&= \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty \\psi_j \\psi_k \\gamma_W(h - j + k) \\\\\n",
    "&= \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty \\psi_j \\psi_k I(k = j - h) \\sigma^2 \\\\\n",
    "&= \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_j \\psi_{j - h} = \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_{j+h} \\psi_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Proposition 2.2.** Let $\\{Y_t\\}$ be a stationary time series with mean 0 and covariance function $\\gamma_Y$. If\n",
    "\n",
    "$$\n",
    "\\sum_{j=-\\infty}^\\infty |\\psi_j| < \\infty,\n",
    "$$\n",
    "\n",
    "then the time series\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=-\\infty}^\\infty \\psi_j Y_{t-j} = \\psi(B) Y_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is stationary with mean 0 and autocovariance function  \n",
    "$$\n",
    "\\gamma_X(h) = \\sum_{j=-\\infty}^\\infty \\sum_{k=-\\infty}^\\infty \\psi_j \\psi_k \\gamma_Y(h - j + k).\n",
    "$$\n",
    "\n",
    "It can be easily seen that white noise, MA(1), AR(1), MA(q) and MA(∞) are all special examples of linear processes.\n",
    "\n",
    "- **White noise:** choose $\\mu$, and $\\psi_j = I(j=0)$, then  \n",
    "  $$\n",
    "  X_t = \\mu + \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j} = \\mu + W_t \\sim WN(\\mu, \\sigma^2).\n",
    "  $$\n",
    "\n",
    "- **MA(1):** choose $\\mu = 0$, $\\psi_j = I(j=0) + \\theta I(j=1)$, then  \n",
    "  $$\n",
    "  X_t = \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j} = W_t + \\theta W_{t-1}.\n",
    "  $$\n",
    "\n",
    "- **AR(1):** choose $\\mu = 0$, $\\psi_j = \\phi^j I(j \\geq 0)$, then  \n",
    "  $$\n",
    "  X_t = \\sum_{j=0}^\\infty \\phi^j W_{t-j} = W_t + \\phi \\sum_{j=0}^\\infty \\phi^j W_{t-1-j} = W_t + \\phi X_{t-1}.\n",
    "  $$\n",
    "\n",
    "- **MA(q):** choose $\\mu = 0$, $\\psi_j = I(j=0) + \\sum_{k=1}^q \\theta_k I(j=k)$, then  \n",
    "  $$\n",
    "  X_t = \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j} = W_t + \\theta_1 W_{t-1} + \\cdots + \\theta_q W_{t-q}.\n",
    "  $$\n",
    "\n",
    "- **MA(∞):** choose $\\mu = 0$, $\\psi_j = \\sum_{k=0}^\\infty \\theta_k I(j=k)$, then  \n",
    "  $$\n",
    "  X_t = \\sum_{j=0}^\\infty \\psi_j W_{t-j}.\n",
    "  $$\n",
    "\n",
    "### 2.3 AR(1) and AR(p) Processes\n",
    "\n",
    "#### 2.3.1 AR(1) process\n",
    "\n",
    "In this section, we provide closer investigation on the AR(1) process which has been briefly introduced in Example 2.6. An AR(1) process was defined in Example 2.6 as a stationary solution $\\{X_t\\}$ of the equations\n",
    "\n",
    "$$\n",
    "X_t - \\phi X_{t-1} = W_t, \\quad \\text{for all } t, \\tag{2.3}\n",
    "$$\n",
    "\n",
    "where $\\{W_t\\} \\sim WN(0, \\sigma^2)$, and $W_t$ is uncorrelated with $X_s$ for $s < t$.\n",
    "\n",
    "- When $\\phi = 0$, it is trivial that $X_t = W_t$.\n",
    "- When $0 < |\\phi| < 1$, we now show that such a solution exists and is the unique stationary solution of (2.3). In the above, we have already shown that\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=0}^\\infty \\phi^j W_{t-j}. \\tag{2.4}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be found easily through the aid of $\\phi(B) = 1 - \\phi B$ and\n",
    "\n",
    "$$\n",
    "\\pi(B) = \\sum_{j=0}^\\infty \\phi^j B^j.\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "X_t - \\phi X_{t-1} = W_t\n",
    "\\implies \\pi(B)(X_t - \\phi X_{t-1}) = \\pi(B) W_t\n",
    "\\implies \\pi(B) \\phi(B) X_t = \\pi(B) W_t\n",
    "\\implies X_t = \\pi(B) W_t = \\sum_{j=0}^\\infty \\phi^j W_{t-j}.\n",
    "$$\n",
    "\n",
    "The last step is due to\n",
    "\n",
    "$$\n",
    "\\pi(B) \\phi(B) = (1 - \\phi B) \\sum_{j=0}^\\infty \\phi^j B^j = \\sum_{j=0}^\\infty \\phi^j B^j - \\sum_{j=1}^\\infty \\phi^j B^j = 1,\n",
    "$$\n",
    "\n",
    "which is similar to the summation of a geometric series:\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^\\infty \\phi^j B^j = \\sum_{j=0}^\\infty (\\phi B)^j = \\frac{1}{1 - \\phi B}.\n",
    "$$\n",
    "\n",
    "It can be easily seen that this process is stationary with mean 0 and autocovariance function\n",
    "\n",
    "$$\n",
    "\\gamma_X(h) = \\frac{\\sigma^2 \\phi^{|h|}}{1 - \\phi^2},\n",
    "$$\n",
    "\n",
    "which are the same as in Example 2.6.\n",
    "\n",
    "Further, we show this solution is unique. Suppose \\${Y\\_t}\\$ is another stationary solution, then by iterating, we have\n",
    "\n",
    "$$\n",
    "Y_t = \\phi Y_{t-1} + W_t = W_t + \\phi W_{t-1} + \\phi^2 Y_{t-2} = \\cdots = W_t + \\phi W_{t-1} + \\cdots + \\phi^k W_{t-k} + \\phi^{k+1} Y_{t-k-1}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "E\\left( Y_t - \\sum_{j=0}^k \\phi^j W_{t-j} \\right)^2 = \\phi^{2k+2} E(Y_{t-k-1}^2) \\to 0 \\quad \\text{as } k \\to \\infty.\n",
    "$$\n",
    "\n",
    "This implies that $Y\\_t$ is equal to the mean square limit \\$\\sum\\_{j=0}^\\infty \\phi^j W\\_{t-j}\\$, and hence the uniqueness is proved.\n",
    "\n",
    "* When $|\\phi| > 1$, the series defined in (2.4) does not converge. However, we can rewrite (2.3) in the form\n",
    "\n",
    "$$\n",
    "X_t = -\\phi^{-1} W_{t+1} + \\phi^{-1} X_{t+1}.\n",
    "$$\n",
    "\n",
    "By iterating, we have\n",
    "\n",
    "$$\n",
    "X_t = -\\phi^{-1} W_{t+1} - \\cdots - \\phi^{-k-1} W_{t+k+1} + \\phi^{-k-1} X_{t+k+1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which shows that\n",
    "\n",
    "$$\n",
    "X_t = - \\sum_{j=1}^\\infty \\phi^{-j} W_{t+j}\n",
    "$$\n",
    "\n",
    "is the unique stationary solution of (2.3). However, this is very hard to interpret, since \\$X\\_t\\$ is defined to be correlated with future values of \\$Z\\_s\\$. Another way to look at this case is to define a new sequence\n",
    "\n",
    "$$\n",
    "W^*_t = X_t - \\frac{1}{\\phi} X_{t-1} = (\\phi - \\phi^{-1}) X_{t-1} + W_t = -(\\phi - \\phi^{-1}) \\sum_{j=1}^\\infty \\phi^{-j} W_{t-1+j} + W_t = \\frac{1}{\\phi^2} W_t - (1 - \\phi^{-2}) \\sum_{j=1}^\\infty \\phi^{-j} W_{t+j}.\n",
    "$$\n",
    "\n",
    "Standard arguments yield that (left as a homework problem)\n",
    "\n",
    "$$\n",
    "E(W^*_t) = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\gamma_{W^*}(h) = \\frac{\\sigma^2}{\\phi^2} I(h=0);\n",
    "$$\n",
    "\n",
    "i.e., \\${W^\\*\\_t}\\$ is a new white noise sequence with mean 0 and variance \\$\\frac{\\sigma^2}{\\phi^2}\\$, then we have a new AR(1) model\n",
    "\n",
    "$$\n",
    "X_t = \\phi^* X_{t-1} + W^*_t\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "|\\phi^*| = \\frac{1}{|\\phi|} < 1.\n",
    "$$\n",
    "\n",
    "Thus, we can rewrite the unique stationary solution as\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=0}^\\infty (\\phi^*)^j W^*_{t-j},\n",
    "$$\n",
    "\n",
    "which now does not depend on future values. Thus, for an AR(1) model, people typically assume that \\$|\\phi| < 1\\$.\n",
    "\n",
    "* When \\$\\|\\phi\\| = 1\\$. If there is a stationary solution to (2.3), check\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X_{t-1}, W_t) = \\mathrm{Cov}(X_{t-1}, X_t - \\phi X_{t-1}) = \\gamma_X(1) - \\phi \\gamma_X(0) = 0.\n",
    "$$\n",
    "\n",
    "This holds if and only if\n",
    "\n",
    "$$\n",
    "X_t = \\phi X_{t-1} + b\n",
    "$$\n",
    "\n",
    "for some constant \\$b\\$. Then \\${W\\_t = b}\\$ is a constant process. Since \\${W\\_t}\\$ is a white noise, \\$b\\$ has to be zero. Now we have\n",
    "\n",
    "$$\n",
    "X_t = \\phi X_{t-1}.\n",
    "$$\n",
    "\n",
    "When $\\phi = -1$, $X_t$ has to be all zeros. When $\\phi = 1$, then $X_t$ are all constants. So if we require $\\sigma > 0$, then there is no stationary solution; if more broadly, we allow $\\sigma = 0$, i.e., ${W_t = 0}$, then when $\\phi = -1$, there is a stationary solution which is $X_t = 0$; when $\\phi = 1$, there is also a stationary solution that $X_t = \\mu_X$. In the following, we require $\\sigma > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark 2.2.** This example introduced a very important terminology: *causality*. We say that $\\{X_t\\}$ \n",
    "is a causal function of $\\{W_t\\}$, or more concisely that $\\{X_t\\}$ is a causal autoregressive process, \n",
    "if $X_t$ has a representation in terms of $\\{W_s, s \\leq t\\}$; i.e., the current status only relates to the past \n",
    "events, not the future.\n",
    "\n",
    "A linear process $\\{X_t\\}$ is causal (strictly, a causal function of $\\{W_t\\}$), if there is a\n",
    "$$\n",
    "\\psi(B) = \\psi_0 + \\psi_1 B + \\psi_2 B^2 + \\cdots\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\\sum_{j=0}^{\\infty} |\\psi_j| < \\infty\n",
    "$$\n",
    "such that\n",
    "$$\n",
    "X_t = \\psi(B) W_t = \\sum_{j=0}^{\\infty} \\psi_j W_{t-j}.\n",
    "$$\n",
    "\n",
    "- When $|\\phi| < 1$, AR(1) process $\\{X_t\\}$ is a causal function of $\\{W_t\\}$.\n",
    "- When $|\\phi| > 1$, AR(1) process is not causal.\n",
    "\n",
    "**Proposition 2.3.** AR(1) process $\\phi(B) X_t = W_t$ with $\\phi(B) = 1 - \\phi B$ is causal if and only if \n",
    "$|\\phi| < 1$ or the root $z_1$ of the polynomial $\\phi(z) = 1 - \\phi z$ satisfies $|z_1| > 1$.\n",
    "\n",
    "### 2.3.2 AR(p) process\n",
    "\n",
    "An AR(p) process $\\{X_t\\}$ is a stationary process that satisfies\n",
    "$$\n",
    "X_t - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p} = W_t\n",
    "$$\n",
    "where $\\{W_t\\} \\sim WN(0, \\sigma^2)$. Equivalently,\n",
    "$$\n",
    "\\phi(B) X_t = W_t\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\phi(B) = 1 - \\phi_1 B - \\cdots - \\phi_p B^p.\n",
    "$$\n",
    "\n",
    "Recall that, for $p=1$, $\\phi(B) = 1 - \\phi_1 B$, and for this AR(1) model, $X_t$ is stationary only if\n",
    "$$\n",
    "|\\phi_1| \\neq 1.\n",
    "$$\n",
    "This is equivalent to that for any $z \\in \\mathbb{R}$ such that $\\phi(z) = 1 - \\phi z$ satisfies $|z| \\neq 1$, or\n",
    "for any $z \\in \\mathbb{C}$ such that $\\phi(z) = 1 - \\phi z$ satisfies $|z| \\neq 1$.\n",
    "\n",
    "Now for the AR(p) model, similarly, we should have\n",
    "$$\n",
    "\\text{for any } z \\in \\mathbb{C} \\text{ such that } \\phi(z) = 1 - \\phi_1 z - \\cdots - \\phi_p z^p, \\quad |z| \\neq 1.\n",
    "$$\n",
    "\n",
    "**Theorem 2.5.** A (unique) stationary solution to $\\phi(B) X_t = W_t$ exists if and only if\n",
    "$$\n",
    "\\phi(z) = 1 - \\phi_1 z - \\cdots - \\phi_p z^p = 0 \\implies |z| \\neq 1.\n",
    "$$\n",
    "\n",
    "Further, this AR(p) process is causal if and only if\n",
    "$$\n",
    "\\phi(z) = 1 - \\phi_1 z - \\cdots - \\phi_p z^p = 0 \\implies |z| > 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When (2.5) is satisfied, based on causality, we can write  \n",
    "$$\n",
    "X_t = \\psi(B) W_t\n",
    "$$  \n",
    "where  \n",
    "$$\n",
    "\\psi(B) = \\psi_0 + \\psi_1 B + \\psi_2 B^2 + \\cdots\n",
    "$$  \n",
    "for some $\\psi_j$'s satisfying  \n",
    "$$\n",
    "\\sum_{j=0}^{\\infty} |\\psi_j| < \\infty.\n",
    "$$  \n",
    "The question is then, how to calculate the $\\psi_j$'s? One way is by matching the coefficients.\n",
    "\n",
    "From  \n",
    "$$\n",
    "\\phi(B) X_t = W_t \\quad \\text{and} \\quad X_t = \\psi(B) W_t,\n",
    "$$  \n",
    "we get  \n",
    "$$\n",
    "1 = \\psi(B) \\phi(B)\n",
    "$$  \n",
    "which implies  \n",
    "$$\n",
    "1 = (\\psi_0 + \\psi_1 B + \\psi_2 B^2 + \\cdots)(1 - \\phi_1 B - \\cdots - \\phi_p B^p).\n",
    "$$  \n",
    "\n",
    "Expanding and matching coefficients gives  \n",
    "$$\n",
    "\\begin{cases}\n",
    "1 = \\psi_0, \\\\\n",
    "0 = \\psi_1 - \\phi_1 \\psi_0, \\\\\n",
    "0 = \\psi_2 - \\phi_1 \\psi_1 - \\phi_2 \\psi_0, \\\\\n",
    "\\vdots\n",
    "\\end{cases}\n",
    "$$  \n",
    "or equivalently,  \n",
    "$$\n",
    "1 = \\psi_0, \\quad 0 = \\psi_j \\quad (j < 0), \\quad 0 = \\phi(B) \\psi_j \\quad (j > 0).\n",
    "$$  \n",
    "\n",
    "### 2.4 MA(1) and MA(q) Processes\n",
    "\n",
    "Now, we look at the MA(1) process defined in Example 2.5. An MA(1) process $\\{X_t\\}$ is defined as  \n",
    "$$\n",
    "X_t = W_t + \\theta W_{t-1}\n",
    "$$  \n",
    "where $\\{W_t\\} \\sim WN(0, \\sigma^2)$. Obviously, $\\{X_t\\}$ is a causal function of $\\{W_t\\}$. But more importantly, there is another terminology: *invertibility*. Just as causality means that $X_t$ is expressible in terms of $\\{W_s, s \\leq t\\}$, invertibility means that $W_t$ is expressible in terms of $\\{X_s, s \\leq t\\}$.\n",
    "\n",
    "A linear process $\\{X_t\\}$ is invertible (strictly, an invertible function of $\\{W_t\\}$), if there is a  \n",
    "$$\n",
    "\\pi(B) = \\pi_0 + \\pi_1 B + \\pi_2 B^2 + \\cdots\n",
    "$$  \n",
    "with  \n",
    "$$\n",
    "\\sum_{j=0}^\\infty |\\pi_j| < \\infty\n",
    "$$  \n",
    "such that  \n",
    "$$\n",
    "W_t = \\pi(B) X_t = \\sum_{j=0}^\\infty \\pi_j X_{t-j}.\n",
    "$$  \n",
    "\n",
    "Obviously, the AR(1) process is invertible. Back to the MA(1) process:\n",
    "\n",
    "- When $|\\theta| < 1$, we have  \n",
    "$$\n",
    "(1 + \\theta B)^{-1} = \\sum_{j=0}^\\infty (-\\theta)^j B^j.\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus  \n",
    "$$\n",
    "X_t = W_t + \\theta W_{t-1} = (1 + \\theta B) W_t\n",
    "$$  \n",
    "$$\n",
    "\\implies (1 + \\theta B)^{-1} X_t = W_t\n",
    "$$  \n",
    "$$\n",
    "\\iff W_t = \\sum_{j=0}^{\\infty} (-\\theta)^j X_{t-j}.\n",
    "$$  \n",
    "We have $\\{X_t\\}$ as an invertible function of $\\{W_t\\}$.\n",
    "\n",
    "- When $|\\theta| > 1$, the sum $\\sum_{j=0}^\\infty (-\\theta)^j X_{t-j}$ diverges, but we can write  \n",
    "$$\n",
    "W_t = -\\theta^{-1} W_{t+1} + \\theta^{-1} X_{t+1} = \\theta^{-1} X_{t+1} - \\theta^{-2} X_{t+2} + \\theta^{-2} W_{t+2} = \\cdots = - \\sum_{j=1}^\\infty (-\\theta)^{-j} X_{t+j}.\n",
    "$$  \n",
    "Now, MA(1) is not invertible.\n",
    "\n",
    "- When $\\theta = 1$, we have  \n",
    "$$\n",
    "X_t = W_t + W_{t-1}.\n",
    "$$  \n",
    "If we have  \n",
    "$$\n",
    "W_t = \\sum_{j=0}^\\infty \\pi_j X_{t-j},\n",
    "$$  \n",
    "then  \n",
    "$$\n",
    "X_t = \\sum_{j=0}^\\infty \\pi_j X_{t-j} + \\sum_{j=0}^\\infty \\pi_j X_{t-1-j} = \\sum_{j=0}^\\infty \\pi_j X_{t-j} + \\sum_{j=1}^\\infty \\pi_{j-1} X_{t-j} = \\pi_0 X_t + \\sum_{j=1}^\\infty (\\pi_j + \\pi_{j-1}) X_{t-j}.\n",
    "$$  \n",
    "Thus, we have  \n",
    "$$\n",
    "\\pi_j + \\pi_{j-1} = 0 \\quad \\text{and} \\quad \\pi_0 = 1,\n",
    "$$  \n",
    "which means  \n",
    "$$\n",
    "\\pi_j = (-1)^j.\n",
    "$$  \n",
    "Then,  \n",
    "$$\n",
    "\\sum_{j=0}^\\infty |\\pi_j| < \\infty\n",
    "$$  \n",
    "is not possible. So MA(1) is not invertible when $\\theta = 1$; similarly when $\\theta = -1$.\n",
    "\n",
    "One may notice that, similarly as the case of $|\\phi| = 1$ in the AR(1) model, if we allow $\\sigma = 0$, then we have $X_t = 0$ and $W_t = 0 = X_t$ so invertible. But this is a nonsense case. So in the following, we require $\\sigma > 0$.\n",
    "\n",
    "Section 4.4 in Brockwell and Davis (2009, *Time Series Theory and Methods*) defines invertibility in a more general way: that is, if we can express  \n",
    "$$\n",
    "W_t = \\sum_{j=0}^\\infty \\pi_j X_{t-j},\n",
    "$$  \n",
    "it does not require that  \n",
    "$$\n",
    "\\sum_{j=0}^\\infty |\\pi_j| < \\infty.\n",
    "$$  \n",
    "With this more general meaning of invertibility, we have $X_t$ is invertible when $|\\theta| = 1$. In the remaining context, we will keep our more realistic restriction of  \n",
    "$$\n",
    "\\sum_{j=0}^\\infty |\\pi_j| < \\infty.\n",
    "$$\n",
    "\n",
    "**Proposition 2.4.** MA(1) process  \n",
    "$$\n",
    "X_t = \\theta(B) W_t\n",
    "$$  \n",
    "where  \n",
    "$$\n",
    "\\theta(B) = 1 + \\theta B\n",
    "$$  \n",
    "is not invertible if and only if  \n",
    "$$\n",
    "|\\theta| \\geq 1\n",
    "$$  \n",
    "or the root $z_1$ of the polynomial  \n",
    "$$\n",
    "\\theta(z) = 1 + \\theta z\n",
    "$$  \n",
    "satisfies  \n",
    "$$\n",
    "|z_1| \\leq 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.6.** The MA($q$) process  \n",
    "$$\n",
    "X_t = \\pi(B) W_t\n",
    "$$  \n",
    "where  \n",
    "$$\n",
    "\\pi(B) = 1 + \\theta_1 B + \\cdots + \\theta_q B^q\n",
    "$$  \n",
    "is not invertible if and only if  \n",
    "$$\n",
    "\\pi(z) = 1 + \\theta_1 z + \\cdots + \\theta_q z^q = 0 \\implies |z| \\leq 1.\n",
    "$$  \n",
    "\n",
    "Based on invertibility, we can write  \n",
    "$$\n",
    "W_t = \\pi(B) X_t\n",
    "$$  \n",
    "where  \n",
    "$$\n",
    "\\pi(B) = \\pi_0 + \\pi_1 B + \\pi_2 B^2 + \\cdots\n",
    "$$  \n",
    "for some $\\pi_j$'s satisfying  \n",
    "$$\n",
    "\\sum_{j=0}^\\infty |\\pi_j| < \\infty.\n",
    "$$  \n",
    "The question is then, how to calculate the $\\pi_j$'s? One way is by matching the coefficients.\n",
    "\n",
    "From  \n",
    "$$\n",
    "X_t = \\theta(B) W_t \\quad \\text{and} \\quad W_t = \\pi(B) X_t,\n",
    "$$  \n",
    "we get  \n",
    "$$\n",
    "1 = \\pi(B) \\theta(B)\n",
    "$$  \n",
    "which implies  \n",
    "$$\n",
    "1 = (\\pi_0 + \\pi_1 B + \\pi_2 B^2 + \\cdots)(1 + \\theta_1 B + \\cdots + \\theta_p B^p).\n",
    "$$  \n",
    "\n",
    "Matching coefficients gives  \n",
    "$$\n",
    "\\begin{cases}\n",
    "1 = \\pi_0, \\\\\n",
    "0 = \\pi_1 + \\theta_1 \\pi_0, \\\\\n",
    "0 = \\pi_2 + \\theta_1 \\pi_1 + \\theta_2 \\pi_0, \\\\\n",
    "\\vdots\n",
    "\\end{cases}\n",
    "$$  \n",
    "or equivalently,  \n",
    "$$\n",
    "1 = \\pi_0, \\quad 0 = \\pi_j \\ (j < 0), \\quad 0 = \\theta(B) \\pi_j \\ (j > 0).\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 ARMA(1,1) Processes\n",
    "\n",
    "In this subsection we introduce, through an example, some of the key properties of an important class of linear processes known as ARMA (autoregressive moving average) processes. This example is the ARMA(1,1) processes. Higher-order ARMA processes will be discussed later.\n",
    "\n",
    "The time series $\\{X_t\\}$ is an ARMA(1,1) process if it is stationary and satisfies (for every $t$)\n",
    "$$\n",
    "X_t - \\phi X_{t-1} = W_t + \\theta W_{t-1}, \\quad (2.6)\n",
    "$$\n",
    "where $\\{W_t\\} \\sim WN(0, \\sigma^2)$, $\\sigma > 0$, and $\\phi + \\theta \\neq 0$.\n",
    "\n",
    "Let us find the expression of $\\{X_t\\}$ in terms of $\\{W_t\\}$:\n",
    "\n",
    "- When $|\\phi| = 0$, we have the trivial solution\n",
    "$$\n",
    "X_t = W_t + \\theta W_{t-1}.\n",
    "$$\n",
    "\n",
    "- When $0 < |\\phi| < 1$, we have a meaningful definition of \n",
    "$$\n",
    "\\sum_{j=0}^\\infty \\phi^j B^j.\n",
    "$$\n",
    "Then applying it to both sides of (2.6) provides that\n",
    "$$\n",
    "\\sum_{j=0}^\\infty \\phi^j B^j (1 - \\phi B) X_t = X_t = \\left( \\sum_{j=0}^\\infty \\phi^j B^j \\right) (1 + \\theta B) W_t = \\left( \\sum_{j=0}^\\infty \\phi^j B^j + \\theta \\sum_{j=0}^\\infty \\phi^j B^{j+1} \\right) W_t,\n",
    "$$\n",
    "which simplifies to\n",
    "$$\n",
    "X_t = W_t + (\\phi + \\theta) \\sum_{j=1}^\\infty \\phi^{j-1} W_{t-j}. \\quad (2.7)\n",
    "$$\n",
    "\n",
    "This is one MA($\\infty$) process, and of course stationary.\n",
    "\n",
    "For uniqueness, suppose we have another stationary solution $Y_t$, then\n",
    "\\begin{align*}\n",
    "Y_t &= W_t + \\theta W_{t-1} + \\phi Y_{t-1} \\\\\n",
    "&= W_t + (\\theta + \\phi) W_{t-1} + \\theta \\phi W_{t-2} + \\phi^2 Y_{t-2} \\\\\n",
    "&= \\cdots = W_t + (\\theta + \\phi) W_{t-1} + (\\theta + \\phi) \\phi W_{t-2} + \\cdots + (\\theta + \\phi) \\phi^{k-1} W_{t-k} + \\phi^{k+1} Y_{t-k-1}.\n",
    "\\end{align*}\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\mathbb{E}\\left( Y_t - W_t - (\\phi + \\theta) \\sum_{j=1}^k \\phi^{j-1} W_{t-j} \\right)^2 = \\phi^{2k+2} \\mathbb{E}(Y_{t-k-1}^2) \\to 0 \\quad \\text{as} \\quad k \\to \\infty.\n",
    "$$\n",
    "\n",
    "Hence, solution (2.7) is the unique stationary solution of (2.6) provided that $|\\phi| < 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When $|\\phi| > 1$, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_t &= - \\theta \\phi^{-1} W_t - \\phi^{-1} W_{t+1} + \\phi^{-1} X_{t+1} \\\\\n",
    "&= - \\theta \\phi^{-1} W_t - (\\theta + \\phi) \\phi^{-2} W_{t+1} - \\phi^{-2} W_{t+2} + \\phi^{-2} X_{t+2} \\\\\n",
    "&= \\cdots = - \\theta \\phi^{-1} W_t - (\\theta + \\phi) \\sum_{j=1}^k \\phi^{-j-1} W_{t+j} - \\phi^{-k-1} W_{t+k+1} + \\phi^{-k-1} X_{t+k+1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\mathbb{E} \\left( X_t - \\left\\{ -\\theta \\phi^{-1} W_t - (\\theta + \\phi) \\sum_{j=1}^k \\phi^{-j-1} W_{t+j} \\right\\} \\right)^2 = \\phi^{-2k-2} \\mathbb{E}\\left( W_{t+k+1} + X_{t+k+1} \\right)^2 \\to 0 \\quad \\text{as} \\quad k \\to \\infty.\n",
    "$$\n",
    "\n",
    "Thus, we have a unique stationary solution of (2.6) when $|\\phi| > 1$ as\n",
    "$$\n",
    "X_t = -\\theta \\phi^{-1} W_t - (\\theta + \\phi) \\sum_{j=1}^\\infty \\phi^{-j-1} W_{t+j}. \\quad (2.8)\n",
    "$$\n",
    "\n",
    "Again, this solution depends on future values of $W_t$.\n",
    "\n",
    "- When $|\\phi| = 1$, there is no stationary solution of (2.6) (left as a HW problem). Thus, no stationary ARMA$(1,1)$ process when $|\\phi| = 1$.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- A stationary solution of the ARMA$(1,1)$ equations exists if and only if $|\\phi| \\neq 1$.\n",
    "- If $|\\phi| < 1$, then the unique stationary solution is given by (2.7). In this case, we say that $\\{X_t\\}$ is *causal* or a causal function of $\\{W_t\\}$, since $X_t$ can be expressed in terms of the current and past values $\\{W_s, s \\leq t\\}$.\n",
    "- If $|\\phi| > 1$, then the unique stationary solution is given by (2.8). In this case, we say that $\\{X_t\\}$ is *noncausal* since $X_t$ is then a function of current and future values $\\{W_s, s \\geq t\\}$.\n",
    "\n",
    "For invertibility, by switching the roles of $X_t$ and $W_t$, and the roles of $\\phi$ and $\\theta$:\n",
    "\n",
    "- If $|\\theta| < 1$, then the ARMA$(1,1)$ process is invertible as\n",
    "$$\n",
    "W_t = X_t - (\\phi + \\theta) \\sum_{j=1}^\\infty (-\\theta)^{j-1} X_{t-j}.\n",
    "$$\n",
    "\n",
    "- If $|\\theta| > 1$, then the ARMA$(1,1)$ process is noninvertible as\n",
    "$$\n",
    "W_t = - \\phi \\theta^{-1} X_t + (\\theta + \\phi) \\sum_{j=1}^\\infty (-\\theta)^{-j-1} W_{t+j}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $|\\theta| = 1$, the $ARMA(1,1)$ process is invertible under the more general definition of invertibility\n",
    "same as in the $MA(1)$ process. Without this more general setting, we say the $ARMA(1,1)$\n",
    "process is noninvertible when $|\\theta| = 1$.\n",
    "\n",
    "Like the argument in last subsection of AR(1) model, if the $ARMA(1,1)$ process $\\{X_t\\}$ is noncausal\n",
    "and noninvertible; i.e., $|\\phi| > 1$ and $|\\theta| > 1$, then we define\n",
    "$$\n",
    "\\tilde{\\phi}(B) = 1 - \\phi^{-1} B \\quad \\text{and} \\quad \\tilde{\\theta}(B) = 1 + \\theta^{-1} B,\n",
    "$$\n",
    "and let\n",
    "$$\n",
    "W_t^* = \\tilde{\\theta}^{-1}(B) \\tilde{\\phi}(B) X_t.\n",
    "$$\n",
    "\n",
    "Once verifying that\n",
    "$$\n",
    "\\{W_t^*\\} \\sim WN(0, \\sigma_*^2) \\quad \\text{and} \\quad \\tilde{\\phi}(B) X_t = \\tilde{\\theta}(B) W_t^*, \\quad (2.9)\n",
    "$$\n",
    "we have $\\{X_t\\}$ being a causal and invertible $ARMA(1,1)$ process relative to the white noise sequence\n",
    "$\\{W_t^*\\}$. There, from a second-order point of view, nothing is lost by restricting attention to causal\n",
    "and invertible $ARMA(1,1)$ models. This statement is also true for higher-ordered ARMA models.\n",
    "\n",
    "Now, we show (2.9). It is easy to see that\n",
    "$$\n",
    "\\tilde{\\theta}(B) W_t^* = \\tilde{\\theta}(B) \\tilde{\\theta}^{-1}(B) \\tilde{\\phi}(B) X_t = \\tilde{\\phi}(B) X_t.\n",
    "$$\n",
    "It suffices to show $\\{W_t^*\\}$ is a white noise. This is left as a HW problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Properties of $(X_n), (\\hat{\\gamma}_X(h))$ and $(\\hat{\\rho}_X(h))$\n",
    "\n",
    "### 2.6.1 For $(X_n)$\n",
    "\n",
    "Recall that, for observations $(x_1, \\ldots, x_n)$ of a time series, the sample mean is\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{t=1}^n x_t.\n",
    "$$\n",
    "\n",
    "The sample autocovariance function is\n",
    "$$\n",
    "\\hat{\\gamma}_X(h) = \\frac{1}{n} \\sum_{t=1}^{n-|h|} (x_{t+|h|} - \\bar{x})(x_t - \\bar{x}), \\quad \\text{for } -n < h < n.\n",
    "$$\n",
    "\n",
    "The sample autocorrelation function (sample ACF) is\n",
    "$$\n",
    "\\hat{\\rho}_X(h) = \\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}.\n",
    "$$\n",
    "\n",
    "Estimation of $(\\mu_X)$: The moment estimator of the mean $(\\mu_X)$ of a stationary process $(\\{X_t\\})$ is the sample mean\n",
    "$$\n",
    "X_n = \\frac{1}{n} \\sum_{t=1}^n X_t. \\tag{2.10}\n",
    "$$\n",
    "\n",
    "Obviously, it is unbiased; i.e., $(E(X_n) = \\mu_X)$. Its mean squared error is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}(X_n) &= E(X_n - \\mu_X)^2 \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{Cov}(X_i, X_j) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\gamma_X(i-j) \\\\\n",
    "&= \\frac{1}{n^2} \\sum_{i-j=-n}^{n} (n - |i-j|) \\gamma_X(i-j) \\\\\n",
    "&= \\frac{1}{n} \\sum_{h=-n}^n \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h) \\\\\n",
    "&= \\underbrace{\\frac{\\gamma_X(0)}{n}}_{\\text{is Var}(X_n) \\text{ when } \\{X_t\\} \\text{ are iid}} + \\frac{2}{n} \\sum_{h=1}^{n-1} \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Depending on the nature of the correlation structure, the standard error of $(X_n)$ may be smaller or larger than the white noise case.\n",
    "\n",
    "- Consider $(X_t = \\mu + W_t - 0.8 W_{t-1})$, where $(\\{W_t\\} \\sim WN(0, \\sigma^2))$, then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}(X_n) &= \\frac{\\gamma_X(0)}{n} + \\frac{2}{n} \\sum_{h=1}^{n-1} \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h) \\\\\n",
    "&= \\frac{1.64 \\sigma^2}{n} - \\frac{1.6 (n-1) \\sigma^2}{n^2} < \\frac{1.64 \\sigma^2}{n}.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And if $(X_t = \\mu + W_t + 0.8 W_{t-1}), where $(\\{W_t\\} \\sim WN(0, \\sigma^2))$, then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}(X_n) &= \\frac{\\gamma_X(0)}{n} + \\frac{2}{n} \\sum_{h=1}^{n-1} \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h) \\\\\n",
    "&= \\frac{1.64 \\sigma^2}{n} + \\frac{1.6 (n - 1) \\sigma^2}{n^2} > \\frac{1.64 \\sigma^2}{n}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- If $(\\gamma_X(h) \\to 0)$ as $(h \\to \\infty)$, we have\n",
    "$$\n",
    "|\\operatorname{Var}(X_n)| \\leq \\frac{\\gamma_X(0)}{n} + \\frac{2}{n} \\sum_{h=1}^n |\\gamma_X(h)| \\to 0 \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "Thus, $(X_n)$ converges in mean square to $\\mu$.\n",
    "\n",
    "- If $\\sum_{h=-\\infty}^\\infty |\\gamma_X(h)| < \\infty$, then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n \\operatorname{Var}(X_n) &= \\sum_{h=-n}^n \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h) \\\\\n",
    "&= \\gamma_X(0) + \\frac{2}{n} \\sum_{h=1}^n (n - h) \\gamma_X(h) \\\\\n",
    "&= \\gamma_X(0) + \\frac{2}{n} \\sum_{h=1}^{n-1} \\sum_{i=1}^h \\gamma_X(i) \\to \\gamma_X(0) + 2 \\sum_{i=1}^\\infty \\gamma_X(i) \\\\\n",
    "&= \\sum_{h=-\\infty}^\\infty \\gamma_X(h) = \\gamma_X(0) \\sum_{h=-\\infty}^\\infty \\rho_X(h).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One interpretation could be that, instead of $\\operatorname{Var}(X_n) \\approx \\frac{\\gamma_X(0)}{n}$, we have\n",
    "$$\n",
    "\\operatorname{Var}(X_n) \\approx \\frac{\\gamma_X(0)}{n / \\tau} \\quad \\text{with} \\quad \\tau = \\sum_{h=-\\infty}^\\infty \\rho_X(h).\n",
    "$$\n",
    "The effect of the correlation is a reduction of sample size from $n$ to $n/\\tau$.\n",
    "\n",
    "**Example 2.10.** For linear processes, i.e., if\n",
    "$$\n",
    "X_t = \\mu + \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j} \\quad \\text{with} \\quad \\sum_{j=-\\infty}^\\infty |\\psi_j| < \\infty,\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{h=-\\infty}^\\infty |\\gamma_X(h)| &= \\sum_{h=-\\infty}^\\infty \\left| \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_j \\psi_{j+h} \\right| \\\\\n",
    "&\\leq \\sigma^2 \\sum_{h=-\\infty}^\\infty \\sum_{j=-\\infty}^\\infty |\\psi_j| \\cdot |\\psi_{j+h}| \\\\\n",
    "&= \\sigma^2 \\left(\\sum_{j=-\\infty}^\\infty |\\psi_j|\\right) \\left(\\sum_{h=-\\infty}^\\infty |\\psi_{j+h}|\\right) \\\\\n",
    "&= \\sigma^2 \\left(\\sum_{j=-\\infty}^\\infty |\\psi_j|\\right)^2 < \\infty.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To make inference about $\\mu_X$ (e.g., is $\\mu_X = 0$?), using the sample mean $X_n$, it is necessary to know the asymptotic distribution of $X_n$:\n",
    "\n",
    "If $\\{X_t\\}$ is a Gaussian stationary time series, then, for any $n$,\n",
    "$$\n",
    "\\sqrt{n} (X_n - \\mu_X) \\sim N\\left(0, \\sum_{h=-n}^n \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h)\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then one can obtain exact confidence intervals for estimating $\\mu_X$, or approximated confidence intervals if it is necessary to estimate $\\gamma_X(\\cdot)$.\n",
    "\n",
    "For the linear process,\n",
    "$$\n",
    "X_t = \\mu + \\sum_{j=-\\infty}^\\infty \\psi_j W_{t-j}\n",
    "$$\n",
    "with $\\{W_t\\} \\sim \\mathrm{IID}(0, \\sigma^2)$, $\\sum_{j=-\\infty}^\\infty |\\psi_j| < \\infty$, and $\\sum_{j=-\\infty}^\\infty \\psi_j \\neq 0$, then\n",
    "$$\n",
    "\\sqrt{n} (X_n - \\mu_X) \\sim AN(0, \\nu), \\tag{2.11}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\nu = \\sum_{h=-\\infty}^\\infty \\gamma_X(h) = \\sigma^2 \\left( \\sum_{j=-\\infty}^\\infty \\psi_j \\right)^2.\n",
    "$$\n",
    "\n",
    "The proof of (2.11) can be found on Page 238 of Brockwell and Davis (2009, *Time Series Theory and Methods*). Very roughly, recall\n",
    "$$\n",
    "\\gamma_X(h) = \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_j \\psi_{j+h},\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\lim_{n \\to \\infty} n \\operatorname{Var}(X_n) = \\lim_{n \\to \\infty} \\sum_{h=-n}^n \\left(1 - \\frac{|h|}{n}\\right) \\gamma_X(h) = \\lim_{n \\to \\infty} \\sigma^2 \\sum_{j=-\\infty}^\\infty \\psi_j \\sum_{h=-n}^n \\left(1 - \\frac{|h|}{n}\\right) \\psi_{j+h} = \\sigma^2 \\left(\\sum_{j=-\\infty}^\\infty \\psi_j \\right)^2.\n",
    "$$\n",
    "\n",
    "The above results for the linear process also hold for ARMA models. Naturally,\n",
    "$$\n",
    "\\left( X_n - 1.96 \\sqrt{\\frac{\\nu}{n}}, \\quad X_n + 1.96 \\sqrt{\\frac{\\nu}{n}} \\right)\n",
    "$$\n",
    "is an approximate 95% confidence interval for $\\mu_X$.\n",
    "\n",
    "Since $\\nu$ is typically unknown, naturally, we have an approximate 95% confidence interval of $\\mu_X$ as\n",
    "$$\n",
    "\\left( X_n - 1.96 \\sqrt{\\frac{\\hat{\\nu}}{n}}, \\quad X_n + 1.96 \\sqrt{\\frac{\\hat{\\nu}}{n}} \\right),\n",
    "$$\n",
    "once we can obtain an estimator $\\hat{\\nu}$ of\n",
    "$$\n",
    "\\nu = \\sum_{h=-\\infty}^\\infty \\gamma_X(h).\n",
    "$$\n",
    "\n",
    "- One intuitive way is to use $\\hat{\\nu} = \\sum_{h=-\\infty}^\\infty \\hat{\\gamma}_X(h)$. However, based on finite sample $\\{X_1, \\ldots, X_n\\}$, it is impossible to obtain a reasonable estimator of $\\gamma_X(h)$ for $h \\geq n$. Then, why not use\n",
    "$$\n",
    "\\hat{\\nu} = \\sum_{h=-(n-1)}^{n-1} \\hat{\\gamma}_X(h).\n",
    "$$\n",
    "Very sadly and interestingly, this $\\hat{\\nu}$ is always zero.\n",
    "\n",
    "A compromising estimator $\\hat{\\nu}$ is then\n",
    "$$\n",
    "\\hat{\\nu} = \\sum_{h=-\\lfloor \\sqrt{n} \\rfloor}^{\\lfloor \\sqrt{n} \\rfloor} \\left( 1 - \\frac{|h|}{\\lfloor \\sqrt{n} \\rfloor} \\right) \\hat{\\gamma}_X(h).\n",
    "$$\n",
    "\n",
    "- If we know the model of the time series, i.e., we have an explicit formula for $\\gamma_X(h)$. For example,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Say we have an AR(1) process $\\{X_t\\}$ with mean $\\mu_X$ that satisfies\n",
    "$$\n",
    "X_t - \\mu_X = \\phi (X_{t-1} - \\mu_X) + W_t,\n",
    "$$\n",
    "we have\n",
    "$$\n",
    "\\gamma_X(h) = \\frac{\\phi^{|h|} \\sigma^2}{1 - \\phi^2}\n",
    "$$\n",
    "and consequently,\n",
    "$$\n",
    "\\nu = \\frac{\\sigma^2}{(1 - \\phi)^2}.\n",
    "$$\n",
    "Then we have the estimator\n",
    "$$\n",
    "\\hat{\\nu} = \\frac{\\hat{\\sigma}^2}{(1 - \\hat{\\phi})^2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 For $\\gamma_X(h)$ and $\\rho_X(h)$\n",
    "\n",
    "Estimators of $\\gamma_X(h)$ and $\\rho_X(h)$ are defined by\n",
    "$$\n",
    "\\hat{\\gamma}_X(h) = \\frac{1}{n} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\bar{X}_n)(X_t - \\bar{X}_n), \\tag{2.12}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\rho}_X(h) = \\frac{\\hat{\\gamma}_X(h)}{\\hat{\\gamma}_X(0)}. \\tag{2.13}\n",
    "$$\n",
    "\n",
    "First, let us check that \n",
    "$$\n",
    "\\hat{\\nu} = \\sum_{h=-(n-1)}^{n-1} \\hat{\\gamma}_X(h)\n",
    "$$\n",
    "is always zero.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\hat{\\nu} &= \\sum_{h=-(n-1)}^{n-1} \\frac{1}{n} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\bar{X}_n)(X_t - \\bar{X}_n) \\\\\n",
    "&= \\frac{1}{n} \\sum_{t=1}^n (X_t - \\bar{X}_n)^2 + \\frac{2}{n} \\sum_{h=1}^{n-1} \\sum_{t=1}^{n-h} (X_{t+h} - \\bar{X}_n)(X_t - \\bar{X}_n) \\\\\n",
    "&= \\frac{1}{n} \\sum_{t=1}^n (X_t^2 - 2 X_t \\bar{X}_n + \\bar{X}_n^2) + \\frac{2}{n} \\sum_{h=1}^{n-1} \\sum_{t=1}^{n-h} (X_{t+h} X_t - X_t \\bar{X}_n - X_{t+h} \\bar{X}_n + \\bar{X}_n^2) \\\\\n",
    "&= \\frac{1}{n} \\sum_{t=1}^n X_t^2 - \\bar{X}_n^2 + \\frac{2}{n} \\sum_{h=1}^{n-1} \\sum_{t=1}^{n-h} (X_{t+h} X_t - X_t \\bar{X}_n - X_{t+h} \\bar{X}_n + \\bar{X}_n^2) \\\\\n",
    "&= \\frac{1}{n} \\sum_{t=1}^n X_t^2 - n \\bar{X}_n^2 + \\frac{2}{n} \\sum_{h=1}^{n-1} \\sum_{t=1}^{n-h} X_{t+h} X_t \\\\\n",
    "&= 0.\n",
    "\\end{aligned}\n",
    "\n",
    "To check the bias of $\\hat{\\gamma}_X(h)$, let us look at the case when $h=0$. We have\n",
    "$$\n",
    "\\hat{\\gamma}_X(0) = \\frac{1}{n} \\sum_{t=1}^n (X_t - \\bar{X}_n)^2.\n",
    "$$\n",
    "Even in the iid case, this is a biased estimator (sample variance is biased which has $(n-1)^{-1}$ instead of $n^{-1}$). Expression for $E\\{\\hat{\\gamma}_X(h)\\}$ is messy (try your best to derive it as a HW problem). Let’s consider instead\n",
    "$$\n",
    "\\gamma_X(h) = \\frac{1}{n} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\mu_X)(X_t - \\mu_X).\n",
    "$$\n",
    "It can be seen that\n",
    "$$\n",
    "E\\{\\gamma_X(h)\\} = \\frac{n - |h|}{n} \\gamma_X(h) \\neq \\gamma_X(h);\n",
    "$$\n",
    "i.e., biased. Rather than using $\\gamma_X(h)$, you might find it more natural to consider\n",
    "$$\n",
    "\\gamma_X^e(h) = \\frac{1}{n - |h|} \\sum_{t=1}^{n-|h|} (X_{t+|h|} - \\mu_X)(X_t - \\mu_X).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have $E\\{\\gamma_X^e(h)\\} = \\gamma_X(h)$, an unbiased estimator. Now, replacing $\\mu_X$ with $\\bar{X}_n$, we have two estimators:\n",
    "$$\n",
    "\\hat{\\gamma}_X(h) \\quad \\text{and} \\quad \\frac{n}{n - |h|} \\hat{\\gamma}_X(h),\n",
    "$$\n",
    "respectively called biased and unbiased ACVF estimators (even though the latter one is actually biased in general!). Generally speaking, both $\\hat{\\gamma}_X(h)$ and $\\hat{\\rho}_X(h)$ are biased even if the factor $n^{-1}$ is replaced by $(n-h)^{-1}$. Nevertheless, under general assumptions, they are nearly unbiased for large sample sizes (conduct a simulation study to see the bias of both estimators as a HW problem).\n",
    "\n",
    "Now, let us talk about the reason why we like $\\hat{\\gamma}_X(h)$, and why I think this is very brilliant.\n",
    "\n",
    "**Lemma 2.3.** For any sequence $x_1, \\ldots, x_n$, the sample ACVF $\\hat{\\gamma}_X$ satisfies:\n",
    "1. $\\hat{\\gamma}_X(h) = \\hat{\\gamma}_X(-h)$,\n",
    "2. $\\hat{\\gamma}_X$ is nonnegative definite, and hence\n",
    "3. $\\hat{\\gamma}_X(0) \\geq 0$ and $|\\hat{\\gamma}_X(h)| \\leq \\hat{\\gamma}_X(0)$.\n",
    "\n",
    "**Proof.** The first one is trivial. It suffices to prove the second property which is equivalent to showing that for each $k \\geq 1$ the $k$-dimensional sample covariance matrix\n",
    "$$\n",
    "\\Gamma_b^k = \\begin{pmatrix}\n",
    "\\hat{\\gamma}_X(0) & \\hat{\\gamma}_X(1) & \\cdots & \\hat{\\gamma}_X(k-1) \\\\\n",
    "\\hat{\\gamma}_X(1) & \\hat{\\gamma}_X(0) & \\cdots & \\hat{\\gamma}_X(k-2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\hat{\\gamma}_X(k-1) & \\hat{\\gamma}_X(k-2) & \\cdots & \\hat{\\gamma}_X(0)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "is nonnegative definite. \n",
    "\n",
    "To see that, we have, for $k \\leq n$,\n",
    "$$\n",
    "\\Gamma_b^k = \\frac{1}{n} M M^T,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "0 & \\cdots & 0 & 0 & Y_1 & Y_2 & \\cdots & Y_k \\\\\n",
    "0 & \\cdots & 0 & Y_1 & Y_2 & \\cdots & Y_k & 0 \\\\\n",
    "\\vdots & & & & \\vdots & & & \\vdots \\\\\n",
    "0 & Y_1 & Y_2 & \\cdots & Y_k & 0 & \\cdots & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "is a $k \\times 2k$ matrix with $Y_i = X_i - \\bar{X}_n$ for $i=1,\\ldots,n$ and $Y_i = 0$ for $i = n+1, \\ldots, k$.\n",
    "\n",
    "Note that if $\\Gamma_b^m$ is nonnegative definite, then all $\\Gamma_b^k$ are nonnegative definite for all $k < m$.\n",
    "\n",
    "The nonnegative definite property is not always true if $n^{-1}$ is replaced by $(n-h)^{-1}$. Further, when $h \\geq n$ or for $h$ slightly smaller than $n$, there is no way to reliably estimate $\\gamma_X(h)$ and $\\rho_X(h)$ since the information around there is too little.\n",
    "\n",
    "Box and Jenkins (1976) suggest that useful estimates of correlation $\\rho_X(h)$ can only be made if $n$ is roughly 50 or more and $h \\leq \\frac{n}{4}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be important to be able to recognize when sample autocorrelations are significantly different from zero so that we can select the correct model to fit our data. In order to draw such statistical inference, we need the following asymptotic joint distribution.\n",
    "\n",
    "**Theorem 2.7.** For an IID process $\\{W_t\\}$, if $E(W_t^4) < \\infty$, we have\n",
    "$$\n",
    "\\hat{\\rho}_W(h) = \n",
    "\\begin{pmatrix}\n",
    "\\hat{\\rho}_W(1) \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{\\rho}_W(h)\n",
    "\\end{pmatrix}\n",
    "\\sim AN\\left(0, \\frac{1}{n} I_h \\right). \\tag{2.14}\n",
    "$$\n",
    "where $I_h$ is an $h \\times h$ identity matrix.\n",
    "\n",
    "**Remark 2.3.** For $\\{W_t\\} \\sim IID(0, \\sigma^2)$, then $\\rho_W(l) = 0$ for $l \\neq 0$. From Theorem 2.7, we have, for large $n$, $\\hat{\\rho}_W(1), \\ldots, \\hat{\\rho}_W(h)$ are approximately independent and identically distributed normal random variables from $N(0, \\frac{1}{n})$. \n",
    "\n",
    "If we plot the sample autocorrelation function $\\hat{\\rho}_W(k)$ as a function of $k$, approximately 0.95 of the sample autocorrelations should lie between the bounds $\\pm 1.96 \\sqrt{\\frac{1}{n}}$. This can be used as a check that the observations truly are from an IID process. \n",
    "\n",
    "In Figure 2.3, we have plotted the sample autocorrelation $\\hat{\\rho}_W(k)$ for $k = 1, \\ldots, 40$ for a sample of 200 iid $N(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(150)\n",
    "wt <- rnorm(200)\n",
    "par(bg = \"white\")\n",
    "par(mar = c(4, 4, 1.5, .5))\n",
    "acf(wt, lag.max = 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all but one of the autocorrelations lie between the bounds $\\pm 1.96 \\sqrt{\\frac{1}{n}}$, and\n",
    "\n",
    "**Remark 2.4.** This theorem yields several procedures for testing\n",
    "$$\n",
    "H_0: \\text{iid} \\quad \\text{vs} \\quad H_a: \\text{not iid}.\n",
    "$$\n",
    "\n",
    "**Method 1:** Based on the values of sample ACF: If for one $h$, \n",
    "$$\n",
    "\\hat{\\rho}_X(h) \\pm \\frac{z_{\\alpha/2}}{\\sqrt{n}}\n",
    "$$\n",
    "does not contain zero, reject $H_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2: The Portmanteau Test I**  \n",
    "Instead of checking $\\hat{\\rho}_X(h)$ for each $h$, it is also possible to consider the single statistic\n",
    "$$\n",
    "Q = n \\sum_{j=1}^{h} \\hat{\\rho}_X^2(j).\n",
    "$$\n",
    "Under $H_0$, $Q \\sim \\chi^2_h$. Thus, the rejection region is $Q > \\chi^2_h(1 - \\alpha)$.\n",
    "\n",
    "**Method 3: The Portmanteau Test II (Ljung and Box, 1978)**  \n",
    "$$\n",
    "Q_{\\text{LB}} = n(n + 2) \\sum_{j=1}^{h} \\frac{\\hat{\\rho}_X^2(j)}{n - j}\n",
    "$$\n",
    "which is better approximated by $\\chi^2_h$, thus the same rejection region applies.\n",
    "\n",
    "**Method 4: The Portmanteau Test III**  \n",
    "If testing residuals $\\{R_t\\}$ rather than a time series $\\{X_t\\}$, then:\n",
    "$$\n",
    "Q_{\\text{LB}} = n(n + 2) \\sum_{j=1}^{h} \\frac{\\hat{\\rho}_R^2(j)}{n - j}\n",
    "$$\n",
    "which is better approximated by $\\chi^2_{h - p}$, where $p$ is the number of parameters estimated in forming $\\{R_t\\}$.\n",
    "\n",
    "**Method 5: Turning Point Test**  \n",
    "**Method 6: Difference-Sign Test**  \n",
    "**Method 7: Rank Test**  \n",
    "### Simulation Study (Suggested Homework Problem)\n",
    "\n",
    "Design a simulation study to compare these testing procedures:\n",
    "\n",
    "1. **Learn** each method. Understand when each test is appropriate, how to calculate its test statistic, and when to reject the null hypothesis.\n",
    "2. **Generate** an iid sequence of a reasonable sample size. Apply each method to test for IID. If the test rejects, count it as 1; if not, count it as 0.\n",
    "3. **Repeat** Step 2 for 1000 iterations. Record the number of rejections. This should be close to the chosen significance level $\\alpha$ under the null.\n",
    "4. **Introduce dependence**: Gradually make your model more non-IID, e.g., use\n",
    "   $$\n",
    "   X_t - \\phi X_{t-1} = W_t,\n",
    "   $$\n",
    "   where initially $\\phi = 0$ (i.e., IID), and then set $\\phi = \\text{seq}(0.02, 1, \\text{by} = 0.02)$. For each $\\phi$, repeat the simulation 1000 times to obtain a rejection rate (i.e., the power).\n",
    "5. **Plot** the power curve to compare which methods are most powerful.\n",
    "6. **Summarize** your simulation results and turn them in with your homework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 2.8.** If $\\{X_t\\}$ is a stationary process,\n",
    "$$\n",
    "X_t = \\mu + \\sum_{j=-\\infty}^{\\infty} \\psi_j W_{t-j}\n",
    "$$\n",
    "where $\\{W_t\\} \\sim IID(0, \\sigma^2)$, $\\sum_{j=-\\infty}^{\\infty} |\\psi_j| < \\infty$, and $E[W_t^4] < \\infty$ (or $\\sum_{j=-\\infty}^{\\infty} \\psi_j^2 |j| < \\infty$), then for each $h$, we have:\n",
    "$$\n",
    "\\hat{\\rho}_X(h) = \n",
    "\\begin{pmatrix}\n",
    "\\hat{\\rho}_X(1) \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{\\rho}_X(h)\n",
    "\\end{pmatrix}\n",
    "\\sim AN \\left(\n",
    "\\begin{pmatrix}\n",
    "\\rho_X(1) \\\\\n",
    "\\vdots \\\\\n",
    "\\rho_X(h)\n",
    "\\end{pmatrix},\n",
    "\\frac{1}{n} \\Omega\n",
    "\\right). \\tag{2.15}\n",
    "$$\n",
    "\n",
    "Where $\\Omega = [\\omega_{ij}]_{i,j=1}^h$ is the covariance matrix whose $(i, j)$-element is given by **Bartlett’s formula**:\n",
    "$$\n",
    "\\omega_{ij} = \\sum_{k=-\\infty}^{\\infty} \\Big(\n",
    "\\rho_X(k+i)\\rho_X(k+j) + \\rho_X(k-i)\\rho_X(k+j) + 2\\rho_X(i)\\rho_X(j)\\rho_X^2(k)\n",
    "- 2\\rho_X(i)\\rho_X(k)\\rho_X(k+j) - 2\\rho_X(j)\\rho_X(k)\\rho_X(k+i)\n",
    "\\Big).\n",
    "$$\n",
    "\n",
    "**Remark 2.5.** Simple algebra shows that\n",
    "$$\n",
    "\\omega_{ij} = \\sum_{k=1}^{\\infty} \\left(\n",
    "\\rho_X(k+i) + \\rho_X(k-i) - 2\\rho_X(i)\\rho_X(k)\n",
    "\\right)\n",
    "\\left(\n",
    "\\rho_X(k+j) + \\rho_X(k-j) - 2\\rho_X(j)\\rho_X(k)\n",
    "\\right),\n",
    "$$\n",
    "which is a more convenient form of $\\omega_{ij}$ for computational purposes.\n",
    "\n",
    "This formula also shows that the asymptotic distribution of $\\sqrt{n}(\\hat{\\rho}_X(h) - \\rho_X(h))$ is the same as the random vector $(Y_1, \\ldots, Y_h)^T$ where\n",
    "$$\n",
    "Y_i = \\sum_{k=1}^{\\infty} \\left(\n",
    "\\rho_X(k+i) + \\rho_X(k-i) - 2\\rho_X(i)\\rho_X(k)\n",
    "\\right) Z_k,\n",
    "$$\n",
    "with $Z_1, Z_2, \\ldots$ being iid $N(0, 1)$.\n",
    "\n",
    "**Example 2.11.** *MA(q)*: If\n",
    "$$\n",
    "X_t = W_t + \\theta_1 W_{t-1} + \\cdots + \\theta_q W_{t-q},\n",
    "$$\n",
    "where $\\{W_t\\} \\sim IID(0, \\sigma^2)$, then from Bartlett’s formula, we have:\n",
    "$$\n",
    "\\omega_{ii} = 1 + 2\\rho_X^2(1) + 2\\rho_X^2(2) + \\cdots + 2\\rho_X^2(q), \\quad i > q,\n",
    "$$\n",
    "as the variance of the asymptotic distribution of $\\sqrt{n} \\hat{\\rho}_X(i)$ as $n \\to \\infty$.\n",
    "\n",
    "For MA(1), we have:\n",
    "$$\n",
    "\\rho_X(1) = \\frac{\\theta}{1 + \\theta^2}, \\quad \\rho_X(h) = 0 \\quad \\text{for } |h| > 1.\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\omega_{11} = 1 - 3\\rho_X^2(1) + 4\\rho_X^4(1),\n",
    "$$\n",
    "$$\n",
    "\\omega_{ii} = 1 + 2\\rho_X^2(1), \\quad i > q.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\theta = 0.8$. In Figure 2.11, we have plotted $\\hat{\\rho}_X(k)$ for $k = 0, \\ldots, 40$, using 200 observations where $\\{W_t\\}$ are iid $N(0, 1)$. It is found that $\\hat{\\rho}_X(1) = -0.465$ and $\\rho_X(1) = -0.4878$. \n",
    "\n",
    "Obviously, $\\hat{\\rho}_X(1)$ is less than $-1.96/\\sqrt{n} = -0.1379$. Thus, we would reject the hypothesis that the data are iid.\n",
    "\n",
    "Further, for $h = 2, \\ldots, 40$, we have $|\\hat{\\rho}_X(h)| \\leq 1.96/\\sqrt{n}$, which strongly suggests that the data are from a model in which observations are uncorrelated past lag 1.\n",
    "\n",
    "In addition, we have $\\rho_X(1) = -0.4878$ inside the 95% confidence interval:\n",
    "$$\n",
    "\\hat{\\rho}_X(1) \\pm 1.96 n^{-1/2} \\left\\{ 1 - 3\\hat{\\rho}_X^2(1) + 4\\hat{\\rho}_X^4(1) \\right\\}^{1/2}\n",
    "= (-0.5667, -0.3633),\n",
    "$$\n",
    "i.e., this further supports the compatibility of the data with the model:\n",
    "$$\n",
    "X_t = W_t - 0.8 W_{t-1}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(150)\n",
    "wt <- rnorm(250)\n",
    "xt <- filter(wt, sides = 1, c(1, -.8))[-(1:50)]\n",
    "par(bg = \"white\")\n",
    "par(mar = c(4, 4, 1.5, .5))\n",
    "acf(xt, lag.max = 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
