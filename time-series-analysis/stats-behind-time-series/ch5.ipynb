{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb7e282d",
   "metadata": {},
   "source": [
    "## 5 Prediction of Stationary Processes\n",
    "\n",
    "In this section, we consider predicting the value $X_{n+h}$ for $h > 0$ of a stationary time series with known mean $\\mu_X$ and autocovariance function (ACVF) $\\gamma_X$, in terms of the values $\\{X_n, \\ldots, X_1\\}$. The prediction is constructed as a linear combination of $1, X_n, \\ldots, X_1$ by minimizing the mean squared error (called the *optimal linear predictor*); i.e., we have the predictor as\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) = a_0 + a_1 X_n + \\cdots + a_n X_1,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a} = (a_0, \\ldots, a_n)^T$ minimizes\n",
    "\n",
    "$$\n",
    "S(\\mathbf{a}) = \\mathbb{E}\\left( X_{n+h} - a_0 - a_1 X_n - \\cdots - a_n X_1 \\right)^2. \\tag{5.1}\n",
    "$$\n",
    "\n",
    "### 5.1 Predict $X_{n+h}$ by $X_n$\n",
    "\n",
    "We start with predicting $X_{n+h}$ by $X_n$ as\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, 1) = a_0 + a_1 X_n.\n",
    "$$\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "$$\n",
    "S(\\mathbf{a}) = \\mathbb{E}\\left( X_{n+h} - a_0 - a_1 X_n \\right)^2 \n",
    "= \\mathbb{E}(X_{n+h}^2 + a_0^2 + a_1^2 X_n^2 - 2 a_0 X_{n+h} - 2 a_1 X_n X_{n+h} + 2 a_0 a_1 X_n).\n",
    "$$\n",
    "\n",
    "Using stationarity, this simplifies to:\n",
    "\n",
    "$$\n",
    "S(\\mathbf{a}) = a_0^2 + (a_1^2 + 1)\\left( \\gamma_X(0) + \\mu_X^2 \\right) + (2 a_0 a_1 - 2 a_0) \\mu_X - 2 a_1 \\left( \\gamma_X(h) + \\mu_X^2 \\right).\n",
    "$$\n",
    "\n",
    "Taking the partial derivatives and setting them to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S(\\mathbf{a})}{\\partial a_0} = 2 a_0 + 2 a_1 \\mu_X - 2 \\mu_X = 0,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S(\\mathbf{a})}{\\partial a_1} = 2 a_0 \\mu_X + 2 a_1 \\left( \\gamma_X(0) + \\mu_X^2 \\right) - 2 \\left( \\gamma_X(h) + \\mu_X^2 \\right) = 0.\n",
    "$$\n",
    "\n",
    "Solving this system gives:\n",
    "\n",
    "$$\n",
    "a_1 = \\rho_X(h), \\quad a_0 = \\mu_X (1 - \\rho_X(h)),\n",
    "$$\n",
    "\n",
    "where $\\rho_X(h) = \\gamma_X(h)/\\gamma_X(0)$ is the autocorrelation function.\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, 1) = \\mu_X + \\rho_X(h)(X_n - \\mu_X),\n",
    "$$\n",
    "\n",
    "and the mean squared error is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\left( P(X_{n+h} \\mid X_n, 1) - X_{n+h} \\right)^2 \\right] = \\gamma_X(0) \\left(1 - \\rho_X(h)^2\\right).\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- If $|\\rho_X(h)| \\to 1$, then $\\mathbb{E}[\\text{error}^2] \\to 0$ (accuracy improves).\n",
    "- If $\\rho_X(h) = \\pm 1$, the error is zero (perfect linear predictability).\n",
    "- If $\\rho_X(h) = 0$, then $P(X_{n+h} \\mid X_n, 1) = \\mu_X$, and the error is $\\gamma_X(0)$ (uncorrelated).\n",
    "\n",
    "If $\\{X_t\\}$ is Gaussian and stationary, then the joint distribution of $(X_n, X_{n+h})$ is:\n",
    "\n",
    "$$\n",
    "\\mathcal{N} \\left(\n",
    "\\begin{pmatrix}\n",
    "\\mu_X \\\\\n",
    "\\mu_X\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "\\gamma_X(0) & \\rho_X(h) \\gamma_X(0) \\\\\n",
    "\\rho_X(h) \\gamma_X(0) & \\gamma_X(0)\n",
    "\\end{pmatrix}\n",
    "\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14da540",
   "metadata": {},
   "source": [
    "The conditional distribution of $X_{n+h}$ given $X_n$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{N} \\left( \\mu_X + \\rho_X(h)(X_n - \\mu_X), \\; \\gamma_X(0)\\left(1 - \\rho_X(h)^2\\right) \\right).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(X_{n+h} \\mid X_n) = \\mu_X + \\rho_X(h)(X_n - \\mu_X).\n",
    "$$\n",
    "\n",
    "Generally speaking, suppose we have a target variable $Y$ and a set of predictor variables $X$. The optimal (least squares) predictor of $Y$ given $X$ is $\\mathbb{E}(Y \\mid X)$:\n",
    "\n",
    "$$\n",
    "\\min_f \\mathbb{E}\\left\\{ (Y - f(X))^2 \\right\\} = \\min_f \\mathbb{E}\\left[ \\mathbb{E}\\left\\{ (Y - f(X))^2 \\mid X \\right\\} \\right] \n",
    "= \\mathbb{E} \\left[ \\mathbb{E} \\left\\{ Y - \\mathbb{E}(Y \\mid X) \\right\\}^2 \\mid X \\right].\n",
    "$$\n",
    "\n",
    "Thus, the optimal predictor of $Y$ given $X$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(Y \\mid X).\n",
    "$$\n",
    "\n",
    "- If $\\{X_t\\}$ is stationary, then $P(X_{n+h} \\mid X_n, 1) = \\mu_X + \\rho_X(h)(X_n - \\mu_X)$ is the **optimal linear predictor**.\n",
    "- If $\\{X_t\\}$ is also Gaussian, then $P(X_{n+h} \\mid X_n, 1) = \\mu_X + \\rho_X(h)(X_n - \\mu_X)$ is the **optimal predictor**.\n",
    "- This extends to longer histories: $\\{X_n, X_{n-1}, \\ldots, X_1\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Predict $X_{n+h}$ by $\\{X_n, \\ldots, X_1, 1\\}$\n",
    "\n",
    "To find $P(X_{n+h} \\mid X_n, \\ldots, X_1)$, we minimize the function (5.1) to find the values of $\\mathbf{a} = (a_0, a_1, \\ldots, a_n)^T$.\n",
    "\n",
    "Taking partial derivatives and setting them to zero, we obtain a system of equations:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S(\\mathbf{a})}{\\partial a_j} = 0, \\quad j = 0, \\ldots, n,\n",
    "$$\n",
    "\n",
    "for $j=0$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left( X_{n+h} - a_0 - \\sum_{i=1}^n a_i X_{n+1-i} \\right) = 0,\n",
    "$$\n",
    "\n",
    "and for $j = 1, \\ldots, n$,\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left[ \\left( X_{n+h} - a_0 - \\sum_{i=1}^n a_i X_{n+1-i} \\right) X_{n+1-j} \\right] = 0.\n",
    "$$\n",
    "\n",
    "It can be shown that $\\mathbf{a}_n = (a_1, \\ldots, a_n)^T$ satisfies:\n",
    "\n",
    "$$\n",
    "\\Gamma_n \\mathbf{a}_n = \\gamma_n(h), \\tag{5.2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "a_0 = \\mu_X \\left(1 - \\sum_{i=1}^n a_i \\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\Gamma_n = [\\gamma_X(i - j)]_{i,j=1}^n$ is the autocovariance matrix, and\n",
    "- $\\gamma_n(h) = \\left( \\gamma_X(h), \\ldots, \\gamma_X(h + n - 1) \\right)^T$ is the cross-covariance vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e86c22",
   "metadata": {},
   "source": [
    "Hence, we have\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) = \\mu_X + \\sum_{i=1}^n a_i (X_{n+1-i} - \\mu_X). \\tag{5.3}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left\\{ \\left( P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) - X_{n+h} \\right)^2 \\right\\} = \\gamma_X(0) - \\mathbf{a}_n^T \\gamma_n(h).\n",
    "$$\n",
    "\n",
    "Now, we show the uniqueness of $P(X_{n+h} \\mid X_n, \\ldots, X_1)$ (left as a homework problem).  \n",
    "**Hint**: Suppose there are two different sets of coefficients:  \n",
    "$\\{a_{j1}, \\; j = 0, \\ldots, n\\}$ and $\\{a_{j2}, \\; j = 0, \\ldots, n\\}$ such that\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) = a_{01} + a_{11} X_n + \\cdots + a_{n1} X_1 = a_{02} + a_{12} X_n + \\cdots + a_{n2} X_1.\n",
    "$$\n",
    "\n",
    "Define\n",
    "\n",
    "$$\n",
    "Z = a_{01} - a_{02} + \\sum_{j=1}^n (a_{j1} - a_{j2}) X_{n+1-j}.\n",
    "$$\n",
    "\n",
    "Show that $\\mathbb{E}(Z^2) = 0$, which implies $Z = 0$.\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition 5.1**  \n",
    "For a stationary process, if $\\gamma_X(0) > 0$ and $\\gamma_X(h) \\to 0$ as $h \\to \\infty$, then the covariance matrix\n",
    "\n",
    "$$\n",
    "\\Gamma_n = [\\gamma_X(i - j)]_{i,j=1}^n\n",
    "$$\n",
    "\n",
    "is positive definite for every $n$.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark 5.1**  \n",
    "When $\\gamma_X(0) > 0$ and $\\gamma_X(h) \\to 0$ as $h \\to \\infty$, the uniqueness can be seen directly from **Proposition 5.1**  \n",
    "i.e., in this case, $\\Gamma_n = [\\gamma_X(i - j)]_{i,j=1}^n$ is non-singular for every $n$, and equation (5.2) has a unique solution:\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_n = \\Gamma_n^{-1} \\gamma_n(h).\n",
    "$$\n",
    "\n",
    "Further, if $\\mu_X = 0$, we have\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) = \\sum_{i=1}^n \\phi_{ni} X_{n+1-i},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left\\{ \\left( P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) - X_{n+h} \\right)^2 \\right\\} = \\gamma_X(0) - \\gamma_n(h)^T \\Gamma_n^{-1} \\gamma_n(h).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Properties of $P(X_{n+h} \\mid X_n, \\ldots, X_1, 1)$\n",
    "\n",
    "1. $P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) = \\mu_X + \\sum_{i=1}^n a_i (X_{n+1-i} - \\mu_X)$, where $\\mathbf{a}_n = (a_1, \\ldots, a_n)^T$ satisfies\n",
    "\n",
    "   $\n",
    "   \\Gamma_n \\mathbf{a}_n = \\gamma_n(h).\n",
    "   $\n",
    "\n",
    "2. $\\mathbb{E} \\left\\{ \\left( P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) - X_{n+h} \\right)^2 \\right\\} = \\gamma_X(0) - \\mathbf{a}_n^T \\gamma_n(h).$\n",
    "\n",
    "3. $\\mathbb{E} \\left\\{ X_{n+h} - P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) \\right\\} = 0.$\n",
    "\n",
    "4. $\\mathbb{E} \\left[ \\left( X_{n+h} - P(X_{n+h} \\mid X_n, \\ldots, X_1, 1) \\right) X_j \\right] = 0, \\quad \\text{for } j = 1, \\ldots, n.$\n",
    "\n",
    "---\n",
    "\n",
    "**Remark 5.2**  \n",
    "Notice that properties 3 and 4 can be interpreted easily by viewing $P(X_{n+h} \\mid X_n, \\ldots, X_1)$ as the **projection** of $X_{n+h}$ on the linear subspace formed by $\\{X_n, \\ldots, X_1, 1\\}$. This comes from the projection mapping theory of Hilbert spaces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5b495",
   "metadata": {},
   "source": [
    "A **Hilbert space** is a complete inner product space. An **inner product space** is a vector space with inner product $\\langle a, b \\rangle$, satisfying:\n",
    "\n",
    "- $\\langle a, b \\rangle = \\langle b, a \\rangle$\n",
    "- $\\langle \\alpha_1 a_1 + \\alpha_2 a_2, b \\rangle = \\alpha_1 \\langle a_1, b \\rangle + \\alpha_2 \\langle a_2, b \\rangle$\n",
    "- $\\langle a, a \\rangle = 0 \\iff a = 0$\n",
    "- The **norm** of $a$ is $\\|a\\| = \\sqrt{\\langle a, a \\rangle}$\n",
    "\n",
    "Note that **complete** means every Cauchy sequence in the space has its limit in the space.\n",
    "\n",
    "### Examples of Hilbert spaces:\n",
    "\n",
    "1. $\\mathbb{R}^n$ with $\\langle x, y \\rangle = \\sum x_i y_i$\n",
    "2. $H = \\{X : \\mathbb{E}[X^2] < \\infty\\}$ with $\\langle X, Y \\rangle = \\mathbb{E}[XY]$\n",
    "\n",
    "The Hilbert space $H$ is the one of interest in this course.\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem 5.1 (The Projection Theorem)\n",
    "\n",
    "If $M$ is a closed subspace of the Hilbert space $H$ and $x \\in H$, then:\n",
    "\n",
    "1. There exists a **unique** element $\\hat{x} \\in M$ such that\n",
    "\n",
    "   $$\n",
    "   \\|x - \\hat{x}\\| = \\inf_{y \\in M} \\|x - y\\|,\n",
    "   $$\n",
    "\n",
    "2. $\\hat{x} \\in M$ and $\\|x - \\hat{x}\\| = \\inf_{y \\in M} \\|x - y\\|$ if and only if $(x - \\hat{x})$ is orthogonal to $M$.\n",
    "\n",
    "We write $P(x \\mid M)$ as the **projection** of $x$ onto $M$.\n",
    "\n",
    "---\n",
    "\n",
    "### Proposition 5.2 (Properties of Projection Mappings)\n",
    "\n",
    "Let $H$ be a Hilbert space and let $P(\\cdot \\mid M)$ denote the projection mapping onto a closed subspace $M$. Then:\n",
    "\n",
    "1. $P(\\alpha x + \\beta y \\mid M) = \\alpha P(x \\mid M) + \\beta P(y \\mid M)$  \n",
    "2. $\\|x\\|^2 = \\|P(x \\mid M) + (x - P(x \\mid M))\\|^2 = \\|P(x \\mid M)\\|^2 + \\|x - P(x \\mid M)\\|^2$  \n",
    "3. Each $x \\in H$ has a **unique representation** as a sum of an element of $M$ and an element orthogonal to $M$, i.e.,\n",
    "\n",
    "   $$\n",
    "   x = P(x \\mid M) + (x - P(x \\mid M))\n",
    "   $$\n",
    "\n",
    "4. $P(x_n \\mid M) \\to P(x \\mid M)$ if $\\|x_n - x\\| \\to 0$  \n",
    "5. $x \\in M$ if and only if $P(x \\mid M) = x$  \n",
    "6. $x$ is orthogonal to $M$ if and only if $P(x \\mid M) = 0$  \n",
    "7. $M_1 \\subseteq M_2$ if and only if\n",
    "\n",
    "   $$\n",
    "   P(P(x \\mid M_2) \\mid M_1) = P(x \\mid M_1), \\quad \\text{for all } x \\in H.\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca79c5a",
   "metadata": {},
   "source": [
    "### 5.3 General Case\n",
    "\n",
    "Suppose now that $Y$ and $Z_n, \\ldots, Z_1$ are any random variables with finite second moments, and that the means $\\mu = \\mathbb{E}Y$, $\\mu_i = \\mathbb{E}Z_i$, and covariances $\\text{Cov}(Y, Y)$, $\\text{Cov}(Y, Z_i)$, and $\\text{Cov}(Z_i, Z_j)$ are all known.\n",
    "\n",
    "Note: This setting does **not** require a stationary process.\n",
    "\n",
    "Define:\n",
    "\n",
    "- $\\mathbf{Z} = (Z_n, \\ldots, Z_1)^T$\n",
    "- $\\mu_Z = (\\mu_n, \\ldots, \\mu_1)^T$\n",
    "- $\\gamma = (\\text{Cov}(Y, Z_n), \\ldots, \\text{Cov}(Y, Z_1))^T$\n",
    "- $\\Gamma = \\text{Cov}(\\mathbf{Z}, \\mathbf{Z}) = \\left[ \\text{Cov}(Z_{n+1-i}, Z_{n+1-j}) \\right]_{i,j=1}^n$\n",
    "\n",
    "Then, using the same projection argument:\n",
    "\n",
    "$$\n",
    "P(Y \\mid \\mathbf{Z}, 1) = \\mu_Y + \\mathbf{a}^T (\\mathbf{Z} - \\mu_Z),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a} = (a_1, \\ldots, a_n)^T$ is any solution to\n",
    "\n",
    "$$\n",
    "\\Gamma \\mathbf{a} = \\gamma.\n",
    "$$\n",
    "\n",
    "And the mean squared error of this predictor is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\left( Y - P(Y \\mid \\mathbf{Z}, 1) \\right)^2 \\right] = \\text{Var}(Y) - \\mathbf{a}^T \\gamma.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Properties of the Prediction Operator $P(\\cdot \\mid \\mathbf{Z})$\n",
    "\n",
    "Suppose that $\\mathbb{E}U^2 < \\infty$, $\\mathbb{E}V^2 < \\infty$, $\\Gamma = \\text{Cov}(\\mathbf{Z}, \\mathbf{Z})$, and $\\beta, \\alpha_1, \\ldots, \\alpha_n$ are constants.\n",
    "\n",
    "- $P(U \\mid \\mathbf{Z}) = \\mathbb{E}U + \\mathbf{a}^T (\\mathbf{Z} - \\mathbb{E}\\mathbf{Z})$, where $\\Gamma \\mathbf{a} = \\text{Cov}(U, \\mathbf{Z})$\n",
    "- $\\mathbb{E}[(U - P(U \\mid \\mathbf{Z})) \\mathbf{Z}] = 0$\n",
    "- $\\mathbb{E}[U - P(U \\mid \\mathbf{Z})] = 0$\n",
    "- $\\mathbb{E}\\left[(U - P(U \\mid \\mathbf{Z}))^2\\right] = \\text{Var}(U) - \\mathbf{a}^T \\text{Cov}(U, \\mathbf{Z})$\n",
    "- $P(\\alpha_1 U + \\alpha_2 V + \\beta \\mid \\mathbf{Z}) = \\alpha_1 P(U \\mid \\mathbf{Z}) + \\alpha_2 P(V \\mid \\mathbf{Z}) + \\beta$\n",
    "- $P\\left( \\sum_{i=1}^n \\alpha_i Z_i + \\beta \\mid \\mathbf{Z} \\right) = \\sum_{i=1}^n \\alpha_i Z_i + \\beta$\n",
    "- $P(U \\mid \\mathbf{Z}) = \\mathbb{E}U$ if $\\text{Cov}(U, \\mathbf{Z}) = 0$\n",
    "- $P(U \\mid \\mathbf{Z}) = P\\left( P(U \\mid \\mathbf{Z}, V) \\mid \\mathbf{Z} \\right)$ if $\\mathbb{E}[V V^T]$ is finite\n",
    "\n",
    "These results follow directly from the standard **projection mapping theory** of a **Hilbert space**.\n",
    "\n",
    "In this case, the Hilbert space is:\n",
    "\n",
    "$$\n",
    "H = \\{ X : \\mathbb{E}[X^2] < \\infty \\}, \\quad \\text{with inner product } \\langle X, Y \\rangle = \\mathbb{E}[XY].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6507acc",
   "metadata": {},
   "source": [
    "If $\\mu = \\mathbb{E}Y = 0$, $\\mu_i = \\mathbb{E}Z_i = 0$ (for example, we consider the **zero-mean stationary process**), then we have:\n",
    "\n",
    "- $\\mathbf{Z} = (Z_n, \\ldots, Z_1)^T$\n",
    "- $\\mu_Z = 0$\n",
    "- $\\gamma = \\left( \\text{Cov}(Y, Z_n), \\ldots, \\text{Cov}(Y, Z_1) \\right)^T$\n",
    "- $\\Gamma = \\text{Cov}(\\mathbf{Z}, \\mathbf{Z}) = \\left[ \\text{Cov}(Z_{n+1-i}, Z_{n+1-j}) \\right]_{i,j=1}^n$\n",
    "\n",
    "It can be easily seen that:\n",
    "\n",
    "$$\n",
    "P(Y \\mid \\mathbf{Z}, 1) = P(Y \\mid \\mathbf{Z}) = \\mathbf{a}^T \\mathbf{Z},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a} = (a_1, \\ldots, a_n)^T$ is any solution of\n",
    "\n",
    "$$\n",
    "\\Gamma \\mathbf{a} = \\gamma.\n",
    "$$\n",
    "\n",
    "The mean squared error of this predictor is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(Y) - \\mathbf{a}^T \\gamma.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example 5.1\n",
    "\n",
    "For an AR(1) series: $X_t = \\phi X_{t-1} + W_t$, where $|\\phi| < 1$ and $\\{W_t\\} \\sim \\text{WN}(0, \\sigma^2)$, find:\n",
    "\n",
    "1. $P(X_{n+1} \\mid X_n, \\ldots, X_1, 1)$  \n",
    "2. $P(X_{n+1} \\mid X_n, \\ldots, X_2, 1)$  \n",
    "3. $P(X_1 \\mid X_n, \\ldots, X_2, 1)$\n",
    "\n",
    "---\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "For parts (1) and (2), we need to compute $P(X_{n+1} \\mid X_n, \\ldots, X_i)$. We define:\n",
    "\n",
    "- $\\mathbf{Z} = (X_n, \\ldots, X_i)^T$\n",
    "- $\\gamma = \\dfrac{\\sigma^2}{1 - \\phi^2} \\cdot (\\phi, \\phi^2, \\ldots, \\phi^{n-i})^T$\n",
    "- $\\Gamma = \\dfrac{\\sigma^2}{1 - \\phi^2} \\cdot\n",
    "\\begin{pmatrix}\n",
    "1 & \\phi & \\phi^2 & \\cdots & \\phi^{n-i} \\\\\n",
    "\\phi & 1 & \\phi & \\cdots & \\phi^{n-i-1} \\\\\n",
    "\\phi^2 & \\phi & 1 & \\cdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\phi \\\\\n",
    "\\phi^{n-i} & \\phi^{n-i-1} & \\cdots & \\phi & 1\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Solving $\\Gamma \\mathbf{a} = \\gamma$ yields:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = (\\phi, 0, \\ldots, 0)^T.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "P(X_{n+1} \\mid X_n, \\ldots, X_i, 1) = P(X_{n+1} \\mid X_n, \\ldots, X_i) = \\phi X_n.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb0351",
   "metadata": {},
   "source": [
    "For Part (3), we have:\n",
    "\n",
    "- $\\mathbf{Z} = (X_n, \\ldots, X_2)^T$\n",
    "- $\\gamma = \\dfrac{\\sigma^2}{1 - \\phi^2} \\cdot (\\phi^{n-1}, \\phi^{n-2}, \\ldots, \\phi)^T$\n",
    "- $\\Gamma = \\dfrac{\\sigma^2}{1 - \\phi^2} \\cdot\n",
    "\\begin{pmatrix}\n",
    "1 & \\phi & \\phi^2 & \\cdots & \\phi^{n-2} \\\\\n",
    "\\phi & 1 & \\phi & \\cdots & \\phi^{n-3} \\\\\n",
    "\\phi^2 & \\phi & 1 & \\cdots & \\vdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\phi \\\\\n",
    "\\phi^{n-2} & \\phi^{n-3} & \\cdots & \\phi & 1\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "Solving $\\Gamma \\mathbf{a} = \\gamma$ gives:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = (0, \\ldots, 0, \\phi)^T.\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} \\mid X_n, \\ldots, X_2, 1) = P(X_{n+1} \\mid X_n, \\ldots, X_2) = \\phi X_2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example 5.2\n",
    "\n",
    "For the **causal AR(p)** process defined by:\n",
    "\n",
    "$$\n",
    "X_t - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p} = W_t, \\quad \\text{where } \\{W_t\\} \\sim \\text{WN}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "and $W_t$ is uncorrelated with $X_s$ for $s < t$, then we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X_{n+1} \\mid X_n, \\ldots, X_1, 1)\n",
    "&= P\\left(\\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + W_{n+1} \\mid X_n, \\ldots, X_1, 1\\right) \\\\\n",
    "&= \\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + P(W_{n+1} \\mid X_n, \\ldots, X_1, 1) \\\\\n",
    "&= \\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "since $W_{n+1}$ is uncorrelated with past values.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 5.3\n",
    "\n",
    "For any **zero-mean stationary process** $\\{X_t\\}$, suppose:\n",
    "\n",
    "$$\n",
    "P(X_{n+1} \\mid X_n, \\ldots, X_2) = \\sum_{j=1}^{n-1} a_j X_{n+1-j},\n",
    "$$\n",
    "\n",
    "then:\n",
    "\n",
    "$$\n",
    "P(X_1 \\mid X_n, \\ldots, X_2) = \\sum_{j=1}^{n-1} a_{n-j} X_{n+1-j},\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[(X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2))^2\\right]\n",
    "= \\mathbb{E}\\left[(X_1 - P(X_1 \\mid X_n, \\ldots, X_2))^2\\right]\n",
    "= \\mathbb{E}\\left[(X_n - P(X_n \\mid X_{n-1}, \\ldots, X_1))^2\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f544d47",
   "metadata": {},
   "source": [
    "## 5.4 The Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Like the autocorrelation function, the PACF is another tool that conveys vital information regarding the dependence structure of a stationary process and depends only on the second order properties of the process.\n",
    "\n",
    "The partial autocorrelation function (PACF) $\\alpha_X(\\cdot)$ of a stationary time series is defined by\n",
    "\n",
    "$$\n",
    "\\alpha_X(1) = \\operatorname{Corr}(X_2, X_1) = \\rho_X(1),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\alpha_X(k) = \\operatorname{Corr}\\bigl(X_{k+1} - P(X_{k+1} \\mid X_k, \\ldots, X_2, 1), \\quad X_1 - P(X_1 \\mid X_k, \\ldots, X_2, 1)\\bigr).\n",
    "$$\n",
    "\n",
    "The value of $\\alpha_X(k)$ is known as the partial autocorrelation of $\\{X_t\\}$ at lag $k$.\n",
    "\n",
    "The PACF $\\alpha_X(k)$ may be regarded as the correlation between $X_1$ and $X_{k+1}$, adjusted for the intervening observations $X_2, \\ldots, X_k$.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark 5.3.** The definition above defines $\\alpha_X(k)$ based on $\\{X_1, X_2, \\ldots, X_k, X_{k+1}\\}$. But it is also equivalent to the one based on $\\{X_{t+1}, X_{t+2}, \\ldots, X_{t+k}, X_{t+k+1}\\}$ for any $t > 0$; i.e.,\n",
    "\n",
    "$$\n",
    "\\alpha_X(k) = \\operatorname{Corr}\\bigl(X_{t+k+1} - P(X_{t+k+1} \\mid X_{t+k}, \\ldots, X_{t+2}, 1), \\quad X_{t+1} - P(X_{t+1} \\mid X_{t+k}, \\ldots, X_{t+2}, 1)\\bigr).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example 5.4.** Let $\\{X_t\\}$ be the zero-mean AR(1) process\n",
    "\n",
    "$$\n",
    "X_t = \\phi X_{t-1} + W_t.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\alpha_X(1) = \\operatorname{Corr}(X_2, X_1) = \\operatorname{Corr}(\\phi X_1 + W_2, X_1) = \\phi.\n",
    "$$\n",
    "\n",
    "Based on Example 5.1, we have\n",
    "\n",
    "$$\n",
    "P(X_{k+1} \\mid X_k, \\ldots, X_2, 1) = \\phi X_k \\quad \\text{and} \\quad P(X_1 \\mid X_k, \\ldots, X_2, 1) = \\phi X_2.\n",
    "$$\n",
    "\n",
    "Then for $k \\geq 2$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha_X(k) &= \\operatorname{Corr}(X_{k+1} - \\phi X_k, \\, X_1 - \\phi X_2) \\\\\n",
    "&= \\operatorname{Corr}(W_k, \\, X_1 - \\phi X_2) \\\\\n",
    "&= 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This says that the correlation between $X_{k+1}$ and $X_1$ adjusted for intermediate values is zero for $k \\geq 2$.\n",
    "\n",
    "---\n",
    "\n",
    "**A HW:** For the MA(1) process: \n",
    "\n",
    "$$\n",
    "X_t = W_t + \\theta W_{t-1}, \\quad |\\theta| < 1, \\quad \\{W_t\\} \\sim \\text{WN}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "find its PACF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ddb07",
   "metadata": {},
   "source": [
    "## Corollary 5.1\n",
    "\n",
    "Let $\\{X_t\\}$ be a zero-mean stationary process with autocovariance function $\\gamma_X(h)$ such that $\\gamma_X(h) \\to 0$ as $h \\to \\infty$. Then\n",
    "\n",
    "$$\n",
    "P(X_{k+1} \\mid X_k, \\ldots, X_1, 1) = \\sum_{j=1}^k \\phi_{kj} X_{k+1-j}.\n",
    "$$\n",
    "\n",
    "From the equations\n",
    "\n",
    "$$\n",
    "E\\bigl[(X_{k+1} - P(X_{k+1} \\mid X_k, \\ldots, X_1, 1)) X_j \\bigr] = 0, \\quad j = k, \\ldots, 1,\n",
    "$$\n",
    "\n",
    "we have the matrix equation\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\rho_X(0) & \\rho_X(1) & \\rho_X(2) & \\cdots & \\rho_X(k-1) \\\\\n",
    "\\rho_X(1) & \\rho_X(0) & \\rho_X(1) & \\cdots & \\rho_X(k-2) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\rho_X(k-1) & \\rho_X(k-2) & \\rho_X(k-3) & \\cdots & \\rho_X(0)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\phi_{k1} \\\\\n",
    "\\phi_{k2} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{kk}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\rho_X(1) \\\\\n",
    "\\rho_X(2) \\\\\n",
    "\\vdots \\\\\n",
    "\\rho_X(k)\n",
    "\\end{pmatrix}.\n",
    "\\tag{5.4}\n",
    "$$\n",
    "\n",
    "The partial autocorrelation $\\alpha_X(k)$ of $\\{X_t\\}$ at lag $k$ is given by\n",
    "\n",
    "$$\n",
    "\\alpha_X(k) = \\phi_{kk}, \\quad k \\geq 1.\n",
    "$$\n",
    "\n",
    "**Proof:** We will prove this corollary later.\n",
    "\n",
    "---\n",
    "\n",
    "The sample partial autocorrelation $\\hat{\\alpha}_X(k)$ at lag $k$ of the observed data $\\{x_1, \\ldots, x_n\\}$, assuming there exists some $i,j$ with $x_i \\neq x_j$, is defined by\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_X(k) = \\hat{\\phi}_{kk}, \\quad 1 \\leq k \\leq n,\n",
    "$$\n",
    "\n",
    "where $\\hat{\\phi}_{kk}$ is uniquely determined by equation (5.4) with each $\\rho_X(j)$ replaced by the corresponding sample autocorrelation $\\hat{\\rho}_X(j)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 Recursive Methods for Computing Best Linear Predictors\n",
    "\n",
    "In this section, we focus on zero-mean stationary processes. We establish two recursive algorithms for determining the one-step predictors\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = P(X_{n+1} \\mid X_n, \\ldots, X_1),\n",
    "$$\n",
    "\n",
    "and show how they can be used to compute the $h$-step predictors\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc458b",
   "metadata": {},
   "source": [
    "## 5.5.1 Recursive Prediction Using the Durbin-Levinson Algorithm\n",
    "\n",
    "We can express the predictor $\\hat{X}_{n+1}$ in the form\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = \\phi_{n1} X_n + \\cdots + \\phi_{nn} X_1, \\quad n \\geq 1.\n",
    "$$\n",
    "\n",
    "Its mean squared error of prediction is denoted by $\\nu_n$ as\n",
    "\n",
    "$$\n",
    "\\nu_n = E\\bigl(X_{n+1} - \\hat{X}_{n+1}\\bigr)^2, \\quad n \\geq 1.\n",
    "$$\n",
    "\n",
    "Clearly, $\\nu_0 = \\gamma_X(0)$.\n",
    "\n",
    "The following proposition specifies an algorithm, known as the **Durbin-Levinson algorithm**, which is a recursive scheme for computing $\\phi_n = (\\phi_{n1}, \\ldots, \\phi_{nn})^T$ and $\\nu_n$ for $n = 1, 2, \\ldots$.\n",
    "\n",
    "---\n",
    "\n",
    "### Proposition 5.3 (The Durbin-Levinson Algorithm)\n",
    "\n",
    "If $\\{X_t\\}$ is a zero-mean stationary process with autocovariance function $\\gamma_X(\\cdot)$ such that $\\gamma_X(0) > 0$ and $\\gamma_X(h) \\to 0$ as $h \\to \\infty$, then the coefficients $\\phi_{nj}$ and mean squared errors $\\nu_n$ as defined above satisfy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\phi_{11} = \\frac{\\gamma_X(1)}{\\gamma_X(0)}, \\quad \\nu_0 = \\gamma_X(0), \\\\\n",
    "& \\phi_{nn} = \\left( \\gamma_X(n) - \\sum_{j=1}^{n-1} \\phi_{n-1,j} \\, \\gamma_X(n - j) \\right) \\nu_{n-1}^{-1}, \\\\\n",
    "& \\begin{pmatrix}\n",
    "\\phi_{n1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n,n-1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\phi_{n-1,1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n-1,n-1}\n",
    "\\end{pmatrix}\n",
    "- \\phi_{nn}\n",
    "\\begin{pmatrix}\n",
    "\\phi_{n-1,n-1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n-1,1}\n",
    "\\end{pmatrix}, \\\\\n",
    "& \\nu_n = \\nu_{n-1} (1 - \\phi_{nn}^2).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Proof Sketch\n",
    "\n",
    "Consider the Hilbert space $\\mathcal{H} = \\{X : E[X^2] < \\infty \\}$ with inner product $\\langle X, Y \\rangle = E[XY]$ and norm $\\|X\\|^2 = \\langle X, X \\rangle$.\n",
    "\n",
    "By definition of $\\hat{X}_{n+1}$, it lies in the linear space of $\\mathcal{H}$ spanned by $\\{X_n, \\ldots, X_1\\}$, denoted by\n",
    "\n",
    "$$\n",
    "\\mathrm{sp}\\{X_n, \\ldots, X_1\\} := \\left\\{ Y : Y = a_1 X_n + \\cdots + a_n X_1, \\, a_i \\in \\mathbb{R} \\right\\}.\n",
    "$$\n",
    "\n",
    "Since $X_1 - P(X_1 \\mid X_n, \\ldots, X_2)$ is orthogonal to $X_n, \\ldots, X_2$, i.e.,\n",
    "\n",
    "$$\n",
    "\\langle X_1 - P(X_1 \\mid X_n, \\ldots, X_2), X_k \\rangle = 0, \\quad k = 2, \\ldots, n,\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "\\mathrm{sp}\\{X_n, \\ldots, X_2, X_1\\} = \\mathrm{sp}\\{X_n, \\ldots, X_2, X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\} = \\mathrm{sp}\\{X_n, \\ldots, X_2\\} + \\mathrm{sp}\\{X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed5f74",
   "metadata": {},
   "source": [
    "Thus,\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = P(X_{n+1} \\mid X_n, \\ldots, X_2) + a \\bigl( X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\bigr), \\tag{5.5}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "a = \\frac{\\langle X_{n+1}, \\, X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle}{\\|X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\|^2}. \\tag{5.6}\n",
    "$$\n",
    "\n",
    "By stationarity, we have\n",
    "\n",
    "$$\n",
    "P(X_1 \\mid X_n, \\ldots, X_2) = \\sum_{j=1}^{n-1} \\phi_{n-1,j} X_{j+1}, \\tag{5.7}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(X_{n+1} \\mid X_n, \\ldots, X_2) = \\sum_{j=1}^{n-1} \\phi_{n-1,j} X_{n+1-j}, \\tag{5.8}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\|X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\|^2 = \\|X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2)\\|^2 = \\|X_n - P(X_n \\mid X_{n-1}, \\ldots, X_1)\\|^2 = \\nu_{n-1}. \\tag{5.9}\n",
    "$$\n",
    "\n",
    "Then Equations (5.5), (5.7), and (5.8) provide\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = a X_1 + \\sum_{j=1}^{n-1} \\bigl(\\phi_{n-1,j} - a \\phi_{n-1,n-j} \\bigr) X_{n+1-j}, \\tag{5.10}\n",
    "$$\n",
    "\n",
    "where from Equations (5.6) and (5.7),\n",
    "\n",
    "$$\n",
    "a = \\left( \\langle X_{n+1}, X_1 \\rangle - \\sum_{j=1}^{n-1} \\phi_{n-1,j} \\langle X_{n+1}, X_{j+1} \\rangle \\right) \\nu_{n-1}^{-1} = \\left( \\gamma_X(n) - \\sum_{j=1}^{n-1} \\phi_{n-1,j} \\gamma_X(n - j) \\right) \\nu_{n-1}^{-1}.\n",
    "$$\n",
    "\n",
    "Remark 5.1 told us that when $\\gamma_X(h) \\to 0$ as $h \\to \\infty$, the representation\n",
    "\n",
    "$$\n",
    "\\hat{X}_{n+1} = \\sum_{j=1}^n \\phi_{nj} X_{n+1-j} \\tag{5.11}\n",
    "$$\n",
    "\n",
    "is unique. Comparing coefficients in (5.10) and (5.11), we therefore deduce that\n",
    "\n",
    "$$\n",
    "\\phi_{nn} = a.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7296eb0",
   "metadata": {},
   "source": [
    "and\n",
    "\n",
    "$$\n",
    "\\phi_{nj} = \\phi_{n-1,j} - a \\phi_{n-1,n-j}, \\quad j = 1, \\ldots, n-1.\n",
    "$$\n",
    "\n",
    "Lastly,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nu_n &= \\| X_{n+1} - \\hat{X}_{n+1} \\|^2 \\\\\n",
    "&= \\| X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2) - a \\bigl( X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\bigr) \\|^2 \\\\\n",
    "&= \\| X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2) \\|^2 + a^2 \\| X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\|^2 \\\\\n",
    "&\\quad - 2a \\langle X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2), \\, X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle \\\\\n",
    "&= \\nu_{n-1} + a^2 \\nu_{n-1} - 2a \\langle X_{n+1}, X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Based on (5.6) and (5.9), we have\n",
    "\n",
    "$$\n",
    "\\nu_n = \\nu_{n-1} + a^2 \\nu_{n-1} - 2a^2 \\nu_{n-1} = \\nu_{n-1} (1 - a^2).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8be0c9",
   "metadata": {},
   "source": [
    "Now we prove Corollary 5.1: The partial autocorrelation $\\alpha_X(k)$ of $\\{X_t\\}$ at lag $k$ is\n",
    "$$\n",
    "\\alpha_X(k) = \\phi_{kk}, \\quad k \\geq 1.\n",
    "$$\n",
    "Proof. We have\n",
    "$$\n",
    "\\phi_{nn} = a =\n",
    "\\frac{\n",
    "\\langle X_{n+1}, X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle\n",
    "}{\n",
    "\\| X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\|^2\n",
    "}\n",
    "=\n",
    "\\frac{\n",
    "\\langle X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2), X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle\n",
    "}{\n",
    "\\| X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\|^2\n",
    "}\n",
    "$$\n",
    "$$\n",
    "=\n",
    "\\frac{\n",
    "\\langle X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2), X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\rangle\n",
    "}{\n",
    "\\| X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2) \\| \\cdot \\| X_1 - P(X_1 \\mid X_n, \\ldots, X_2) \\|\n",
    "}\n",
    "= \\operatorname{Corr}\\{X_{n+1} - P(X_{n+1} \\mid X_n, \\ldots, X_2), X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\}\n",
    "= \\alpha_X(n).\n",
    "$$\n",
    "\n",
    "**5.5.2 Recursive Prediction Using the Innovations Algorithm**\n",
    "\n",
    "The Durbin-Levinson Algorithm is based on the decomposition of\n",
    "$$\n",
    "\\operatorname{sp}\\{X_n, \\ldots, X_2, X_1\\} = \\operatorname{sp}\\{X_n, \\ldots, X_2, X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\}\n",
    "= \\operatorname{sp}\\{X_n, \\ldots, X_2\\} + \\operatorname{sp}\\{X_1 - P(X_1 \\mid X_n, \\ldots, X_2)\\}.\n",
    "$$\n",
    "\n",
    "The Innovation Algorithm is based on the decomposition of $\\operatorname{sp}\\{X_n, \\ldots, X_2, X_1\\}$ into $n$ orthogonal subspaces; i.e.,\n",
    "$$\n",
    "\\operatorname{sp}\\{X_n, \\ldots, X_1\\} = \\operatorname{sp}\\{X_1 - X_{b1}, X_2 - X_{b2}, \\ldots, X_n - X_{bn}\\}\n",
    "= \\operatorname{sp}\\{X_1 - X_{b1}\\} + \\operatorname{sp}\\{X_2 - X_{b2}\\} + \\cdots + \\operatorname{sp}\\{X_n - X_{bn}\\},\n",
    "$$\n",
    "where noting that\n",
    "$$\n",
    "X_{bi} = P(X_i \\mid X_{i-1}, \\ldots, X_1) \\quad \\text{and} \\quad X_{b1} = 0.\n",
    "$$\n",
    "Thus, we have\n",
    "$$\n",
    "X_{bn+1} = \\sum_{j=1}^n \\theta_{nj} \\left( X_{n+1-j} - X_{bn+1-j} \\right).\n",
    "$$\n",
    "We now establish the recursive scheme for computing $\\{\\theta_{nj}, j=1, \\ldots, n; \\nu_n\\}$ for $n=1, 2, \\ldots$ through the following proposition. Note that this proposition is more generally applicable than the previous one since we allow $\\{X_t\\}$ to be possibly non-stationary with mean zero and autocovariance function\n",
    "$$\n",
    "\\kappa_X(i,j) = \\langle X_i, X_j \\rangle = E(X_i X_j).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac21ac",
   "metadata": {},
   "source": [
    "\\textbf{Proposition 5.4. (The Innovations Algorithm).} If $\\{X_t\\}$ is a process with mean zero and\n",
    "$$\n",
    "E(X_i X_j) = \\kappa_X(i, j),\n",
    "$$\n",
    "where the matrix $[\\kappa_X(i, j)]_{i,j=1}^n$ is non-singular for each $n = 1, 2, \\ldots$, then the one-step predictors $X_{b,n+1}$, $n \\geq 0$, and their mean squared errors $\\nu_n$, $n \\geq 1$, are given by\n",
    "$$\n",
    "X_{b,n+1} = \n",
    "\\begin{cases}\n",
    "0, & n = 0, \\\\\n",
    "\\sum_{j=1}^n \\theta_{nj} \\left( X_{n+1-j} - X_{b,n+1-j} \\right), & n \\geq 1,\n",
    "\\end{cases}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\nu_0 = \\kappa_X(1,1), \\\\\n",
    "\\theta_{11} = \\nu_0^{-1} \\kappa_X(2,1), \\\\\n",
    "\\nu_1 = \\kappa_X(2,2) - \\theta_{11}^2 \\nu_0, \\\\\n",
    "\\theta_{nn} = \\nu_0^{-1} \\kappa_X(n+1,1), \\quad n \\geq 2, \\\\\n",
    "\\theta_{n,n-k} = \\nu_k^{-1} \\left( \\kappa_X(n+1, k+1) - \\sum_{j=0}^{k-1} \\theta_{k,k-j} \\theta_{n,n-j} \\nu_j \\right), \\quad k=1, \\ldots, n-1, \\quad n \\geq 2, \\\\\n",
    "\\nu_n = \\kappa_X(n+1,n+1) - \\sum_{j=0}^{n-1} \\theta_{n,n-j}^2 \\nu_j, \\quad n \\geq 2.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\\textbf{Proof.} By the orthogonality, we have\n",
    "$$\n",
    "\\langle X_{b,n+1}, X_{k+1} - X_{b,k+1} \\rangle = \\left\\langle \\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b,n+1-j}), \\; X_{k+1} - X_{b,k+1} \\right\\rangle = \\theta_{n,n-k} \\langle X_{k+1} - X_{b,k+1}, X_{k+1} - X_{b,k+1} \\rangle = \\theta_{n,n-k} \\nu_k.\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "\\theta_{n,n-k} = \\nu_k^{-1} \\langle X_{b,n+1}, X_{k+1} - X_{b,k+1} \\rangle = \\nu_k^{-1} \\langle X_{n+1}, X_{k+1} - X_{b,k+1} \\rangle,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\theta_{n,n-k} = \\nu_k^{-1} \\left( \\kappa_X(n+1,k+1) - \\sum_{j=1}^k \\theta_{k,j} \\langle X_{n+1}, X_{k+1-j} - X_{b,k+1-j} \\rangle \\right)\n",
    "$$\n",
    "$$\n",
    "= \\nu_k^{-1} \\left( \\kappa_X(n+1,k+1) - \\sum_{j=0}^{k-1} \\theta_{k,k-j} \\langle X_{n+1}, X_{j+1} - X_{b,j+1} \\rangle \\right)\n",
    "$$\n",
    "$$\n",
    "= \\nu_k^{-1} \\left( \\kappa_X(n+1,k+1) - \\sum_{j=0}^{k-1} \\theta_{k,k-j} \\theta_{n,n-j} \\nu_j \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5a226",
   "metadata": {},
   "source": [
    "Then\n",
    "$$\n",
    "\\nu_n = \\| X_{n+1} - X_{b,n+1} \\|^2 = \\| X_{n+1} \\|^2 - \\| X_{b,n+1} \\|^2\n",
    "= \\kappa_X(n+1, n+1) - \\left\\| \\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b,n+1-j}) \\right\\|^2\n",
    "$$\n",
    "$$\n",
    "= \\kappa_X(n+1, n+1) - \\sum_{j=1}^n \\theta_{nj}^2 \\| X_{n+1-j} - X_{b,n+1-j} \\|^2\n",
    "= \\kappa_X(n+1, n+1) - \\sum_{j=1}^n \\theta_{nj}^2 \\nu_{n-j}\n",
    "= \\kappa_X(n+1, n+1) - \\sum_{k=0}^{n-1} \\theta_{n,n-k}^2 \\nu_k.\n",
    "$$\n",
    "\n",
    "**Example 5.5. (Prediction of an MA(1) Process Using the Innovations Algorithm).** \n",
    "If $\\{X_t\\}$ is the process\n",
    "$$\n",
    "X_t = W_t + \\theta W_{t-1}, \\quad \\{W_t\\} \\sim WN(0, \\sigma^2),\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\nu_0 = (1 + \\theta^2) \\sigma^2, \\quad \\theta_{11} = \\nu_0^{-1} \\kappa_X(2,1) = \\nu_0^{-1} \\theta \\sigma^2,\n",
    "$$\n",
    "$$\n",
    "\\nu_1 = (1 + \\theta^2) \\sigma^2 - \\theta_{11}^2 \\nu_0 = \\left( 1 + \\theta^2 - \\nu_0^{-1} \\theta^2 \\sigma^2 \\right) \\sigma^2,\n",
    "$$\n",
    "$$\n",
    "\\theta_{22} = 0, \\quad \\theta_{21} = \\nu_1^{-1} \\theta \\sigma^2, \\quad \\ldots,\n",
    "$$\n",
    "$$\n",
    "\\nu_n = \\left( 1 + \\theta^2 - \\nu_n^{-1} \\theta^2 \\sigma^2 \\right) \\sigma^2.\n",
    "$$\n",
    "\n",
    "If we define $r_n = \\nu_n / \\sigma^2$, then we can write\n",
    "$$\n",
    "X_{b,n+1} = \\frac{\\theta (X_n - X_{b,n})}{r_{n-1}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "r_0 = 1 + \\theta^2, \\quad r_{n+1} = 1 + \\theta^2 - \\frac{\\theta^2}{r_n}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b528164",
   "metadata": {},
   "source": [
    "5.5.3 Recursive Calculation of the $h$-Step Predictors, $h \\geq 1$\n",
    "\n",
    "Since $\\operatorname{sp}\\{X_n, \\ldots, X_1\\}$ is a linear subspace of $\\operatorname{sp}\\{X_{n+h-1}, \\ldots, X_1\\}$, and when $j < h$, $X_{n+h-j} - X_{b,n+h-j}$ is orthogonal to $\\operatorname{sp}\\{X_n, \\ldots, X_1\\}$, we have\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1) = P\\bigl(P(X_{n+h} \\mid X_{n+h-1}, \\ldots, X_1) \\mid X_n, \\ldots, X_1 \\bigr)\n",
    "= P(X_{b,n+h} \\mid X_n, \\ldots, X_1)\n",
    "$$\n",
    "$$\n",
    "= P\\left( \\sum_{j=1}^{h-1} \\theta_{n+h-1,j} (X_{n+h-j} - X_{b,n+h-j}) \\mid X_n, \\ldots, X_1 \\right)\n",
    "= \\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j} (X_{n+h-j} - X_{b,n+h-j}),\n",
    "\\tag{5.12, 5.13}\n",
    "$$\n",
    "\n",
    "Further, the mean squared error can be expressed as\n",
    "$$\n",
    "E\\bigl\\{X_{n+h} - P(X_{n+h} \\mid X_n, \\ldots, X_1)\\bigr\\}^2 = \\| X_{n+h} \\|^2 - \\| P(X_{n+h} \\mid X_n, \\ldots, X_1) \\|^2\n",
    "= \\kappa_X(n+h, n+h) - \\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j}^2 \\nu_{n+h-j-1}.\n",
    "$$\n",
    "\n",
    "\\textbf{Remark 5.4.} Note that, while the Durbin-Levinson algorithm gives the coefficients $X_1, \\ldots, X_n$ in the representation\n",
    "$$\n",
    "X_{b,n+1} = \\sum_{j=1}^n \\phi_{nj} X_{n+1-j},\n",
    "$$\n",
    "the Innovations algorithm gives the coefficients of the \"innovations\", $(X_j - X_{b,j})$, $j=1, \\ldots, n$, in the orthogonal expansion\n",
    "$$\n",
    "X_{b,n+1} = \\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b,n+1-j}).\n",
    "$$\n",
    "The latter expansion is extremely simple to use, especially in the case of ARMA$(p,q)$ processes.\n",
    "\n",
    "5.6 Recursive Prediction of an ARMA$(p,q)$ Process\n",
    "\n",
    "For a causal ARMA$(p,q)$ process $\\{X_t\\}$ defined by\n",
    "$$\n",
    "\\phi(B) X_t = \\theta(B) W_t, \\quad \\{W_t\\} \\sim WN(0, \\sigma^2).\n",
    "$$\n",
    "Instead of applying the Innovations algorithm to $\\{X_t\\}$, we apply it to the transformed process\n",
    "$$\n",
    "Z_t = \\sigma^{-1} X_t, \\quad t=1, \\ldots, m,\n",
    "$$\n",
    "$$\n",
    "Z_t = \\sigma^{-1} \\phi(B) X_t = \\sigma^{-1} \\bigl(X_t - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p} \\bigr) = \\sigma^{-1} \\theta(B) W_t, \\quad t > m,\n",
    "$$\n",
    "where $m = \\max(p, q)$. For notational convenience, we define $\\theta_0 = 1$ and assume that $p \\geq 1$ and $q \\geq 1$.\n",
    "\n",
    "It can be seen that\n",
    "$$\n",
    "\\operatorname{sp}\\{X_1, \\ldots, X_n\\} = \\operatorname{sp}\\{Z_1, \\ldots, Z_n\\}, \\quad n \\geq 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50fa65",
   "metadata": {},
   "source": [
    "For $n \\geq 1$, we also use the notation $X_{b,n+1}$ and $Z_{b,n+1}$ to denote the predictor\n",
    "$$\n",
    "X_{b,n+1} = P(X_{n+1} \\mid X_1, \\ldots, X_n) \\quad \\text{and} \\quad Z_{b,n+1} = P(Z_{n+1} \\mid Z_1, \\ldots, Z_n),\n",
    "$$\n",
    "respectively. Of course, we have $X_{b,1} = Z_{b,1} = 0$.\n",
    "\n",
    "Now we apply the Innovations algorithm to $\\{Z_t\\}$. For $\\{Z_t\\}$, we have\n",
    "$$\n",
    "\\kappa_Z(i,j) = \n",
    "\\begin{cases}\n",
    "\\sigma^{-2} \\gamma_X(i - j), & 1 \\leq i, j \\leq m, \\\\\n",
    "\\sigma^{-2} \\left\\{ \\gamma_X(i - j) - \\sum_{r=1}^p \\phi_r \\gamma_X(r - |i-j|) \\right\\}, & \\min(i,j) \\leq m < \\max(i,j) \\leq 2m, \\\\\n",
    "\\sum_{r=0}^q \\theta_r \\theta_{r+|i-j|}, & \\min(i,j) > m, \\\\\n",
    "0, & \\text{otherwise},\n",
    "\\end{cases}\n",
    "\\tag{5.14}\n",
    "$$\n",
    "where we set $\\theta_j = 0$ for $j > q$.\n",
    "\n",
    "Then based on the Innovations algorithm, we can obtain $\\theta_{nj}$ such that\n",
    "$$\n",
    "Z_{b,n+1} = \n",
    "\\begin{cases}\n",
    "\\sum_{j=1}^n \\theta_{nj} (Z_{n+1-j} - Z_{b,n+1-j}), & 1 \\leq n < m, \\\\\n",
    "\\sum_{j=1}^q \\theta_{nj} (Z_{n+1-j} - Z_{b,n+1-j}), & n \\geq m,\n",
    "\\end{cases}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "r_n = E\\bigl(Z_{n+1} - Z_{b,n+1}\\bigr)^2.\n",
    "$$\n",
    "It is worthwhile to point out that $\\theta_{nj} = 0$ when both $n \\geq m$ and $j > q$. Why?\n",
    "\n",
    "Now, we show the relationship between $X_{b,t}$ and $Z_{b,t}$.\n",
    "\n",
    "When $t = 1, \\ldots, m$,\n",
    "$$\n",
    "Z_{b,t} = P(Z_t \\mid Z_{t-1}, \\ldots, Z_1) = P(\\sigma^{-1} X_t \\mid X_{t-1}, \\ldots, X_1) = \\sigma^{-1} X_{b,t}.\n",
    "$$\n",
    "\n",
    "For $t > m$, we have\n",
    "$$\n",
    "Z_{b,t} = P(Z_t \\mid Z_{t-1}, \\ldots, Z_1) = P\\bigl(\\sigma^{-1} \\phi(B) X_t \\mid X_{t-1}, \\ldots, X_1 \\bigr)\n",
    "= \\sigma^{-1} P\\bigl(X_t - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p} \\mid X_{t-1}, \\ldots, X_1\\bigr)\n",
    "$$\n",
    "$$\n",
    "= \\sigma^{-1} \\bigl(X_{b,t} - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p}\\bigr)\n",
    "= \\sigma^{-1} \\bigl\\{X_{b,t} + \\phi(B) X_t - X_t \\bigr\\}.\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "X_t - X_{b,t} = \\sigma (Z_t - Z_{b,t}), \\quad \\forall t \\geq 1.\n",
    "$$\n",
    "\n",
    "Thus, when $1 \\leq n < m$,\n",
    "$$\n",
    "X_{b,n+1} = X_{n+1} - \\sigma (Z_{n+1} - Z_{b,n+1}) = (X_{n+1} - \\sigma Z_{n+1}) + \\sigma Z_{b,n+1} = \\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b,n+1-j}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa9593",
   "metadata": {},
   "source": [
    "When $n \\geq m$,\n",
    "\n",
    "$$\n",
    "X_{b_{n+1}} = X_{n+1} - \\sigma(Z_{n+1} - Z_{b_{n+1}}) = X_{n+1} - (X_{n+1} - \\phi_1 X_n - \\cdots - \\phi_p X_{n+1-p}) + \\sigma Z_{b_{n+1}} \\\\\n",
    "= \\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + \\sum_{j=1}^q \\theta_{nj} (X_{n+1-j} - X_{b_{n+1-j}}).\n",
    "$$\n",
    "\n",
    "Thus, we have\n",
    "\n",
    "$$\n",
    "X_{b_{n+1}} = \\begin{cases}\n",
    "\\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b_{n+1-j}}), & 1 \\leq n < m, \\\\\n",
    "\\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + \\sum_{j=1}^q \\theta_{nj} (X_{n+1-j} - X_{b_{n+1-j}}), & n \\geq m,\n",
    "\\end{cases}\n",
    "\\tag{5.15}\n",
    "$$\n",
    "\n",
    "and further,\n",
    "\n",
    "$$\n",
    "E\\bigl(X_{n+1} - X_{b_{n+1}}\\bigr)^2 = \\sigma^2 E\\bigl(Z_{n+1} - Z_{b_{n+1}}\\bigr)^2 = \\sigma^2 r_n.\n",
    "\\tag{5.16}\n",
    "$$\n",
    "\n",
    "Equations (5.14), (5.15) and (5.16) provide a recursive calculation of one-step predictor $P(X_{n+1} \\mid X_n, \\ldots, X_1)$ for a general ARMA$(p,q)$ process.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark 5.5.** Note that the covariance $\\kappa_Z(i,j)$ depends only on $\\phi_1, \\ldots, \\phi_p$, $\\theta_1, \\ldots, \\theta_q$ and not on $\\sigma^2$. The same is therefore true of $\\theta_{nj}$ and $r_n$.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark 5.6.** The representation (5.15) is particularly convenient from a practical point of view, not only because of the simple recursion relations for the coefficients, but also because for $n \\geq m$ it requires the storage of at most $p$ past observations $X_n, \\ldots, X_{n+1-p}$ and at most $q$ past innovations $(X_{n+1-j} - X_{b_{n+1-j}})$, $j=1,\\ldots,q$, in order to predict $X_{n+1}$. Direct application of the Innovations algorithm to ${X_t}$, on the other hand, leads to a representation of $X_{b_{n+1}}$ in terms of all the $n$ preceding innovations $(X_j - X_{b_j})$, $j=1, \\ldots, n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b9418",
   "metadata": {},
   "source": [
    "**Example 5.6. (Prediction of an MA(q) Process).**\n",
    "For an MA$(q)$ process; i.e., ${X_t}$ is defined as\n",
    "\n",
    "$$\n",
    "X_t = W_t + \\theta_1 W_{t-1} + \\cdots + \\theta_q W_{t-q},\n",
    "$$\n",
    "\n",
    "which can be viewed as an ARMA(1, q) process. Thus, we can apply (5.15) and obtain\n",
    "\n",
    "$$\n",
    "X_{b_{n+1}} = \\begin{cases}\n",
    "\\sum_{j=1}^n \\theta_{nj} (X_{n+1-j} - X_{b_{n+1-j}}), & 1 \\leq n < q, \\\\\n",
    "\\sum_{j=1}^q \\theta_{nj} (X_{n+1-j} - X_{b_{n+1-j}}), & n \\geq q,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where the coefficients $\\theta_{nj}$ are found by the Innovations algorithm using the covariance $\\kappa_Z(i,j)$, where\n",
    "\n",
    "$$\n",
    "Z_t = \\begin{cases}\n",
    "\\sigma^{-1} X_t, & t = 1, \\ldots, q, \\\\\n",
    "\\sigma^{-1} \\phi(B) X_t = \\sigma^{-1} X_t, & t > q,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus the process ${Z_t}$ and ${\\sigma^{-1} X_t}$ are identical, and the covariances are simply\n",
    "\n",
    "$$\n",
    "\\kappa_Z(i,j) = \\sigma^{-2} \\gamma_X(i-j) = \\sigma^{-2} \\sum_{r=0}^{q - |i-j|} \\theta_r \\theta_{r+|i-j|}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6.1 h-step prediction of an ARMA(p, q) process\n",
    "\n",
    "When $h \\geq 1$, similarly as in Section 5.5.3; based on (5.12), we have\n",
    "\n",
    "$$\n",
    "P(Z_{b_{n+h}} \\mid Z_n, \\ldots, Z_1) = P(Z_{b_{n+h}} \\mid X_n, \\ldots, X_1) = \\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j} \\bigl(Z_{n+h-j} - Z_{b_{n+h-j}}\\bigr) = \\sigma^{-1} \\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j} \\bigl(X_{n+h-j} - X_{b_{n+h-j}}\\bigr).\n",
    "$$\n",
    "\n",
    "Then when $1 \\leq h \\leq m - n$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1) &= P\\bigl(X_{n+h} - \\sigma(Z_{n+h} - Z_{b_{n+h}}) \\mid X_n, \\ldots, X_1 \\bigr) \\\\\n",
    "&= \\sigma P(Z_{b_{n+h}} \\mid X_n, \\ldots, X_1) = \\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j} \\bigl(X_{n+h-j} - X_{b_{n+h-j}}\\bigr).\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7d1229ce",
   "metadata": {},
   "source": [
    "When $h > m - n$,\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1) = P \\big\\{ X_{n+h} - \\sigma (Z_{n+h} - Z_{b_{n+h}}) \\mid X_n, \\ldots, X_1 \\big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= P \\left( \\sum_{i=1}^p \\phi_i X_{n+h-i} \\mid X_n, \\ldots, X_1 \\right) + \\sigma P \\{ Z_{b_{n+h}} \\mid X_n, \\ldots, X_1 \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^p \\phi_i P(X_{n+h-i} \\mid X_n, \\ldots, X_1) + \\sum_{j=h}^{n+h-1} \\theta_{n+h-1, j} (X_{n+h-j} - X_{b_{n+h-j}})\n",
    "$$\n",
    "\n",
    "Since $\\theta_{nj} = 0$ when both $n \\geq m$ and $j > q$, we have obtained the $h$-step predictor\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1) =\n",
    "\\begin{cases}\n",
    "\\sum_{j=h}^{n+h-1} \\theta_{n+h-1,j} (X_{n+h-j} - X_{b_{n+h-j}}), & 1 \\leq h \\leq m - n, \\\\\n",
    "\\sum_{i=1}^p \\phi_i P(X_{n+h-i} \\mid X_n, \\ldots, X_1) + \\sum_{j=h}^q \\theta_{n+h-1,j} (X_{n+h-j} - X_{b_{n+h-j}}), & h > m - n.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "One last thing I would like to say about prediction is that, under a Gaussian assumption, we can use the prediction and the prediction mean squared error to construct a confidence interval. For an unknown $X_{n+h}$, a 95% confidence interval is\n",
    "\n",
    "$$\n",
    "P(X_{n+h} \\mid X_n, \\ldots, X_1) \\pm 1.96 \\left[ \\mathbb{E} \\left\\{ X_{n+h} - P(X_{n+h} \\mid X_n, \\ldots, X_1) \\right\\}^2 \\right]^{1/2}\n",
    "$$\n",
    "\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5ae07",
   "metadata": {},
   "source": [
    "## 5.7 Miscellanea\n",
    "\n",
    "**Example 5.7. (Prediction of an AR(p) Process).**\n",
    "\n",
    "For an AR(p) process; i.e., $(\\{X_t\\})$ satisfies that\n",
    "\n",
    "$$\n",
    "X_t - \\phi_1 X_{t-1} - \\cdots - \\phi_p X_{t-p} = W_t\n",
    "$$\n",
    "\n",
    "and $$(\\mathrm{Cov}(W_t, X_s) = 0)$$ for $(s < t)$. With no difficulty, we can see that, when $(n \\geq p)$,\n",
    "\n",
    "$$\n",
    "\\hat{X}_{b_{n+1}} = P(X_{b_{n+1}} \\mid X_n, \\ldots, X_1)\n",
    "= P(\\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + W_t \\mid X_n, \\ldots, X_1)\n",
    "= \\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p}.\n",
    "$$\n",
    "\n",
    "The one-step prediction is fully determined by its previous $(p)$ observations. Further,\n",
    "\n",
    "$$\n",
    "E\\{X_{n+1} - \\hat{X}_{b_{n+1}}\\}^2 = E W_t^2 = \\sigma^2.\n",
    "$$\n",
    "\n",
    "Noting that, we can view that, conditional on $(\\{X_n, \\ldots, X_1\\})$, \n",
    "\n",
    "$$\n",
    "X_{n+1} = \\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p} + W_t\n",
    "$$\n",
    "\n",
    "follows a distribution with mean $(\\phi_1 X_n + \\cdots + \\phi_p X_{n+1-p})$ and variance $(\\sigma^2)$. Recall that the optimal predictor of $(X)$ given $(Y)$ is $(E(X \\mid Y))$. Thus, in our AR(p) case, we have the optimal predictor being the same as the optimal linear predictor.\n",
    "\n",
    "The other interesting thing is that, in this example, neither the Durbin-Levinson algorithm nor the Innovations algorithm is needed to compute $(\\hat{X}_{b_{n+1}})$. Notice that, in both algorithms, we need the autocovariance or more generally the covariance function. For example, in the Durbin-Levinson algorithm, we have\n",
    "\n",
    "$$\n",
    "\\hat{X}_{b_{n+1}} = \\phi_{n1} X_n + \\cdots + \\phi_{nn} X_1, \\quad n \\geq 1,\n",
    "$$\n",
    "\n",
    "and its mean squared error of prediction will be denoted by $(\\nu_n)$ as\n",
    "\n",
    "$$\n",
    "\\nu_n = E(X_{n+1} - \\hat{X}_{b_{n+1}})^2, \\quad n \\geq 1,\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\phi_{11} = \\frac{\\gamma_X(1)}{\\gamma_X(0)}, \\quad \\nu_0 = \\gamma_X(0),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_{nn} = \\frac{\\gamma_X(n) - \\sum_{j=1}^{n-1} \\phi_{n-1,j} \\gamma_X(n-j)}{\\nu_{n-1}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\phi_{n1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n,n-1}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\phi_{n-1,1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n-1,n-1}\n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\phi_{nn}\n",
    "\\begin{pmatrix}\n",
    "\\phi_{n-1,n-1} \\\\\n",
    "\\vdots \\\\\n",
    "\\phi_{n-1,1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\nu_n = \\nu_{n-1}(1 - \\phi_{nn}^2).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954ee33",
   "metadata": {},
   "source": [
    "Thus, with the acknowledgement of $(\\gamma_X(h))$, the coefficients $(\\phi_n,j)$ are fully determined. Now we know $(\\phi_{n,j})$ from the model definition directly. It does not make use of $(\\gamma_X(h))$ of $(\\{X_t\\})$. Naturally, a question rises as: can we use the $(\\phi)$ to find all the $(\\gamma_X(h))$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376815c6",
   "metadata": {},
   "source": [
    "\n",
    "To do so, we look at the Step-Down Durbin-Levinson algorithm: given $(\\phi_{n1}, \\ldots, \\phi_{nn})$ and $(\\nu_n)$,\n",
    "\n",
    "$$\n",
    "\\phi_{n-1,j} = \\frac{\\phi_{nj} + \\phi_{nn} \\phi_{n,n-j}}{1 - \\phi_{nn}^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nu_{n-1} = \\frac{\\nu_n}{1 - \\phi_{nn}^2}\n",
    "$$\n",
    "\n",
    "Thus, start with $(\\phi_{p,1} = \\phi_1, \\ldots, \\phi_{p,p} = \\phi_p)$ and $(\\nu_p = \\sigma^2)$, we can step down recursively to get\n",
    "\n",
    "$$\n",
    "\\phi_{p-1,j} \\text{ and } \\nu_{p-1}, \\quad \\phi_{p-2,j} \\text{ and } \\nu_{p-2}, \\quad \\ldots, \\quad \\phi_{11} \\text{ and } \\nu_1.\n",
    "$$\n",
    "\n",
    "Then we can find all the $(\\gamma_X(h))$ via\n",
    "\n",
    "$$\n",
    "\\gamma_X(0) = \\nu_0 = \\frac{\\nu_1}{1 - \\phi_{11}^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_X(1) = \\gamma_X(0) \\phi_{11}\n",
    "$$\n",
    "\n",
    "and for $(n \\geq 2)$,\n",
    "\n",
    "$$\n",
    "\\gamma_X(n) = \\nu_{n-1} \\phi_{nn} + \\sum_{j=1}^{n-1} \\phi_{n-1,j} \\gamma_X(n - j).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_X(2) &= \\phi_{22} \\nu_1 + \\phi_{11} \\gamma_X(1), \\\\\n",
    "\\gamma_X(3) &= \\phi_{33} \\nu_2 + \\phi_{32} \\gamma_X(2) + \\phi_{22} \\gamma_X(1), \\\\\n",
    "&\\vdots \\\\\n",
    "\\gamma_X(p) &= \\phi_{pp} \\nu_{p-1} + \\phi_{p-1,1} \\gamma_X(p-1) + \\cdots + \\phi_{p-1,p-1} \\gamma_X(1).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Noting that, once we have $(\\gamma_X(0), \\ldots, \\gamma_X(p))$, for $(k \\geq p + 1)$,\n",
    "\n",
    "$$\n",
    "\\gamma_X(k) = \\phi_1 \\gamma_X(k - 1) + \\cdots + \\phi_p \\gamma_X(k - p).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3828acb",
   "metadata": {},
   "source": [
    "**Example 5.8. (Generating a realization of an ARMA(p, q) process.)**\n",
    "\n",
    "How to generate exact realizations of ARMA processes? Let us consider generating stationary and causal Gaussian AR(p) processes:\n",
    "\n",
    "$$\n",
    "Y_t - \\phi_1 Y_{t-1} - \\cdots - \\phi_p Y_{t-p} = W_t, \\quad W_t \\sim N(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "Recall that, for any $t \\geq p + 1$, we have\n",
    "\n",
    "$$\n",
    "\\hat{Y}_t = P(Y_t \\mid Y_{t-1}, \\ldots, Y_1) = \\phi_1 Y_{t-1} + \\cdots + \\phi_p Y_{t-p}.\n",
    "$$\n",
    "\n",
    "The innovations are\n",
    "\n",
    "$$\n",
    "U_t = Y_t - \\hat{Y}_t = Z_t,\n",
    "$$\n",
    "\n",
    "with mean squared error (MSE) being\n",
    "\n",
    "$$\n",
    "\\nu_{t-1} = \\mathrm{Var}(U_t) = \\sigma^2.\n",
    "$$\n",
    "\n",
    "Now, we can use step-down Levinson-Durbin recursions to get coefficients for\n",
    "\n",
    "$$\n",
    "\\hat{Y}_t = \\phi_{t-1,1} Y_{t-1} + \\cdots + \\phi_{t-1,t-1} Y_1, \\quad t = 2, 3, \\ldots, p,\n",
    "$$\n",
    "\n",
    "and also the associated MSEs $\\nu_{t-1}$.\n",
    "\n",
    "Now we have all the innovations $U_t = Y_t - \\hat{Y}_t$, for $t = 1, \\ldots, p$, where\n",
    "\n",
    "1. $E[U_t] = 0$ and $\\mathrm{Var}(U_t) = \\nu_{t-1}$.\n",
    "2. $U_1, U_2, \\ldots, U_p$ are uncorrelated random variables (by the Gaussian assumption, this means they are independent normals).\n",
    "\n",
    "Thus, we can easily simulate $U_t$, $t = 1, \\ldots, p$. Then we unroll $U_t$ to get simulations for $Y_t$ for $t = 1, \\ldots, p$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U_1 &= Y_1, \\\\\n",
    "U_2 &= Y_2 - \\phi_{11} Y_1, \\\\\n",
    "U_3 &= Y_3 - \\phi_{21} Y_2 - \\phi_{22} Y_1, \\\\\n",
    "&\\vdots \\\\\n",
    "U_p &= Y_p - \\phi_{p-1,1} Y_{p-1} - \\cdots - \\phi_{p-1,p-1} Y_1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now, we have $\\{Y_t, t = 1, \\ldots, p\\}$, we can start generating $Y_t$ for $t > p$ based on the definition of the AR(p) model.\n",
    "\n",
    "Once we know how to simulate the AR process $\\phi(B) Y_t = Z_t$, we can easily simulate an ARMA process $\\phi(B) X_t = \\theta(B) Z_t$ based on\n",
    "\n",
    "$$\n",
    "X_t = \\theta(B) Y_t.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815d8d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
