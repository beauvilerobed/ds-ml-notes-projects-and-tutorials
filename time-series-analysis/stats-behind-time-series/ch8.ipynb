{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "676a908c",
   "metadata": {},
   "source": [
    "# 8 Multivariate Time Series\n",
    "\n",
    "In this section, we consider $m$ time series $\\{ X_{t i}, t = 0, \\pm 1, \\pm 2, \\ldots \\}$, $i = 1, 2, \\ldots, m$ jointly.\n",
    "\n",
    "## 8.1 Second order properties of multivariate time series\n",
    "\n",
    "Denote\n",
    "\n",
    "$$\n",
    "X_t = (X_{t1}, \\ldots, X_{tm})^T, \\quad t = 0, \\pm 1, \\pm 2, \\ldots\n",
    "$$\n",
    "\n",
    "The second-order properties of the multivariate time series $\\{ X_t \\}$ are then specified by the mean vectors\n",
    "\n",
    "$$\n",
    "\\mu_t = E X_t = (\\mu_{t1}, \\ldots, \\mu_{tm})^T,\n",
    "$$\n",
    "\n",
    "and covariance matrices\n",
    "\n",
    "$$\n",
    "\\Gamma(t+h, t) = \\text{Cov}(X_{t+h}, X_t) = E \\{ (X_{t+h} - \\mu_{t+h})(X_t - \\mu_t)^T \\} = [\\gamma_{ij}(t+h, t)]_{i,j=1}^m.\n",
    "$$\n",
    "\n",
    "**(Stationary Multivariate Time Series).** The series $\\{ X_t \\}$ is said to be stationary if $\\mu_t$ and $\\Gamma(t+h, t)$, $h = 0, \\pm 1, \\pm 2, \\ldots$, are independent of $t$.\n",
    "\n",
    "For a stationary series, we shall use the notation\n",
    "\n",
    "$$\n",
    "\\mu = E X_t\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Gamma(h) = E \\{ (X_{t+h} - \\mu)(X_t - \\mu)^T \\} = [\\gamma_{ij}(h)]_{i,j=1}^m\n",
    "$$\n",
    "\n",
    "to represent the mean of the series and the covariance matrix at lag $h$, respectively.\n",
    "\n",
    "Note that, for each $i$, $\\{ X_{t i} \\}$ is stationary with covariance function $\\gamma_{ii}(\\cdot)$ and mean function $\\mu_i$.\n",
    "\n",
    "The function $\\gamma_{ij}(\\cdot)$ where $i \\neq j$ is called the **cross-covariance function** of the two series $\\{ X_{t i} \\}$ and $\\{ X_{t j} \\}$. It should be noted that $\\gamma_{ij}(\\cdot)$ is not in general the same as $\\gamma_{ji}(\\cdot)$.\n",
    "\n",
    "Further, the correlation matrix function $R(\\cdot)$ is defined by\n",
    "\n",
    "$$\n",
    "R(h) = \\left[ \\frac{\\gamma_{ij}(h)}{\\sqrt{\\gamma_{ii}(0) \\gamma_{jj}(0)}} \\right]_{i,j=1}^m = [\\rho_{ij}(h)]_{i,j=1}^m.\n",
    "$$\n",
    "\n",
    "The function $R(\\cdot)$ is the covariance matrix function of the normalized series obtained by subtracting $\\mu$ from $X_t$ and then dividing each component by its standard deviation.\n",
    "\n",
    "---\n",
    "\n",
    "**Lemma 8.1.** The covariance matrix function $\\Gamma(\\cdot) = [\\gamma_{ij}(\\cdot)]_{i,j=1}^m$ of a stationary time series $\\{ X_t \\}$ has the properties\n",
    "\n",
    "1. $\\Gamma(h) = \\Gamma^T(-h)$;\n",
    "2. $|\\gamma_{ij}(h)| \\leq \\sqrt{\\gamma_{ii}(0) \\gamma_{jj}(0)}$, $i, j = 1, \\ldots, m$;\n",
    "3. $\\gamma_{ii}(\\cdot)$ is an autocovariance function, $i = 1, \\ldots, m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb254fb7",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "$$\n",
    "\\sum_{j,k=1}^n a_j^0 \\Gamma(j-k) a_k \\geq 0 \\quad \\text{for all } n \\in \\{1, 2, \\ldots\\} \\text{ and } a_1, \\ldots, a_n \\in \\mathbb{R}^m.\n",
    "$$\n",
    "\n",
    "And the correlation matrix $R$ satisfies the above four properties and further\n",
    "\n",
    "$$\n",
    "\\rho_{ii}(0) = 1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example 8.1.** Consider the bivariate stationary process $\\{ X_t \\}$ defined by\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "X_{t1} = W_t \\\\\n",
    "X_{t2} = W_t + 0.75 W_{t-10}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\{ W_t \\} \\sim WN(0,1)$ (white noise with mean 0 and variance 1). Then\n",
    "\n",
    "$$\n",
    "\\mu = 0,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Gamma(-10) = \\text{Cov}\\left(\n",
    "\\begin{pmatrix} X_{t-10,1} \\\\ X_{t-10,2} \\end{pmatrix},\n",
    "\\begin{pmatrix} X_{t,1} \\\\ X_{t,2} \\end{pmatrix}\n",
    "\\right)\n",
    "= \\text{Cov}\\left(\n",
    "\\begin{pmatrix} W_{t-10} \\\\ W_{t-10} + 0.75 W_{t-20} \\end{pmatrix},\n",
    "\\begin{pmatrix} W_t \\\\ W_t + 0.75 W_{t-10} \\end{pmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 & 0.75 \\\\\n",
    "0 & 0.75\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma(0) = \\text{Cov}\\left(\n",
    "\\begin{pmatrix} X_{t,1} \\\\ X_{t,2} \\end{pmatrix},\n",
    "\\begin{pmatrix} X_{t,1} \\\\ X_{t,2} \\end{pmatrix}\n",
    "\\right)\n",
    "= \\text{Cov}\\left(\n",
    "\\begin{pmatrix} W_t \\\\ W_t + 0.75 W_{t-10} \\end{pmatrix},\n",
    "\\begin{pmatrix} W_t \\\\ W_t + 0.75 W_{t-10} \\end{pmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1.5625\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma(10) = \\text{Cov}\\left(\n",
    "\\begin{pmatrix} X_{t+10,1} \\\\ X_{t+10,2} \\end{pmatrix},\n",
    "\\begin{pmatrix} X_{t,1} \\\\ X_{t,2} \\end{pmatrix}\n",
    "\\right)\n",
    "= \\text{Cov}\\left(\n",
    "\\begin{pmatrix} W_{t+10} \\\\ W_{t+10} + 0.75 W_t \\end{pmatrix},\n",
    "\\begin{pmatrix} W_t \\\\ W_t + 0.75 W_{t-10} \\end{pmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0.75 & 0.75\n",
    "\\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf676d1",
   "metadata": {},
   "source": [
    "Otherwise, $\\Gamma(j) = 0$. The correlation matrix function is given by\n",
    "\n",
    "$$\n",
    "R(-10) = \\begin{pmatrix}\n",
    "0 & 0.6 \\\\\n",
    "0 & 0.48\n",
    "\\end{pmatrix}, \\quad\n",
    "R(0) = \\begin{pmatrix}\n",
    "1 & 0.8 \\\\\n",
    "0.8 & 1\n",
    "\\end{pmatrix}, \\quad\n",
    "R(10) = \\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0.6 & 0.48\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "and $R(j) = 0$ otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "**(Multivariate White Noise).** The $m$-variate series $\\{ W_t, t = 0, \\pm 1, \\pm 2, \\ldots \\}$ is said to be white noise with mean 0 and covariance matrix $\\Sigma$, written as\n",
    "\n",
    "$$\n",
    "\\{ W_t \\} \\sim WN(0, \\Sigma),\n",
    "$$\n",
    "\n",
    "if and only if $\\{ W_t \\}$ is stationary with mean vector 0 and covariance matrix function\n",
    "\n",
    "$$\n",
    "\\Gamma(h) =\n",
    "\\begin{cases}\n",
    "\\Sigma & \\text{if } h = 0, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If further, the $\\{ W_t \\}$ are independent, then we write\n",
    "\n",
    "$$\n",
    "\\{ W_t \\} \\sim IID(0, \\Sigma).\n",
    "$$\n",
    "\n",
    "Multivariate white noise is used as a building block from which an enormous variety of multivariate time series can be constructed. The **linear processes** are those of the form\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=-\\infty}^{\\infty} C_j W_{t-j}, \\quad \\{ W_t \\} \\sim WN(0, \\Sigma),\n",
    "$$\n",
    "\n",
    "where $\\{ C_j \\}$ is a sequence of matrices whose components are absolutely summable. It is easy to see that this linear process has mean 0 and covariance matrix function\n",
    "\n",
    "$$\n",
    "\\Gamma(h) = \\sum_{j=-\\infty}^{\\infty} C_{j+h} \\, \\Sigma \\, C_j^T, \\quad h = 0, \\pm 1, \\pm 2, \\ldots\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Estimation of $\\mu$.** Based on the observations $X_1, \\ldots, X_n$, an unbiased estimate of $\\mu$ is given by the vector of sample means\n",
    "\n",
    "$$\n",
    "\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t.\n",
    "$$\n",
    "\n",
    "This estimator is consistent and asymptotically normal with rate $\\sqrt{n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d667c0",
   "metadata": {},
   "source": [
    "### Estimation of $\\Gamma(h)$\n",
    "\n",
    "Based on the observations $X_1, \\ldots, X_n$, as in the univariate case, a natural estimate of the covariance matrix $\\Gamma(h)$ is\n",
    "\n",
    "$$\n",
    "\\hat{\\Gamma}(h) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{n-1} \\sum_{t=1}^{n-h} (X_{t+h} - \\bar{X}_n)(X_t - \\bar{X}_n)^T, & 0 \\leq h \\leq n-1, \\\\\n",
    "\\\\\n",
    "\\frac{1}{n-1} \\sum_{t=-h+1}^n (X_{t+h} - \\bar{X}_n)(X_t - \\bar{X}_n)^T, & -n+1 \\leq h < 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\bar{X}_n = \\frac{1}{n} \\sum_{t=1}^n X_t$.\n",
    "\n",
    "Denoting $\\hat{\\Gamma}(h) = [\\hat{\\gamma}_{ij}(h)]_{i,j=1}^m$, we estimate the cross-correlation function by\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_{ij}(h) = \\frac{\\hat{\\gamma}_{ij}(h)}{\\sqrt{\\hat{\\gamma}_{ii}(0) \\hat{\\gamma}_{jj}(0)}}.\n",
    "$$\n",
    "\n",
    "If $i = j$, this reduces to the sample autocorrelation function of the $i$-th series.\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem 8.1\n",
    "\n",
    "Let $\\{X_t\\}$ be the bivariate time series\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{k=-\\infty}^{\\infty} C_k W_{t-k}, \\quad \\{ W_t = (W_{t1}, W_{t2})^T \\} \\sim IID(0, \\Sigma),\n",
    "$$\n",
    "\n",
    "where $\\{ C_k = [C_k(i,j)]_{i,j=1}^2 \\}$ is a sequence of matrices with\n",
    "\n",
    "$$\n",
    "\\sum_{k=-\\infty}^{\\infty} |C_k(i,j)| < \\infty, \\quad i,j=1,2.\n",
    "$$\n",
    "\n",
    "Then as $n \\to \\infty$,\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}_{ij}(h) \\xrightarrow{p} \\gamma_{ij}(h),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_{ij}(h) \\xrightarrow{p} \\rho_{ij}(h),\n",
    "$$\n",
    "\n",
    "for each fixed $h \\geq 0$ and for $i,j = 1,2$.\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem 8.2\n",
    "\n",
    "Suppose that\n",
    "\n",
    "$$\n",
    "X_{t1} = \\sum_{j=-\\infty}^\\infty \\alpha_j W_{t-j,1}, \\quad \\{ W_{t1} \\} \\sim IID(0, \\sigma_1^2),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "X_{t2} = \\sum_{j=-\\infty}^\\infty \\beta_j W_{t-j,2}, \\quad \\{ W_{t2} \\} \\sim IID(0, \\sigma_2^2),\n",
    "$$\n",
    "\n",
    "where the two sequences $\\{ W_{t1} \\}$ and $\\{ W_{t2} \\}$ are independent, and\n",
    "\n",
    "$$\n",
    "\\sum_j |\\alpha_j| < \\infty, \\quad \\sum_j |\\beta_j| < \\infty.\n",
    "$$\n",
    "\n",
    "Then if $h \\geq 0$,\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_{12}(h) \\sim AN\\left(0, n^{-1} \\sum_{j=-\\infty}^\\infty \\rho_{11}(j) \\rho_{22}(j) \\right),\n",
    "$$\n",
    "\n",
    "where $AN$ stands for asymptotically normal distribution.\n",
    "\n",
    "If $h, k \\geq 0$ and $h \\neq k$, then\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\rho}_{12}(h) \\\\\n",
    "\\hat{\\rho}_{12}(k)\n",
    "\\end{pmatrix}\n",
    "\\sim AN \\left(\n",
    "0,\n",
    "\\begin{pmatrix}\n",
    "n^{-1} \\sum_{j=-\\infty}^\\infty \\rho_{11}(j) \\rho_{22}(j) & n^{-1} \\sum_{j=-\\infty}^\\infty \\rho_{11}(j) \\rho_{22}(j + k - h) \\\\\n",
    "n^{-1} \\sum_{j=-\\infty}^\\infty \\rho_{11}(j) \\rho_{22}(j + k - h) & n^{-1} \\sum_{j=-\\infty}^\\infty \\rho_{11}(j) \\rho_{22}(j)\n",
    "\\end{pmatrix}\n",
    "\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f0988",
   "metadata": {},
   "source": [
    "This theorem plays an important role in testing for correlation between two processes. If one of the two processes is white noise, then\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}_{12}(h) \\sim AN\\left(0, n^{-1}\\right),\n",
    "$$\n",
    "\n",
    "in which case it is straightforward to test the hypothesis that $\\rho_{12}(h) = 0$. The rejection region is\n",
    "\n",
    "$$\n",
    "|\\hat{\\rho}_{12}(h)| > \\frac{z_{\\alpha/2}}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "However, if neither process is white noise, then a value of $|\\hat{\\rho}_{12}(h)|$ which is large relative to $n^{-1/2}$ does not necessarily indicate that $\\rho_{12}(h)$ is different from zero. For example, suppose that $\\{ X_{t1} \\}$ and $\\{ X_{t2} \\}$ are two independent and identical AR(1) processes with\n",
    "\n",
    "$$\n",
    "\\rho_{11}(h) = \\rho_{22}(h) = 0.8^{|h|}.\n",
    "$$\n",
    "\n",
    "Then the asymptotic variance of $\\hat{\\rho}_{12}(h)$ is\n",
    "\n",
    "$$\n",
    "n^{-1} \\left(1 + 2 \\sum_{k=1}^\\infty (0.8)^{2k} \\right) = 4.556 \\, n^{-1}.\n",
    "$$\n",
    "\n",
    "Thus, the rejection region is\n",
    "\n",
    "$$\n",
    "|\\hat{\\rho}_{12}(h)| > z_{\\alpha/2} \\frac{\\sqrt{4.556}}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "It would not be surprising to observe a value of $\\hat{\\rho}_{12}(h)$ as large as $3 n^{-1/2}$ even though $\\{ X_{t1} \\}$ and $\\{ X_{t2} \\}$ are independent. On the other hand, if\n",
    "\n",
    "$$\n",
    "\\rho_{11}(h) = 0.8^{|h|} \\quad \\text{and} \\quad \\rho_{22}(h) = (-0.8)^{|h|},\n",
    "$$\n",
    "\n",
    "then the asymptotic variance of $\\hat{\\rho}_{12}(h)$ is $0.2195 n^{-1}$, and an observed value of $3 n^{-1/2}$ for $\\hat{\\rho}_{12}(h)$ would be very unlikely.\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 Multivariate ARMA processes\n",
    "\n",
    "**(Multivariate ARMA(p, q) process).** The process $\\{ X_t, t = 0, \\pm 1, \\pm 2, \\ldots \\}$ is an $m$-variate ARMA(p, q) process if $\\{ X_t \\}$ is a stationary solution of the difference equations\n",
    "\n",
    "$$\n",
    "X_t - \\Phi_1 X_{t-1} - \\cdots - \\Phi_p X_{t-p} = W_t + \\Theta_1 W_{t-1} + \\cdots + \\Theta_q W_{t-q},\n",
    "$$\n",
    "\n",
    "where $\\Phi_1, \\ldots, \\Phi_p, \\Theta_1, \\ldots, \\Theta_q$ are real $m \\times m$ matrices, and $\\{ W_t \\} \\sim WN(0, \\Sigma)$.\n",
    "\n",
    "We can write this more compactly as\n",
    "\n",
    "$$\n",
    "\\Phi(B) X_t = \\Theta(B) W_t, \\quad \\{ W_t \\} \\sim WN(0, \\Sigma),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Phi(z) = I - \\Phi_1 z - \\cdots - \\Phi_p z^p,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Theta(z) = I + \\Theta_1 z + \\cdots + \\Theta_q z^q,\n",
    "$$\n",
    "\n",
    "are matrix-valued polynomials, and $I$ is the $m \\times m$ identity matrix.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 8.2. (Multivariate AR(1) process).**\n",
    "\n",
    "This process satisfies\n",
    "\n",
    "$$\n",
    "X_t = \\Phi X_{t-1} + W_t, \\quad \\{ W_t \\} \\sim WN(0, \\Sigma).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd1179",
   "metadata": {},
   "source": [
    "We have\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=0}^{\\infty} \\Phi^j W_{t-j},\n",
    "$$\n",
    "\n",
    "provided all the eigenvalues of $\\Phi$ are less than 1 in absolute value; that is,\n",
    "\n",
    "$$\n",
    "\\det(I - z\\Phi) \\ne 0 \\quad \\text{for all } z \\in \\mathbb{C} \\text{ such that } |z| \\leq 1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem 8.3 (Causality Criterion)\n",
    "\n",
    "If\n",
    "\n",
    "$$\n",
    "\\det \\Phi(z) \\ne 0 \\quad \\text{for all } z \\in \\mathbb{C} \\text{ such that } |z| \\leq 1,\n",
    "$$\n",
    "\n",
    "then there exists exactly one **stationary solution** of the ARMA process:\n",
    "\n",
    "$$\n",
    "X_t = \\sum_{j=0}^{\\infty} \\Psi_j W_{t-j},\n",
    "$$\n",
    "\n",
    "where the matrices $\\Psi_j$ are determined uniquely by the generating function\n",
    "\n",
    "$$\n",
    "\\Psi(z) = \\sum_{j=0}^{\\infty} \\Psi_j z^j = \\Phi^{-1}(z) \\Theta(z), \\quad |z| \\leq 1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Theorem 8.4 (Invertibility Criterion)\n",
    "\n",
    "If\n",
    "\n",
    "$$\n",
    "\\det \\Theta(z) \\ne 0 \\quad \\text{for all } z \\in \\mathbb{C} \\text{ such that } |z| \\leq 1,\n",
    "$$\n",
    "\n",
    "and $\\{X_t\\}$ is a stationary solution of the ARMA equation, then the white noise sequence $\\{W_t\\}$ can be recovered as\n",
    "\n",
    "$$\n",
    "W_t = \\sum_{j=0}^{\\infty} \\Pi_j X_{t-j},\n",
    "$$\n",
    "\n",
    "where the matrices $\\Pi_j$ are uniquely determined by\n",
    "\n",
    "$$\n",
    "\\Pi(z) = \\sum_{j=0}^{\\infty} \\Pi_j z^j = \\Theta^{-1}(z) \\Phi(z), \\quad |z| \\leq 1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4ae9d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
