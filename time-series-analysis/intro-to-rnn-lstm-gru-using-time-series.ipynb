{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "340c411000d38f9d7e7cbfee7daf46d49571960a"
   },
   "source": [
    "# Aim and motivation\n",
    "The primary reason for this notebook is to practice and use RNNs for various tasks and applications. First of which is time series data. RNNs have truly changed the way sequential data is forecasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "trusted": true
   },
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In a **recurrent neural network (RNN)**, we store the output activations from one or more of the network’s layers—often the *hidden layer* activations. When we feed the next input example into the network, we include these previously stored outputs as **additional inputs**.\n",
    "\n",
    "You can think of these additional inputs as being **concatenated** to the end of the “normal” inputs to the previous layer.\n",
    "\n",
    "For example:\n",
    "* Suppose a hidden layer has:\n",
    "  * 10 regular input nodes\n",
    "  * 128 hidden nodes\n",
    "* If you feed the layer’s outputs back into itself (à la *Elman network*), the layer would now receive **138 total inputs** (10 original + 128 recurrent).\n",
    "\n",
    "Of course, the **first time** you compute the output of the network, you’ll need to initialize those extra 128 inputs—typically with zeros or some default values.\n",
    "\n",
    "Source: [Quora](https://www.quora.com/What-is-a-simple-explanation-of-a-recurrent-neural-network)\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*NKhwsOYNUT5xU7Pyf6Znhg.png\">\n",
    "\n",
    "Source: [Medium](https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9)\n",
    "\n",
    "Let me give you the best explanation of Recurrent Neural Networks that I found on internet: https://www.youtube.com/watch?v=UNmqTiOnRfg&t=3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "trusted": false
   },
   "source": [
    "## Limitations of RNNs: The Vanishing Gradient Problem\n",
    "\n",
    "Even though **Recurrent Neural Networks (RNNs)** are quite powerful, they suffer from the **vanishing gradient problem**, which limits their ability to learn from long-term dependencies.\n",
    "\n",
    "RNNs are typically effective at capturing information from **3–4 previous time steps**, but performance significantly degrades when dealing with longer sequences. Because of this, we don’t rely solely on regular RNNs for sequence modeling tasks.\n",
    "\n",
    "Instead, we use a more advanced type of RNN:\n",
    "\n",
    "> ### **Long Short-Term Memory Networks (LSTMs)**\n",
    ">\n",
    "> LSTMs are specially designed to handle long-range dependencies and overcome the vanishing gradient problem.\n",
    "\n",
    "### What is the Vanishing Gradient Problem?\n",
    "\n",
    "The **vanishing gradient problem** is a common difficulty encountered when training artificial neural networks using **gradient-based optimization methods** such as backpropagation.\n",
    "\n",
    "During training:\n",
    "\n",
    "* Each weight in the network is updated based on the **partial derivative of the error** with respect to that weight.\n",
    "* If this gradient is **very small**, the corresponding weight receives only a minimal update.\n",
    "* In extreme cases, the gradients become so small that weights **stop updating altogether**, halting the learning process.\n",
    "\n",
    "#### Why does this happen?\n",
    "\n",
    "* Traditional activation functions (like the **tanh** function) produce gradients in the range *(0, 1)*.\n",
    "* During backpropagation, gradients are calculated using the **chain rule** across many layers.\n",
    "* Multiplying many small values (gradients) together causes the final gradient for earlier layers to **shrink exponentially** with network depth.\n",
    "\n",
    "> As a result, the **front layers** (closer to the input) learn very slowly or not at all.\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1460/1*FWy4STsp8k0M5Yd8LifG_Q.png\">\n",
    "\n",
    "Source: [Medium](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory (LSTM)\n",
    "\n",
    "**Long Short-Term Memory (LSTM)** units—also called **LSTM blocks**—are the fundamental building components of certain types of **recurrent neural networks (RNNs)**. An RNN composed of LSTM units is often referred to as an **LSTM network**.\n",
    "\n",
    "### Key Components of an LSTM Unit\n",
    "\n",
    "A typical LSTM unit consists of:\n",
    "\n",
    "* **Cell** – Maintains the internal memory across time steps.\n",
    "* **Input gate** – Controls how much new information flows into the cell.\n",
    "* **Forget gate** – Decides what information from the cell state should be discarded.\n",
    "* **Output gate** – Determines how much of the cell state should be output.\n",
    "\n",
    "The **cell** is the core component responsible for “remembering” values over long time intervals, which is why \"memory\" is a central concept in LSTMs.\n",
    "\n",
    "Each gate behaves like a standard artificial neuron:\n",
    "\n",
    "* It performs a **weighted sum** of its inputs.\n",
    "* Passes the result through an **activation function** (typically sigmoid or tanh).\n",
    "* Acts as a **regulator** for the flow of information.\n",
    "\n",
    "These gates interact with the cell to control how information is stored, forgotten, or passed on at each time step.\n",
    "\n",
    "### Why \"Long Short-Term\"?\n",
    "\n",
    "The name **\"Long Short-Term Memory\"** refers to the model’s ability to capture **short-term patterns** in data that can be **retained over long durations**. This is in contrast to standard RNNs, which tend to forget long-term dependencies due to the **vanishing gradient problem**.\n",
    "\n",
    "### Applications\n",
    "\n",
    "LSTMs are well-suited for tasks involving:\n",
    "\n",
    "* **Time series prediction**\n",
    "* **Natural language processing (NLP)**\n",
    "* **Speech recognition**\n",
    "* **Music generation**\n",
    "\n",
    "They are especially effective when the time lags between important events are **variable and unknown**.\n",
    "\n",
    ">  LSTMs were specifically designed to address the **vanishing and exploding gradient problems** that hinder training of traditional RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ac683d1b85322f6ed4a564637914a8d803221b"
   },
   "source": [
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.\">\n",
    "\n",
    "Source: [Medium](https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968)\n",
    "\n",
    "The best LSTM explanation on internet: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n",
    "\n",
    "Refer above link for deeper insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of LSTMs\n",
    "\n",
    "An **LSTM cell** consists of several key components that work together to manage and control memory and state across time steps.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "* **Forget Gate `f`** – A neural network layer with a **sigmoid** activation.\n",
    "  Determines what portion of the past cell state should be forgotten.\n",
    "\n",
    "* **Candidate Layer `~C`** – A neural network layer with a **tanh** activation.\n",
    "  Proposes new candidate values that could be added to the memory.\n",
    "\n",
    "* **Input Gate `i`** – A neural network layer with a **sigmoid** activation.\n",
    "  Decides how much of the candidate values should be added to the memory.\n",
    "\n",
    "* **Output Gate `o`** – A neural network layer with a **sigmoid** activation.\n",
    "  Controls how much of the memory should be output as the hidden state.\n",
    "\n",
    "* **Hidden State `H`** – A vector representing the output of the LSTM at the current time step.\n",
    "\n",
    "* **Memory State `C`** – A vector that stores the internal memory of the LSTM across time steps.\n",
    "\n",
    "### Inputs and Outputs at Time Step *t*\n",
    "\n",
    "At any given time step **t**, the LSTM cell receives:\n",
    "\n",
    "* **Inputs**:\n",
    "\n",
    "  * `Xₜ` – Current input\n",
    "  * `Hₜ₋₁` – Previous hidden state\n",
    "  * `Cₜ₋₁` – Previous memory state\n",
    "\n",
    "* **Outputs**:\n",
    "\n",
    "  * `Hₜ` – Current hidden state\n",
    "  * `Cₜ` – Current memory state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7f3771ce38c69191eb23681561744863ee30ea8"
   },
   "source": [
    "## Working of Gates in LSTMs\n",
    "\n",
    "The **LSTM cell** uses gates and element-wise operations to manage memory and output at each time step.\n",
    "\n",
    "### Step 1: Forget Gate Operation\n",
    "\n",
    "The cell first uses the **forget gate** $f_t$ to decide which parts of the **previous memory state** $C_{t-1}$ to retain or forget.\n",
    "\n",
    "* If $f_t = 0$, the corresponding part of $C_{t-1}$ is completely forgotten.\n",
    "* If $f_t = 1$, it is fully retained.\n",
    "\n",
    "$$\n",
    "C_t = C_{t-1} \\cdot f_t\n",
    "$$\n",
    "\n",
    "### Step 2: Update Memory with Input Gate\n",
    "\n",
    "The **input gate** $i_t$ and the **candidate memory** $\\tilde{C}_t$ (generated via a tanh activation) are used to update the memory:\n",
    "\n",
    "$$\n",
    "C_t = C_t + (i_t \\cdot \\tilde{C}_t)\n",
    "$$\n",
    "\n",
    "This step adds new, relevant information to the memory.\n",
    "\n",
    "### Step 3: Compute the Output\n",
    "\n",
    "The **hidden state** $H_t$ is then computed by applying the `tanh` function to the updated memory:\n",
    "\n",
    "$$\n",
    "H_t = \\tanh(C_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f82cc24efc6283e5a98cadb78fc1a465e44c143",
    "collapsed": true,
    "trusted": true
   },
   "source": [
    "### And now we get to the code...\n",
    "I will use LSTMs for predicting the price of stocks of IBM for the year 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da0236e4b36ce514c1fec3fd72f236d1fa259131",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b288a8e2caf6196daec9cd2bc4ca78fe50345845",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Some functions to help out with\n",
    "def plot_predictions(test,predicted):\n",
    "    plt.plot(test, color='red',label='Real IBM Stock Price')\n",
    "    plt.plot(predicted, color='blue',label='Predicted IBM Stock Price')\n",
    "    plt.title('IBM Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('IBM Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    print(\"The root mean squared error is {}.\".format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cf10cf27420eb383b93b15c0895139ea96c0ed3",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First, we get the data\n",
    "dataset = pd.read_csv('../input/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb4c9db6d8a5bcf20ffad41747cfa5b6215ba220",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "training_set = dataset[:'2016'].iloc[:,1:2].values\n",
    "test_set = dataset['2017':].iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf5a9463d58e73852d2b70be9611e8cf1f4166fd",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# We have chosen 'High' attribute for prices. Let's see what it looks like\n",
    "dataset[\"High\"][:'2016'].plot(figsize=(16,4),legend=True)\n",
    "dataset[\"High\"]['2017':].plot(figsize=(16,4),legend=True)\n",
    "plt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\n",
    "plt.title('IBM stock price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc9c36165fc07d258bd5ea87874d2da17fa4a4d",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Scaling the training set\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fccfb866a2b4c702e0b2742f7c0289512d713d1b",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n",
    "# So for each element of training set, we have 60 previous training set elements \n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,2769):\n",
    "    X_train.append(training_set_scaled[i-60:i,0])\n",
    "    y_train.append(training_set_scaled[i,0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "637f699d3c4bde4b783de56ed4dd70a1bf59760d",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reshaping X_train for efficient modelling\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "df20eb7e8062dae0a3aff2182aa440faddd0017d",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n",
    "# Fitting to the training set\n",
    "regressor.fit(X_train,y_train,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "326fa85615622feb484cc4c848edeec6f7133913",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now to get the test set ready in a similar way as the training set.\n",
    "# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n",
    "# 'High' attribute data for processing\n",
    "dataset_total = pd.concat((dataset[\"High\"][:'2016'],dataset[\"High\"]['2017':]),axis=0)\n",
    "inputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "435b8024814939ac4fbd372baa0cd8cfc78f80bc",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparing X_test and predicting the prices\n",
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b774a8e79e53eac89694cafef6b11aa99226b95f",
    "collapsed": true,
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the results for LSTM\n",
    "plot_predictions(test_set,predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6f6db0b6e1f17ac63c06ce49856873d98ba5f00",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluating our model\n",
    "return_rmse(test_set,predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a4cf704ab3cd091f63b7b9a1b9224a49f0913171",
    "trusted": true
   },
   "source": [
    "## Gated Recurrent Units (GRU)\n",
    "\n",
    "Truth be told, that's one awesome score.\n",
    "But **LSTMs** aren't the only game in town when it comes to advanced recurrent units in deep learning. Another widely used and powerful alternative is the **Gated Recurrent Unit (GRU)**.\n",
    "\n",
    "It's still an open question which is *better*—**GRU** or **LSTM**—as they often show **comparable performance**. However:\n",
    "\n",
    "* **GRUs** are generally **easier to train** and require **fewer parameters**.\n",
    "* **LSTMs** might outperform GRUs on **larger datasets** due to their higher expressiveness.\n",
    "\n",
    "### What is a GRU?\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "* Unlike LSTMs, **GRUs do not use a separate memory unit** (`C_t`).\n",
    "* GRUs operate directly on **hidden states** to control the flow of information.\n",
    "\n",
    "This makes GRUs:\n",
    "\n",
    "* **Faster to train**\n",
    "* **Less prone to overfitting on small datasets**\n",
    "* **Structurally simpler** than LSTMs\n",
    "\n",
    "### GRU vs LSTM: Key Differences\n",
    "\n",
    "| Feature              | LSTM                  | GRU           |\n",
    "| -------------------- | --------------------- | ------------- |\n",
    "| Memory Cell          | Yes (`C_t`)           | No            |\n",
    "| Hidden State         | `H_t`                 | `H_t`         |\n",
    "| Gates                | Input, Forget, Output | Reset, Update |\n",
    "| Output Gate          | Yes                   | No            |\n",
    "| Second Non-linearity | Yes (before output)   | No            |\n",
    "| Parameters           | More                  | Fewer         |\n",
    "| Training             | Slower                | Faster        |\n",
    "\n",
    "### GRU Gates Explained\n",
    "\n",
    "* **Reset Gate $r_t$**:\n",
    "  Determines **how to combine the new input** with the previous memory.\n",
    "\n",
    "* **Update Gate $z_t$**:\n",
    "  Controls **how much of the past information** to keep.\n",
    "  It plays a similar role as the **input and forget gates** in LSTMs.\n",
    "\n",
    "Unlike LSTMs, GRUs **do not have**:\n",
    "\n",
    "* An **output gate**\n",
    "* A **second tanh non-linearity** before producing the hidden state\n",
    "\n",
    "> GRUs strike a balance between **simplicity** and **power**, often matching LSTM performance with less computational overhead.\n",
    "\n",
    "**Source**: [Quora](https://www.quora.com/Whats-the-difference-between-LSTM-and-GRU-Why-are-GRU-efficient-to-train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9b616c5112d707d16cc4b277007e286cffd58f6",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The GRU architecture\n",
    "regressorGRU = Sequential()\n",
    "# First GRU layer with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Second GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Third GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Fourth GRU layer\n",
    "regressorGRU.add(GRU(units=50, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1))\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
    "# Fitting to the training set\n",
    "regressorGRU.fit(X_train,y_train,epochs=50,batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "98628386f141545aa77d70f48478ac82bd9c1608"
   },
   "source": [
    "The current version version uses a dense GRU network with 100 units as opposed to the GRU network with 50 units in previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f20ca021ea3ce05f6c6a98db93775f1b2c9c022c",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparing X_test and predicting the prices\n",
    "X_test = []\n",
    "for i in range(60,311):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "GRU_predicted_stock_price = regressorGRU.predict(X_test)\n",
    "GRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da8e9fa28510aa03e7dd06d5070d7b16e05ebb6e",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the results for GRU\n",
    "plot_predictions(test_set,GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23aec5ab1a717e3458c8d5cae68db0e7add091ae",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluating GRU\n",
    "return_rmse(test_set,GRU_predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3aa099ecc057f972525df4c7ef4209744d27106a"
   },
   "source": [
    "## Sequence Generation\n",
    "\n",
    "Here, I will generate a sequence using **just the initial 60 values** instead of using the last 60 values for every new prediction.\n",
    "\n",
    "**Due to doubts raised in various comments about predictions using test set values, I decided to include sequence generation.**\n",
    "\n",
    "The models above rely on the test set by using the last 60 **true** values to predict the next one (which I’ll call the **benchmark**). This explains why their error is so low.\n",
    "\n",
    "Strong models *can* achieve similar results when generating sequences, but they require more than just previous data points. For example, in stock prediction, you also need market sentiment, movements of related stocks, and much more. So, don’t expect a remotely accurate plot here—the error will be significant. The best I can do is generate a trend roughly similar to the test set.\n",
    "\n",
    "### Model Choice and Results\n",
    "\n",
    "I will use the **GRU model** for this sequence generation. You can also try this with LSTMs.\n",
    "\n",
    "I modified the GRU model from above to get the best possible sequence. I ran the model four times:\n",
    "\n",
    "* Twice, the error was around **8 to 9**\n",
    "* Worst case had an error of around **11**\n",
    "\n",
    "Let's see how these iterations perform.\n",
    "\n",
    "The previous GRU model is fine too; just some tweaking was required to improve sequence quality.\n",
    "\n",
    "### Final Note\n",
    "\n",
    "**The main goal of this kernel is to show how to build RNN models.**\n",
    "How you predict data and what data you use is up to you.\n",
    "\n",
    "I can’t provide a 100-line script that automatically trains on your data and gives world-class results. That’s something you need to figure out yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2949de3c668a788117221000567aaa7033cd1a17",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparing sequence data\n",
    "initial_sequence = X_train[2708,:]\n",
    "sequence = []\n",
    "for i in range(251):\n",
    "    new_prediction = regressorGRU.predict(initial_sequence.reshape(initial_sequence.shape[1],initial_sequence.shape[0],1))\n",
    "    initial_sequence = initial_sequence[1:]\n",
    "    initial_sequence = np.append(initial_sequence,new_prediction,axis=0)\n",
    "    sequence.append(new_prediction)\n",
    "sequence = sc.inverse_transform(np.array(sequence).reshape(251,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "be390cade0cf4b31cb99e069570e1be2c5594c8b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the sequence\n",
    "plot_predictions(test_set,sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59095a00a05a68b97d76fc13bb2d618c337e1c80",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the sequence\n",
    "return_rmse(test_set,sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1003e9a17b23c1e3c7b7298250f8b071a93ae91"
   },
   "source": [
    "So, **GRU works better than LSTM** in this case. Using a **Bidirectional LSTM** is also a good way to make the model stronger. However, this may vary depending on the dataset.\n",
    "\n",
    "**Applying both LSTM and GRU together gave even better results.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
